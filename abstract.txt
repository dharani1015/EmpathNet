Research in non-intrusive and convenient monitoring of depression levels, especially using deep learning techniques, has gained a lot of steam in recent years. In this paper, we aim to explore deep learning methods to predict depression based on video and audio data of individuals. A comprehensive literature survey is performed, mainly focusing on the various video and audio based approaches to solve this problem. The Extended Distress Analysis Interview Corpus database is used for this purpose. The Multiscale Spatiotemporal Network proposed in De Melo et al. [24] is selected as an effective approach for only video-based depression detection and its architecture is reimplemented in PyTorch. We employ and observe the efficacy of data augmentation techniques based on Gaussian Noise Injection and Google Brain's SpecAugment strategy. We experiment with Siamese networks for depression classification for the first time in the domain, to the best of our knowledge. Thirty-four models from the Keras Applications library are experimented with along with a  simple 6-layer Custom CNN architecture, investigating the performance of simple and complex models for this problem. Finally, we investigate the effect of using text along with audio for the purpose of depression score prediction (regression). The maximum depression classification recall and accuracy achieved are 84% (by the Custom CNN Model) and 81% (by the Xception Model) respectively. A VGG16 model that uses both audio, and text encoded using Google's BERT, achieves the best performance while maintaining the ability to generalize over all PHQ-8 score ranges, resulting in a mean absolute error of 6.54.