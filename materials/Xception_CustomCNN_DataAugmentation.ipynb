{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmpathNetMVP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "F3rFz91J12kj",
        "VGXCiQHj17Ui",
        "_WHTz8cv1_Hd",
        "m6i8Q7Y0LGVG",
        "ABIV6nhh0cYO",
        "tV1nYjRgYEPC",
        "L36qV8mUUItO",
        "Odco1PT-cR58",
        "lha9KO71VsfS",
        "GaOrj2yZWJZJ"
      ],
      "machine_shape": "hm",
      "background_execution": "on",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "F3rFz91J12kj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmw7pqgWbnX_",
        "outputId": "4659e0b3-184c-4bc3-cd7b-b869da4ab80d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import os\n",
        "'CV-EmpathNet' in os.listdir('/content/drive/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/spect_images\n",
        "!mkdir spect_images"
      ],
      "metadata": {
        "id": "2aZ7O5AQdFwb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/CV-EmpathNet/spectogram_images.zip -d spect_images/\n",
        "!unzip /content/drive/MyDrive/CV-EmpathNet/spectogram_images_old.zip -d spect_images/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwkiMG6FcIWw",
        "outputId": "85951453-7ee0-4688-fc0d-f6c7404b39b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/CV-EmpathNet/spectogram_images_old.zip\n",
            "   creating: spect_images/spectogram_images/\n",
            "  inflating: spect_images/spectogram_images/713_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/390_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/444_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/669_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/340_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/627_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/368_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/641_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/326_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/691_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/422_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/470_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/374_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/312_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/488_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/416_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/458_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/622_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/345_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/491_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/716_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/441_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/395_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/427_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/469_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/323_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/658_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/371_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/475_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/688_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/389_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/413_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/359_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/670_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/317_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/712_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/445_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/391_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/626_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/341_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/327_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/640_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/369_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/.DS_Store  \n",
            "  inflating: spect_images/__MACOSX/spectogram_images/._.DS_Store  \n",
            "  inflating: spect_images/spectogram_images/423_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/471_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/375_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/612_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/489_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/313_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/459_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/417_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/490_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/344_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/623_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/717_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/440_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/695_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/468_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/426_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/322_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/617_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/370_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/659_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/689_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/474_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/412_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/388_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/316_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/358_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/446_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/392_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/408_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/625_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/324_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/420_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/693_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/472_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/338_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/376_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/677_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/310_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/414_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/309_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/347_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/620_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/397_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/443_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/696_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/425_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/608_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/321_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/373_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/477_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/439_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/411_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/708_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/315_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/409_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/393_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/447_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/710_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/343_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/624_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/325_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/692_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/421_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/473_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/377_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/339_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/311_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/676_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/638_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/415_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/346_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/492_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/308_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/442_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/396_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/715_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/424_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/697_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/320_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/609_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/372_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/615_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/438_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/476_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/410_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/709_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/673_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/314_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/463_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/329_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/600_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/367_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/628_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/301_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/666_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/405_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/457_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/383_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/419_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/353_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/634_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/487_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/652_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/335_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/431_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/682_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/362_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/605_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/466_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/428_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/400_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/663_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/304_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/318_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/482_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/631_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/356_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/705_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/386_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/452_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/687_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/434_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/619_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/330_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/657_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/462_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/366_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/601_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/328_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/667_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/300_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/629_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/404_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/418_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/382_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/456_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/486_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/635_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/352_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/334_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/653_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/683_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/430_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/604_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/363_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/429_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/467_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/718_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/401_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/305_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/662_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/357_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/483_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/319_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/453_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/387_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/435_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/656_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/331_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/618_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/461_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/365_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/602_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/664_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/303_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/407_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/449_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/381_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/455_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/702_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/485_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/636_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/351_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/379_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/337_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/650_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/680_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/433_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/649_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/607_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/360_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/699_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/464_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/402_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/348_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/306_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/661_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/354_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/633_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/480_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/450_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/384_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/707_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/436_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/478_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/655_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/332_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/603_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/364_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/302_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/448_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/406_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/454_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/380_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/703_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/350_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/637_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/484_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/679_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/651_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/336_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/378_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/432_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/361_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/606_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/465_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/698_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/403_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/399_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/660_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/307_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/349_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/481_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/632_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/355_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/385_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/451_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/479_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/684_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/437_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/333_P_spectrogram.png  \n",
            "  inflating: spect_images/spectogram_images/654_P_spectrogram.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read The Train and Test csvs\n",
        "import pandas as pd\n",
        "trdf = pd.read_csv('/content/drive/MyDrive/CV-EmpathNet/train_text_split.csv')\n",
        "trdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "aV_LrTOKc0OP",
        "outputId": "97a3ae63-edc1-4e7c-c7d1-83bfa7f5ef28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Participant_ID   Gender  PHQ_Binary  PHQ_Score  PCL-C (PTSD)  \\\n",
              "0               302     male           0          4             0   \n",
              "1               303   female           0          0             0   \n",
              "2               304   female           0          6             0   \n",
              "3               305     male           0          7             0   \n",
              "4               307  female            0          4             0   \n",
              "..              ...      ...         ...        ...           ...   \n",
              "158             695     male           0          7             1   \n",
              "159             697     male           0          5             0   \n",
              "160             702     male           0          0             0   \n",
              "161             703     male           0          8             0   \n",
              "162             707     male           0          1             0   \n",
              "\n",
              "     PTSD Severity  Sample                                         Transcript  \n",
              "0               28     302  just move around a little bit  when you're fin...  \n",
              "1               17     303  wow okay  when you're finished when she's done...  \n",
              "2               20     304  so we'll just move around a little bit tonight...  \n",
              "3               28     305  okay looks good so we can just move around a l...  \n",
              "4               23     307  looking at you all right okay so now let's mak...  \n",
              "..             ...     ...                                                ...  \n",
              "158             62     695  okay  I will  press the button  just push the ...  \n",
              "159             24     697  okie dokie yeah  okay  here we go  I'm not a t...  \n",
              "160             19     702  hi I'm not a therapist  are you okay  yes  oka...  \n",
              "161             28     703  and please  are you okay with this yes  doing ...  \n",
              "162             23     707  okay but but okay but I don't stop until it's ...  \n",
              "\n",
              "[163 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91a546cd-12c5-4898-a120-0f2f83a55734\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>PHQ_Binary</th>\n",
              "      <th>PHQ_Score</th>\n",
              "      <th>PCL-C (PTSD)</th>\n",
              "      <th>PTSD Severity</th>\n",
              "      <th>Sample</th>\n",
              "      <th>Transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>302</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>302</td>\n",
              "      <td>just move around a little bit  when you're fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>303</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>303</td>\n",
              "      <td>wow okay  when you're finished when she's done...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>304</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>304</td>\n",
              "      <td>so we'll just move around a little bit tonight...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>305</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>305</td>\n",
              "      <td>okay looks good so we can just move around a l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>307</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>307</td>\n",
              "      <td>looking at you all right okay so now let's mak...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>695</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>695</td>\n",
              "      <td>okay  I will  press the button  just push the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>697</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>697</td>\n",
              "      <td>okie dokie yeah  okay  here we go  I'm not a t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>702</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>702</td>\n",
              "      <td>hi I'm not a therapist  are you okay  yes  oka...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>703</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>703</td>\n",
              "      <td>and please  are you okay with this yes  doing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>707</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>707</td>\n",
              "      <td>okay but but okay but I don't stop until it's ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>163 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91a546cd-12c5-4898-a120-0f2f83a55734')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91a546cd-12c5-4898-a120-0f2f83a55734 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91a546cd-12c5-4898-a120-0f2f83a55734');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tedf = pd.read_csv('/content/drive/MyDrive/CV-EmpathNet/test_text_split.csv')\n",
        "tedf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_ttOjZTgpEuX",
        "outputId": "27b40dab-24e2-4cb1-e3f7-da3ceea43832"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Participant_ID  Gender  PHQ_Binary  PHQ_Score  PCL-C (PTSD)  \\\n",
              "0              600  female           0          5             0   \n",
              "1              602  female           1         13             1   \n",
              "2              604    male           1         12             0   \n",
              "3              605    male           0          2             0   \n",
              "4              606  female           0          5             0   \n",
              "5              607  female           0          7             0   \n",
              "6              609    male           0          0             0   \n",
              "7              615    male           0          3             0   \n",
              "8              618    male           0          4             0   \n",
              "9              619  female           0          6             0   \n",
              "10             620    male           0          0             0   \n",
              "11             622    male           0          0             0   \n",
              "12             623    male           0          9             0   \n",
              "13             624    male           1         22             1   \n",
              "14             625    male           0          2             0   \n",
              "15             626    male           0          0             0   \n",
              "16             629    male           0          0             0   \n",
              "17             631    male           0          0             0   \n",
              "18             634  female           0          2             0   \n",
              "19             635    male           0          9             0   \n",
              "20             636  female           1         15             1   \n",
              "21             637  female           1         16             0   \n",
              "22             638  female           1         19             1   \n",
              "23             640    male           1         17             0   \n",
              "24             649    male           1         19             1   \n",
              "25             650    male           0          3             0   \n",
              "26             651    male           0          5             0   \n",
              "27             652    male           0          7             1   \n",
              "28             655  female           1         12             1   \n",
              "29             656    male           0          8             0   \n",
              "30             658  female           1         11             1   \n",
              "31             659    male           1         16             1   \n",
              "32             661  female           1         19             1   \n",
              "33             663    male           0          9             1   \n",
              "34             664    male           0          2             0   \n",
              "35             666    male           0          5             0   \n",
              "36             669    male           0          7             1   \n",
              "37             676    male           0          0             0   \n",
              "38             679    male           0          8             1   \n",
              "39             682    male           0         12             0   \n",
              "40             683    male           0          6             0   \n",
              "41             688    male           1         19             1   \n",
              "42             689  female           1         13             1   \n",
              "43             691    male           0         11             1   \n",
              "44             693    male           0          3             0   \n",
              "45             696    male           0         13             1   \n",
              "46             699    male           1         16             1   \n",
              "47             705    male           1         20             1   \n",
              "48             708    male           0          0             0   \n",
              "49             709    male           0         10             0   \n",
              "50             710    male           0          7             1   \n",
              "51             712    male           0          0             0   \n",
              "52             715    male           0          7             0   \n",
              "53             716    male           1         15             1   \n",
              "54             717    male           0          1             0   \n",
              "55             718    male           0          3             0   \n",
              "\n",
              "    PTSD Severity  Sample                                         Transcript  \n",
              "0            23.0     600  okay there she is coming to go ahead and Shrin...  \n",
              "1            67.0     602  this is super need I like this  me either at a...  \n",
              "2            30.0     604  I passed out to make sure that our audio recog...  \n",
              "3            23.0     605  remember to bring up a virtual human for a sec...  \n",
              "4            46.0     606  hey I just got it I just got a new so okay so ...  \n",
              "5            29.0     607  bring her up for a quick second here  bring he...  \n",
              "6            19.0     609  research experience  actually going to see a v...  \n",
              "7            22.0     615  I'm going to have you alert me to look up  sur...  \n",
              "8            23.0     618  you live the  are you  what are you going to g...  \n",
              "9            37.0     619  that should be fine in the matching to put thi...  \n",
              "10           17.0     620  oh okay  okay  so I'll be looking at that imag...  \n",
              "11           23.0     622  hi therapist  would love to learn about you I'...  \n",
              "12           26.0     623  I'm doing it on but I'm not  oh okay  smartpho...  \n",
              "13           81.0     624  so I'm going to bring up I understand I know h...  \n",
              "14           26.0     625  regular and so the virtual human to ask you so...  \n",
              "15           18.0     626  some of those pictures all right  yes  I'm fin...  \n",
              "16           20.0     629  to respond to you and keep  Mount taal Interac...  \n",
              "17           17.0     631  can I go ahead  fishing walleye  there she is ...  \n",
              "18           18.0     634  okay hold on what you said she should stay goo...  \n",
              "19           36.0     635  reviewed how this works  and then you're going...  \n",
              "20           62.0     636  you just look at Ali so she's asking a series ...  \n",
              "21           49.0     637  okay  I think I got it  what year Buick Minnie...  \n",
              "22           44.0     638  when she sounds you should say but definition ...  \n",
              "23           35.0     640  okay  okay  okay  okay so I'm just going to an...  \n",
              "24           68.0     649  but when you deal with humans got to deal with...  \n",
              "25           24.0     650  number to certify recording equipment  and ple...  \n",
              "26           17.0     651  behind you just  tell me to bring up a virtual...  \n",
              "27           43.0     652  all right  you know what the dod is going to u...  \n",
              "28           71.0     655  well I got the last piece  okay is the mic goo...  \n",
              "29           42.0     656  I'll bring it by virtue of human for a second ...  \n",
              "30           66.0     658  I just upgraded to a present  cricket  to let ...  \n",
              "31           55.0     659  now I'm going to pull up over to human yoga to...  \n",
              "32           67.0     661  I think they're amazing invention really  okay...  \n",
              "33           52.0     663  after the after goodbye that means I'm done  y...  \n",
              "34           23.0     664  kind of flies in here a little bit virtual of ...  \n",
              "35           32.0     666  there she is  if I press this button I'll come...  \n",
              "36            NaN     669  I just say I don't feel like I'm not  I'm read...  \n",
              "37           19.0     676  she is hi Emily  you're welcome  I'm not a the...  \n",
              "38           58.0     679  hi I'm Ellie  I'm not a therapist  and please ...  \n",
              "39           37.0     682  so they had  right overload  no problem  okay ...  \n",
              "40            NaN     683  very long  hussy  okay  it made it onto the ca...  \n",
              "41           70.0     688  hi I'm Ellie I'm pleased  yes  I'm doing okay ...  \n",
              "42           64.0     689  okay  okay okay  okay  goodbye okay okay  I'll...  \n",
              "43           60.0     691  where are the cameras I see that one  let's do...  \n",
              "44           36.0     693  play 20 seconds or more  allergy her every onc...  \n",
              "45           53.0     696  I'm going to bring her up on the screen  are y...  \n",
              "46           77.0     699  okay  20 seconds or more go ahead and press th...  \n",
              "47           77.0     705  causes for 20 seconds from or go ahead and pre...  \n",
              "48           19.0     708  Minister for camcorder recording when I reach ...  \n",
              "49           39.0     709  look at you now then go ahead and press that b...  \n",
              "50           53.0     710  maybe in something  gotcha  hi Emily  I'm not ...  \n",
              "51           21.0     712  so friggin I'm pleased  are you okay with this...  \n",
              "52           55.0     715  okay  okay okay  and once you spend just push ...  \n",
              "53           73.0     716  not very typical to that happen but just in ca...  \n",
              "54           20.0     717  okay if you notice that she pauses for more th...  \n",
              "55           29.0     718  how long should this person last okay  sure  I...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44f701cc-55a1-4477-bf8a-753a615af467\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>PHQ_Binary</th>\n",
              "      <th>PHQ_Score</th>\n",
              "      <th>PCL-C (PTSD)</th>\n",
              "      <th>PTSD Severity</th>\n",
              "      <th>Sample</th>\n",
              "      <th>Transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>600</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>600</td>\n",
              "      <td>okay there she is coming to go ahead and Shrin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>602</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>67.0</td>\n",
              "      <td>602</td>\n",
              "      <td>this is super need I like this  me either at a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>604</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>604</td>\n",
              "      <td>I passed out to make sure that our audio recog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>605</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>605</td>\n",
              "      <td>remember to bring up a virtual human for a sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>606</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>606</td>\n",
              "      <td>hey I just got it I just got a new so okay so ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>607</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>607</td>\n",
              "      <td>bring her up for a quick second here  bring he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>609</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>609</td>\n",
              "      <td>research experience  actually going to see a v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>615</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>615</td>\n",
              "      <td>I'm going to have you alert me to look up  sur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>618</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>618</td>\n",
              "      <td>you live the  are you  what are you going to g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>619</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>619</td>\n",
              "      <td>that should be fine in the matching to put thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>620</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>620</td>\n",
              "      <td>oh okay  okay  so I'll be looking at that imag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>622</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>622</td>\n",
              "      <td>hi therapist  would love to learn about you I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>623</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>623</td>\n",
              "      <td>I'm doing it on but I'm not  oh okay  smartpho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>624</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>81.0</td>\n",
              "      <td>624</td>\n",
              "      <td>so I'm going to bring up I understand I know h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>625</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>625</td>\n",
              "      <td>regular and so the virtual human to ask you so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>626</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>626</td>\n",
              "      <td>some of those pictures all right  yes  I'm fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>629</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>629</td>\n",
              "      <td>to respond to you and keep  Mount taal Interac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>631</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>631</td>\n",
              "      <td>can I go ahead  fishing walleye  there she is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>634</td>\n",
              "      <td>female</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>634</td>\n",
              "      <td>okay hold on what you said she should stay goo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>635</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>635</td>\n",
              "      <td>reviewed how this works  and then you're going...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>636</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>62.0</td>\n",
              "      <td>636</td>\n",
              "      <td>you just look at Ali so she's asking a series ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>637</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>637</td>\n",
              "      <td>okay  I think I got it  what year Buick Minnie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>638</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>44.0</td>\n",
              "      <td>638</td>\n",
              "      <td>when she sounds you should say but definition ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>640</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>640</td>\n",
              "      <td>okay  okay  okay  okay so I'm just going to an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>649</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>68.0</td>\n",
              "      <td>649</td>\n",
              "      <td>but when you deal with humans got to deal with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>650</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>650</td>\n",
              "      <td>number to certify recording equipment  and ple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>651</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>651</td>\n",
              "      <td>behind you just  tell me to bring up a virtual...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>652</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>43.0</td>\n",
              "      <td>652</td>\n",
              "      <td>all right  you know what the dod is going to u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>655</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>71.0</td>\n",
              "      <td>655</td>\n",
              "      <td>well I got the last piece  okay is the mic goo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>656</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>656</td>\n",
              "      <td>I'll bring it by virtue of human for a second ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>658</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>66.0</td>\n",
              "      <td>658</td>\n",
              "      <td>I just upgraded to a present  cricket  to let ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>659</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>55.0</td>\n",
              "      <td>659</td>\n",
              "      <td>now I'm going to pull up over to human yoga to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>661</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>67.0</td>\n",
              "      <td>661</td>\n",
              "      <td>I think they're amazing invention really  okay...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>663</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>52.0</td>\n",
              "      <td>663</td>\n",
              "      <td>after the after goodbye that means I'm done  y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>664</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>664</td>\n",
              "      <td>kind of flies in here a little bit virtual of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>666</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>666</td>\n",
              "      <td>there she is  if I press this button I'll come...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>669</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>669</td>\n",
              "      <td>I just say I don't feel like I'm not  I'm read...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>676</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>676</td>\n",
              "      <td>she is hi Emily  you're welcome  I'm not a the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>679</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>58.0</td>\n",
              "      <td>679</td>\n",
              "      <td>hi I'm Ellie  I'm not a therapist  and please ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>682</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>682</td>\n",
              "      <td>so they had  right overload  no problem  okay ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>683</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>683</td>\n",
              "      <td>very long  hussy  okay  it made it onto the ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>688</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>70.0</td>\n",
              "      <td>688</td>\n",
              "      <td>hi I'm Ellie I'm pleased  yes  I'm doing okay ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>689</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>64.0</td>\n",
              "      <td>689</td>\n",
              "      <td>okay  okay okay  okay  goodbye okay okay  I'll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>691</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>60.0</td>\n",
              "      <td>691</td>\n",
              "      <td>where are the cameras I see that one  let's do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>693</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>693</td>\n",
              "      <td>play 20 seconds or more  allergy her every onc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>696</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>53.0</td>\n",
              "      <td>696</td>\n",
              "      <td>I'm going to bring her up on the screen  are y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>699</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>77.0</td>\n",
              "      <td>699</td>\n",
              "      <td>okay  20 seconds or more go ahead and press th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>705</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>77.0</td>\n",
              "      <td>705</td>\n",
              "      <td>causes for 20 seconds from or go ahead and pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>708</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>708</td>\n",
              "      <td>Minister for camcorder recording when I reach ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>709</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>709</td>\n",
              "      <td>look at you now then go ahead and press that b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>710</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>53.0</td>\n",
              "      <td>710</td>\n",
              "      <td>maybe in something  gotcha  hi Emily  I'm not ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>712</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>712</td>\n",
              "      <td>so friggin I'm pleased  are you okay with this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>715</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>715</td>\n",
              "      <td>okay  okay okay  and once you spend just push ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>716</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>73.0</td>\n",
              "      <td>716</td>\n",
              "      <td>not very typical to that happen but just in ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>717</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>717</td>\n",
              "      <td>okay if you notice that she pauses for more th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>718</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>718</td>\n",
              "      <td>how long should this person last okay  sure  I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44f701cc-55a1-4477-bf8a-753a615af467')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-44f701cc-55a1-4477-bf8a-753a615af467 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-44f701cc-55a1-4477-bf8a-753a615af467');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tp = trdf.shape[0]\n",
        "ts = tedf.shape[0]\n",
        "print('Train Participants:', tp)\n",
        "print('Test Participants:', ts)\n",
        "print('Total Participants:', ts+tp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJM3B4YnpNvD",
        "outputId": "a59d7f8c-219a-4c1f-87e3-aed9e2645d2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Participants: 163\n",
            "Test Participants: 56\n",
            "Total Participants: 219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "VGXCiQHj17Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf train\n",
        "# !rm -rf test"
      ],
      "metadata": {
        "id": "EAgi65LYGLwD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seperate Images Into Train and Test Folders and 0 and 1 class.\n",
        "import os\n",
        "\n",
        "os.makedirs('/content/train/1', exist_ok=True)\n",
        "os.makedirs('/content/train/0', exist_ok=True)\n",
        "\n",
        "os.makedirs('/content/test/1', exist_ok=True)\n",
        "os.makedirs('/content/test/0', exist_ok=True)"
      ],
      "metadata": {
        "id": "3x5sIcKGpVlk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copy2\n",
        "src = '/content/spect_images/spectogram_images'\n",
        "dst = '/content/train/0'\n",
        "for id in trdf[trdf['PHQ_Binary']==0]['Participant_ID']:\n",
        "  copy2(os.path.join(src, str(id) +'_P_spectrogram.png'), dst)\n",
        "\n",
        "dst = '/content/train/1'\n",
        "for id in trdf[trdf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  copy2(os.path.join(src, str(id) +'_P_spectrogram.png'), dst)\n",
        "\n",
        "dst = '/content/test/1'\n",
        "for id in tedf[tedf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  copy2(os.path.join(src, str(id) +'_P_spectrogram.png'), dst)\n",
        "\n",
        "dst = '/content/test/0'\n",
        "for id in tedf[tedf['PHQ_Binary']==0]['Participant_ID']:\n",
        "  copy2(os.path.join(src, str(id) +'_P_spectrogram.png'), dst)\n"
      ],
      "metadata": {
        "id": "qFOAy9GCp-Zm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "from tqdm import tqdm\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "mIU2jdX4rEvO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preliminary Train and Test ImageDataGenerators\n",
        "preproc_func = tf.keras.applications.densenet.preprocess_input\n",
        "# preproc_func = tf.keras.applications.xception.preprocess_input"
      ],
      "metadata": {
        "id": "Hbrb9tWm9TtR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 4\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "train_gen = train_datagen.flow_from_directory('/content/train', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "# train_gen = train_datagen.flow_from_directory('/content/train', (224,224), class_mode = 'categorical', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(train_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTr9LLvLBIoT",
        "outputId": "fa0b6bb6-f6e3-49a2-8071-b1f65c2089ba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 163 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 4\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "# test_gen = train_datagen.flow_from_directory('/content/test', (224,224), class_mode = 'categorical', batch_size = bs, shuffle=True, seed=42)\n",
        "test_gen = train_datagen.flow_from_directory('/content/test', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(test_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmL8-tRQDsyk",
        "outputId": "72404b5c-a0de-4939-a910-03f5d813f327"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 56 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection"
      ],
      "metadata": {
        "id": "_WHTz8cv1_Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################################################\n",
        "# The following article was referred to for writing the below lines of code\n",
        "# https://towardsdatascience.com/how-to-choose-the-best-keras-pre-trained-model-for-image-classification-b850ca4428d4\n"
      ],
      "metadata": {
        "id": "vECrDH9eR4hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a model dictionary\n",
        "model_dictionary = {m[0]:m[1] for m in inspect.getmembers(tf.keras.applications, inspect.isfunction)}"
      ],
      "metadata": {
        "id": "WnWmk56r-xZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt1PRGuw_Rj6",
        "outputId": "55d901f3-a8b5-4fe8-8415-b1dfc672b08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DenseNet121': <function keras.applications.densenet.DenseNet121>,\n",
              " 'DenseNet169': <function keras.applications.densenet.DenseNet169>,\n",
              " 'DenseNet201': <function keras.applications.densenet.DenseNet201>,\n",
              " 'EfficientNetB0': <function keras.applications.efficientnet.EfficientNetB0>,\n",
              " 'EfficientNetB1': <function keras.applications.efficientnet.EfficientNetB1>,\n",
              " 'EfficientNetB2': <function keras.applications.efficientnet.EfficientNetB2>,\n",
              " 'EfficientNetB3': <function keras.applications.efficientnet.EfficientNetB3>,\n",
              " 'EfficientNetB4': <function keras.applications.efficientnet.EfficientNetB4>,\n",
              " 'EfficientNetB5': <function keras.applications.efficientnet.EfficientNetB5>,\n",
              " 'EfficientNetB6': <function keras.applications.efficientnet.EfficientNetB6>,\n",
              " 'EfficientNetB7': <function keras.applications.efficientnet.EfficientNetB7>,\n",
              " 'EfficientNetV2B0': <function keras.applications.efficientnet_v2.EfficientNetV2B0>,\n",
              " 'EfficientNetV2B1': <function keras.applications.efficientnet_v2.EfficientNetV2B1>,\n",
              " 'EfficientNetV2B2': <function keras.applications.efficientnet_v2.EfficientNetV2B2>,\n",
              " 'EfficientNetV2B3': <function keras.applications.efficientnet_v2.EfficientNetV2B3>,\n",
              " 'EfficientNetV2L': <function keras.applications.efficientnet_v2.EfficientNetV2L>,\n",
              " 'EfficientNetV2M': <function keras.applications.efficientnet_v2.EfficientNetV2M>,\n",
              " 'EfficientNetV2S': <function keras.applications.efficientnet_v2.EfficientNetV2S>,\n",
              " 'InceptionResNetV2': <function keras.applications.inception_resnet_v2.InceptionResNetV2>,\n",
              " 'InceptionV3': <function keras.applications.inception_v3.InceptionV3>,\n",
              " 'MobileNet': <function keras.applications.mobilenet.MobileNet>,\n",
              " 'MobileNetV2': <function keras.applications.mobilenet_v2.MobileNetV2>,\n",
              " 'MobileNetV3Large': <function keras.applications.mobilenet_v3.MobileNetV3Large>,\n",
              " 'MobileNetV3Small': <function keras.applications.mobilenet_v3.MobileNetV3Small>,\n",
              " 'NASNetLarge': <function keras.applications.nasnet.NASNetLarge>,\n",
              " 'NASNetMobile': <function keras.applications.nasnet.NASNetMobile>,\n",
              " 'ResNet101': <function keras.applications.resnet.ResNet101>,\n",
              " 'ResNet101V2': <function keras.applications.resnet_v2.ResNet101V2>,\n",
              " 'ResNet152': <function keras.applications.resnet.ResNet152>,\n",
              " 'ResNet152V2': <function keras.applications.resnet_v2.ResNet152V2>,\n",
              " 'ResNet50': <function keras.applications.resnet.ResNet50>,\n",
              " 'ResNet50V2': <function keras.applications.resnet_v2.ResNet50V2>,\n",
              " 'VGG16': <function keras.applications.vgg16.VGG16>,\n",
              " 'VGG19': <function keras.applications.vgg19.VGG19>,\n",
              " 'Xception': <function keras.applications.xception.Xception>}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('No. of models considered = ',len(list(model_dictionary.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9m73jww9rIH",
        "outputId": "e6bce4ef-2966-4834-c1c2-c92429af82c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of models considered =  35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over each model available in Keras\n",
        "from tensorflow.python.keras.layers import Dense, Flatten\n",
        "\n",
        "model_benchmarks = {'model_name': [], 'num_model_params': [], 'validation_accuracy': [], 'train_accuracy': [], 'train_loss':[], 'val_loss':[]}\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "error_models = []\n",
        "count = 1\n",
        "for model_name, model in tqdm(model_dictionary.items()):\n",
        "    print('-'*50)\n",
        "    print(count,'.',model_name)\n",
        "        \n",
        "    # load the pre-trained model with global average pooling as the last layer and freeze the model weights\n",
        "    \n",
        "    input_shape = (224,224,3)\n",
        "\n",
        "    try:\n",
        "      pre_trained_model = model(include_top=False, input_shape=input_shape)\n",
        "      pre_trained_model.trainable = False\n",
        "      \n",
        "      clf_model = tf.keras.models.Sequential()\n",
        "      clf_model.add(pre_trained_model)\n",
        "\n",
        "      clf_model.add(Flatten())\n",
        "\n",
        "      clf_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "      clf_model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "      history = clf_model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)\n",
        "      \n",
        "      # Calculate all relevant metrics\n",
        "      model_benchmarks['model_name'].append(model_name)\n",
        "      model_benchmarks['num_model_params'].append(pre_trained_model.count_params())\n",
        "      model_benchmarks['validation_accuracy'].append(history.history['val_accuracy'][-1])\n",
        "      model_benchmarks['train_accuracy'].append(history.history['accuracy'][-1])\n",
        "      model_benchmarks['train_loss'].append(history.history['loss'][-1])\n",
        "      model_benchmarks['val_loss'].append(history.history['val_loss'][-1])\n",
        "      \n",
        "      print('Model Trained', '-'*30)\n",
        "\n",
        "    except Exception as e:\n",
        "      print('!'*50)\n",
        "      print('Error',e)\n",
        "      error_models.append(model_name)\n",
        "      clf_model.summary()\n",
        "      print('!'*50)\n",
        "      # break\n",
        "\n",
        "    count += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OOkM1m77Er1N",
        "outputId": "815361fd-7e9e-4b79-ed0d-336a4305e448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/35 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "1 . DenseNet121\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 8s 65ms/step - loss: 1.3915 - accuracy: 0.6855 - val_loss: 0.6231 - val_accuracy: 0.6964\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.0867 - accuracy: 0.6604 - val_loss: 0.6145 - val_accuracy: 0.6964\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.0705 - accuracy: 0.6855 - val_loss: 0.7287 - val_accuracy: 0.6964\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.0633 - accuracy: 0.6730 - val_loss: 0.6572 - val_accuracy: 0.6964\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 1s 25ms/step - loss: 1.1092 - accuracy: 0.6730 - val_loss: 1.2489 - val_accuracy: 0.6964\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.9954 - accuracy: 0.6918 - val_loss: 0.9399 - val_accuracy: 0.3036\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.0855 - accuracy: 0.6352 - val_loss: 1.3652 - val_accuracy: 0.6964\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 1s 25ms/step - loss: 1.0868 - accuracy: 0.7170 - val_loss: 1.6708 - val_accuracy: 0.3036\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.8403 - accuracy: 0.6855 - val_loss: 1.2554 - val_accuracy: 0.6964\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.1352 - accuracy: 0.6541 - val_loss: 1.2742 - val_accuracy: 0.3036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 1/35 [00:19<11:15, 19.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Added **************************************************\n",
            "--------------------------------------------------\n",
            "2 . DenseNet169\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 11s 89ms/step - loss: 1.6720 - accuracy: 0.6855 - val_loss: 1.8633 - val_accuracy: 0.6964\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 33ms/step - loss: 1.3582 - accuracy: 0.6289 - val_loss: 2.8335 - val_accuracy: 0.6964\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 1s 33ms/step - loss: 1.3364 - accuracy: 0.7170 - val_loss: 2.1316 - val_accuracy: 0.6964\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 1s 32ms/step - loss: 1.5456 - accuracy: 0.6478 - val_loss: 0.6888 - val_accuracy: 0.5179\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 1s 32ms/step - loss: 1.3975 - accuracy: 0.6604 - val_loss: 2.7666 - val_accuracy: 0.6964\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 1s 33ms/step - loss: 1.5117 - accuracy: 0.6289 - val_loss: 0.7084 - val_accuracy: 0.6964\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 1s 32ms/step - loss: 1.6486 - accuracy: 0.6101 - val_loss: 1.1978 - val_accuracy: 0.6964\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 1s 33ms/step - loss: 1.3634 - accuracy: 0.6667 - val_loss: 0.8361 - val_accuracy: 0.4286\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 1s 33ms/step - loss: 1.2396 - accuracy: 0.6855 - val_loss: 1.3585 - val_accuracy: 0.6964\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 1s 33ms/step - loss: 1.3099 - accuracy: 0.6918 - val_loss: 1.5207 - val_accuracy: 0.6964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 2/35 [00:46<13:12, 24.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Added **************************************************\n",
            "--------------------------------------------------\n",
            "3 . DenseNet201\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 13s 107ms/step - loss: 1.5327 - accuracy: 0.6792 - val_loss: 1.4130 - val_accuracy: 0.6964\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 2s 40ms/step - loss: 1.2446 - accuracy: 0.6730 - val_loss: 1.2654 - val_accuracy: 0.6964\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 2s 40ms/step - loss: 1.1582 - accuracy: 0.6938 - val_loss: 0.6312 - val_accuracy: 0.6964\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 2s 40ms/step - loss: 1.0625 - accuracy: 0.6855 - val_loss: 1.4285 - val_accuracy: 0.6964\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 2s 40ms/step - loss: 1.1340 - accuracy: 0.6792 - val_loss: 1.4002 - val_accuracy: 0.6964\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 2s 41ms/step - loss: 1.0388 - accuracy: 0.7107 - val_loss: 2.7688 - val_accuracy: 0.3036\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 2s 41ms/step - loss: 1.1681 - accuracy: 0.6101 - val_loss: 2.1737 - val_accuracy: 0.6964\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 2s 42ms/step - loss: 1.2689 - accuracy: 0.6352 - val_loss: 0.6301 - val_accuracy: 0.5893\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 2s 41ms/step - loss: 1.0968 - accuracy: 0.6289 - val_loss: 0.6452 - val_accuracy: 0.5893\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 2s 41ms/step - loss: 0.8825 - accuracy: 0.7296 - val_loss: 0.6487 - val_accuracy: 0.6964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 3/35 [01:21<15:19, 28.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Added **************************************************\n",
            "--------------------------------------------------\n",
            "4 . EfficientNetB0\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 7s 48ms/step - loss: 3.6950 - accuracy: 0.6730 - val_loss: 1.1058 - val_accuracy: 0.3036\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 2.7048 - accuracy: 0.6541 - val_loss: 1.0468 - val_accuracy: 0.3036\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 2.6804 - accuracy: 0.6792 - val_loss: 1.6718 - val_accuracy: 0.6964\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 2.7881 - accuracy: 0.6855 - val_loss: 4.5751 - val_accuracy: 0.6964\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 2.7494 - accuracy: 0.6604 - val_loss: 5.1359 - val_accuracy: 0.6964\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 2.9996 - accuracy: 0.6604 - val_loss: 1.9875 - val_accuracy: 0.6964\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 2.8384 - accuracy: 0.6604 - val_loss: 1.1020 - val_accuracy: 0.3036\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 3.6643 - accuracy: 0.5786 - val_loss: 1.8701 - val_accuracy: 0.6964\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 2.4826 - accuracy: 0.6730 - val_loss: 2.3013 - val_accuracy: 0.6964\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 2.8597 - accuracy: 0.6855 - val_loss: 4.8618 - val_accuracy: 0.3036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█▏        | 4/35 [01:36<12:09, 23.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Added **************************************************\n",
            "--------------------------------------------------\n",
            "5 . EfficientNetB1\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 12s 70ms/step - loss: 4.6935 - accuracy: 0.6164 - val_loss: 3.1558 - val_accuracy: 0.6964\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.1215 - accuracy: 0.6541 - val_loss: 5.8768 - val_accuracy: 0.6964\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.3499 - accuracy: 0.6289 - val_loss: 3.8560 - val_accuracy: 0.6964\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.9219 - accuracy: 0.6289 - val_loss: 2.2069 - val_accuracy: 0.6964\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.4655 - accuracy: 0.6478 - val_loss: 3.5403 - val_accuracy: 0.3036\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.8432 - accuracy: 0.6101 - val_loss: 4.7765 - val_accuracy: 0.6964\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.2266 - accuracy: 0.6604 - val_loss: 4.6009 - val_accuracy: 0.6964\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.3852 - accuracy: 0.6289 - val_loss: 2.4442 - val_accuracy: 0.3036\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.1740 - accuracy: 0.6604 - val_loss: 7.3879 - val_accuracy: 0.6964\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.2709 - accuracy: 0.6415 - val_loss: 1.7179 - val_accuracy: 0.6964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 5/35 [01:59<11:41, 23.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Added **************************************************\n",
            "--------------------------------------------------\n",
            "6 . EfficientNetB2\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 10s 67ms/step - loss: 6.0716 - accuracy: 0.6730 - val_loss: 6.9418 - val_accuracy: 0.6964\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 5.9116 - accuracy: 0.6164 - val_loss: 8.3744 - val_accuracy: 0.6964\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 5.1697 - accuracy: 0.6415 - val_loss: 8.0082 - val_accuracy: 0.6964\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 4.9216 - accuracy: 0.6478 - val_loss: 4.7812 - val_accuracy: 0.6964\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.7645 - accuracy: 0.6667 - val_loss: 14.9795 - val_accuracy: 0.3036\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 6.4561 - accuracy: 0.5660 - val_loss: 12.4968 - val_accuracy: 0.6964\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.2453 - accuracy: 0.6667 - val_loss: 3.8659 - val_accuracy: 0.6964\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 5.4543 - accuracy: 0.6289 - val_loss: 1.6458 - val_accuracy: 0.3036\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 5.8795 - accuracy: 0.6038 - val_loss: 6.8489 - val_accuracy: 0.6964\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 4.7300 - accuracy: 0.6289 - val_loss: 4.1311 - val_accuracy: 0.3036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 6/35 [02:20<10:56, 22.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Added **************************************************\n",
            "--------------------------------------------------\n",
            "7 . EfficientNetB3\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 13s 78ms/step - loss: 5.1349 - accuracy: 0.6478 - val_loss: 6.6876 - val_accuracy: 0.6964\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 1s 26ms/step - loss: 3.9395 - accuracy: 0.6667 - val_loss: 3.7003 - val_accuracy: 0.6964\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.1553 - accuracy: 0.6164 - val_loss: 11.1215 - val_accuracy: 0.6964\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.3245 - accuracy: 0.6918 - val_loss: 0.6278 - val_accuracy: 0.6964\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.7260 - accuracy: 0.6164 - val_loss: 0.6394 - val_accuracy: 0.6964\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 3.9926 - accuracy: 0.6918 - val_loss: 1.3658 - val_accuracy: 0.6964\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.5562 - accuracy: 0.6541 - val_loss: 10.6977 - val_accuracy: 0.3036\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 4.2608 - accuracy: 0.6667 - val_loss: 0.7863 - val_accuracy: 0.6964\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 3.9960 - accuracy: 0.6415 - val_loss: 6.7805 - val_accuracy: 0.6964\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 1s 27ms/step - loss: 5.1285 - accuracy: 0.6101 - val_loss: 0.6746 - val_accuracy: 0.6964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 7/35 [02:46<11:04, 23.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Added **************************************************\n",
            "--------------------------------------------------\n",
            "8 . EfficientNetB4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 7/35 [02:49<11:19, 24.25s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0058873e8c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m# pre_trained_model = model(include_top=False, pooling='avg', input_shape=input_shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mpre_trained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mpre_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNetB4\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m       \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m       \u001b[0mclassifier_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, activation, blocks_args, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         file_hash=file_hash)\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2603\u001b[0m               f, self, skip_mismatch)\n\u001b[1;32m   2604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2605\u001b[0;31m           \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m     \u001b[0;31m# Perform any layer defined finalization of the layer state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0msymbolic_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_legacy_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5i.pyx\u001b[0m in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Results to DataFrame for easy viewing\n",
        "benchmark_df = pd.DataFrame(model_benchmarks)\n",
        "\n",
        "# sort in ascending order of num_model_params column\n",
        "benchmark_df.sort_values('num_model_params', inplace=True)\n",
        "\n",
        "# write results to csv file\n",
        "# benchmark_df.to_csv('/content/drive/MyDrive/CV-EmpathNet/benchmark_df.csv', index=False)\n",
        "benchmark_df.to_csv('/content/drive/MyDrive/CV-EmpathNet/benchmark_new_df.csv', index=False)\n",
        "benchmark_df"
      ],
      "metadata": {
        "id": "PPKbaR1WGUWY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "712077b8-2ecd-437b-a05a-5a5ec7c9eb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           model_name  num_model_params  validation_accuracy  train_accuracy  \\\n",
              "23   MobileNetV3Small            939120             0.696429        0.610063   \n",
              "21        MobileNetV2           2257984             0.696429        0.666667   \n",
              "22   MobileNetV3Large           2996352             0.696429        0.641509   \n",
              "20          MobileNet           3228864             0.696429        0.735849   \n",
              "3      EfficientNetB0           4049571             0.696429        0.654088   \n",
              "24       NASNetMobile           4269716             0.696429        0.660377   \n",
              "11   EfficientNetV2B0           5919312             0.696429        0.616352   \n",
              "4      EfficientNetB1           6575239             0.696429        0.628931   \n",
              "12   EfficientNetV2B1           6931124             0.696429        0.679245   \n",
              "0         DenseNet121           7037504             0.642857        0.679245   \n",
              "5      EfficientNetB2           7768569             0.696429        0.641509   \n",
              "13   EfficientNetV2B2           8769374             0.696429        0.635220   \n",
              "6      EfficientNetB3          10783535             0.696429        0.647799   \n",
              "1         DenseNet169          12642880             0.696429        0.685535   \n",
              "14   EfficientNetV2B3          12930622             0.696429        0.603774   \n",
              "31              VGG16          14714688             0.696429        0.660377   \n",
              "7      EfficientNetB4          17673823             0.696429        0.610063   \n",
              "2         DenseNet201          18321984             0.446429        0.666667   \n",
              "32              VGG19          20024384             0.696429        0.716981   \n",
              "17    EfficientNetV2S          20331360             0.696429        0.622642   \n",
              "33           Xception          20861480             0.696429        0.767296   \n",
              "19        InceptionV3          21802784             0.696429        0.691824   \n",
              "30         ResNet50V2          23564800             0.696429        0.710692   \n",
              "29           ResNet50          23587712             0.696429        0.641509   \n",
              "8      EfficientNetB5          28513527             0.696429        0.610063   \n",
              "9      EfficientNetB6          40960143             0.696429        0.628931   \n",
              "26        ResNet101V2          42626560             0.696429        0.754717   \n",
              "25          ResNet101          42658176             0.696429        0.559748   \n",
              "16    EfficientNetV2M          53150388             0.303571        0.679245   \n",
              "18  InceptionResNetV2          54336736             0.696429        0.723270   \n",
              "28        ResNet152V2          58331648             0.696429        0.716981   \n",
              "27          ResNet152          58370944             0.303571        0.610063   \n",
              "10     EfficientNetB7          64097687             0.696429        0.704403   \n",
              "15    EfficientNetV2L         117746848             0.696429        0.679245   \n",
              "\n",
              "    train_loss   val_loss  \n",
              "23    1.261865   0.641164  \n",
              "21    3.931767   4.132025  \n",
              "22    1.020504   1.215575  \n",
              "20    0.772681   0.838050  \n",
              "3     2.758051   4.060661  \n",
              "24    1.742669   2.048274  \n",
              "11    3.054909   1.487831  \n",
              "4     3.755259   5.526180  \n",
              "12    2.554912   1.696293  \n",
              "0     0.963008   0.653126  \n",
              "5     4.960029   5.174424  \n",
              "13    4.251212   5.159295  \n",
              "6     4.238076   8.877633  \n",
              "1     1.069914   2.758311  \n",
              "14    5.956751   5.856483  \n",
              "31    0.753402   0.936005  \n",
              "7     6.457074  12.464493  \n",
              "2     1.063560   0.807484  \n",
              "32    0.851402   1.660126  \n",
              "17    7.168417   9.962553  \n",
              "33    0.545874   0.877682  \n",
              "19    0.878346   1.367819  \n",
              "30    0.727831   1.128280  \n",
              "29    2.267992   2.204662  \n",
              "8    10.997963  13.233499  \n",
              "9    15.089182   0.647317  \n",
              "26    0.666170   1.286456  \n",
              "25    2.067032   3.175153  \n",
              "16    5.956095   8.798914  \n",
              "18    0.674017   0.963250  \n",
              "28    0.693627   1.022689  \n",
              "27    2.159722   0.907543  \n",
              "10   12.874093   2.736376  \n",
              "15    5.917149   3.508417  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08cddb1c-483c-49f5-a339-76557eee3043\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_model_params</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>MobileNetV3Small</td>\n",
              "      <td>939120</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>1.261865</td>\n",
              "      <td>0.641164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>2257984</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>3.931767</td>\n",
              "      <td>4.132025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>MobileNetV3Large</td>\n",
              "      <td>2996352</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>1.020504</td>\n",
              "      <td>1.215575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>3228864</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.735849</td>\n",
              "      <td>0.772681</td>\n",
              "      <td>0.838050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>4049571</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.654088</td>\n",
              "      <td>2.758051</td>\n",
              "      <td>4.060661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>4269716</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>1.742669</td>\n",
              "      <td>2.048274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>EfficientNetV2B0</td>\n",
              "      <td>5919312</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.616352</td>\n",
              "      <td>3.054909</td>\n",
              "      <td>1.487831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>6575239</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>3.755259</td>\n",
              "      <td>5.526180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>EfficientNetV2B1</td>\n",
              "      <td>6931124</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>2.554912</td>\n",
              "      <td>1.696293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>7037504</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>0.963008</td>\n",
              "      <td>0.653126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>7768569</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>4.960029</td>\n",
              "      <td>5.174424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>EfficientNetV2B2</td>\n",
              "      <td>8769374</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>4.251212</td>\n",
              "      <td>5.159295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>10783535</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.647799</td>\n",
              "      <td>4.238076</td>\n",
              "      <td>8.877633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>12642880</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.685535</td>\n",
              "      <td>1.069914</td>\n",
              "      <td>2.758311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>EfficientNetV2B3</td>\n",
              "      <td>12930622</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.603774</td>\n",
              "      <td>5.956751</td>\n",
              "      <td>5.856483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>14714688</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>0.753402</td>\n",
              "      <td>0.936005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>17673823</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>6.457074</td>\n",
              "      <td>12.464493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>18321984</td>\n",
              "      <td>0.446429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.063560</td>\n",
              "      <td>0.807484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>20024384</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.851402</td>\n",
              "      <td>1.660126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>EfficientNetV2S</td>\n",
              "      <td>20331360</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>7.168417</td>\n",
              "      <td>9.962553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Xception</td>\n",
              "      <td>20861480</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.767296</td>\n",
              "      <td>0.545874</td>\n",
              "      <td>0.877682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>21802784</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.691824</td>\n",
              "      <td>0.878346</td>\n",
              "      <td>1.367819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>23564800</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.710692</td>\n",
              "      <td>0.727831</td>\n",
              "      <td>1.128280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>23587712</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>2.267992</td>\n",
              "      <td>2.204662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>28513527</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>10.997963</td>\n",
              "      <td>13.233499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>40960143</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>15.089182</td>\n",
              "      <td>0.647317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>42626560</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.666170</td>\n",
              "      <td>1.286456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>42658176</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.559748</td>\n",
              "      <td>2.067032</td>\n",
              "      <td>3.175153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetV2M</td>\n",
              "      <td>53150388</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.956095</td>\n",
              "      <td>8.798914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>54336736</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.723270</td>\n",
              "      <td>0.674017</td>\n",
              "      <td>0.963250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>58331648</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.693627</td>\n",
              "      <td>1.022689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>58370944</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>2.159722</td>\n",
              "      <td>0.907543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>64097687</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.704403</td>\n",
              "      <td>12.874093</td>\n",
              "      <td>2.736376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>EfficientNetV2L</td>\n",
              "      <td>117746848</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.917149</td>\n",
              "      <td>3.508417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08cddb1c-483c-49f5-a339-76557eee3043')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-08cddb1c-483c-49f5-a339-76557eee3043 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-08cddb1c-483c-49f5-a339-76557eee3043');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_df.sort_values('validation_accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EPB2_ERizgBN",
        "outputId": "e55c0903-0626-41c1-a0c7-dc4f8428822f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           model_name  num_model_params  validation_accuracy  train_accuracy  \\\n",
              "27          ResNet152          58370944             0.303571        0.610063   \n",
              "16    EfficientNetV2M          53150388             0.303571        0.679245   \n",
              "2         DenseNet201          18321984             0.446429        0.666667   \n",
              "0         DenseNet121           7037504             0.642857        0.679245   \n",
              "17    EfficientNetV2S          20331360             0.696429        0.622642   \n",
              "33           Xception          20861480             0.696429        0.767296   \n",
              "19        InceptionV3          21802784             0.696429        0.691824   \n",
              "30         ResNet50V2          23564800             0.696429        0.710692   \n",
              "29           ResNet50          23587712             0.696429        0.641509   \n",
              "23   MobileNetV3Small            939120             0.696429        0.610063   \n",
              "9      EfficientNetB6          40960143             0.696429        0.628931   \n",
              "26        ResNet101V2          42626560             0.696429        0.754717   \n",
              "25          ResNet101          42658176             0.696429        0.559748   \n",
              "18  InceptionResNetV2          54336736             0.696429        0.723270   \n",
              "28        ResNet152V2          58331648             0.696429        0.716981   \n",
              "32              VGG19          20024384             0.696429        0.716981   \n",
              "8      EfficientNetB5          28513527             0.696429        0.610063   \n",
              "7      EfficientNetB4          17673823             0.696429        0.610063   \n",
              "31              VGG16          14714688             0.696429        0.660377   \n",
              "14   EfficientNetV2B3          12930622             0.696429        0.603774   \n",
              "1         DenseNet169          12642880             0.696429        0.685535   \n",
              "6      EfficientNetB3          10783535             0.696429        0.647799   \n",
              "13   EfficientNetV2B2           8769374             0.696429        0.635220   \n",
              "5      EfficientNetB2           7768569             0.696429        0.641509   \n",
              "12   EfficientNetV2B1           6931124             0.696429        0.679245   \n",
              "4      EfficientNetB1           6575239             0.696429        0.628931   \n",
              "11   EfficientNetV2B0           5919312             0.696429        0.616352   \n",
              "24       NASNetMobile           4269716             0.696429        0.660377   \n",
              "3      EfficientNetB0           4049571             0.696429        0.654088   \n",
              "20          MobileNet           3228864             0.696429        0.735849   \n",
              "22   MobileNetV3Large           2996352             0.696429        0.641509   \n",
              "21        MobileNetV2           2257984             0.696429        0.666667   \n",
              "10     EfficientNetB7          64097687             0.696429        0.704403   \n",
              "15    EfficientNetV2L         117746848             0.696429        0.679245   \n",
              "\n",
              "    train_loss   val_loss  \n",
              "27    2.159722   0.907543  \n",
              "16    5.956095   8.798914  \n",
              "2     1.063560   0.807484  \n",
              "0     0.963008   0.653126  \n",
              "17    7.168417   9.962553  \n",
              "33    0.545874   0.877682  \n",
              "19    0.878346   1.367819  \n",
              "30    0.727831   1.128280  \n",
              "29    2.267992   2.204662  \n",
              "23    1.261865   0.641164  \n",
              "9    15.089182   0.647317  \n",
              "26    0.666170   1.286456  \n",
              "25    2.067032   3.175153  \n",
              "18    0.674017   0.963250  \n",
              "28    0.693627   1.022689  \n",
              "32    0.851402   1.660126  \n",
              "8    10.997963  13.233499  \n",
              "7     6.457074  12.464493  \n",
              "31    0.753402   0.936005  \n",
              "14    5.956751   5.856483  \n",
              "1     1.069914   2.758311  \n",
              "6     4.238076   8.877633  \n",
              "13    4.251212   5.159295  \n",
              "5     4.960029   5.174424  \n",
              "12    2.554912   1.696293  \n",
              "4     3.755259   5.526180  \n",
              "11    3.054909   1.487831  \n",
              "24    1.742669   2.048274  \n",
              "3     2.758051   4.060661  \n",
              "20    0.772681   0.838050  \n",
              "22    1.020504   1.215575  \n",
              "21    3.931767   4.132025  \n",
              "10   12.874093   2.736376  \n",
              "15    5.917149   3.508417  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1006ed98-ab81-4ea8-bac6-3e5a4f77302d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_model_params</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>58370944</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>2.159722</td>\n",
              "      <td>0.907543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetV2M</td>\n",
              "      <td>53150388</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.956095</td>\n",
              "      <td>8.798914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>18321984</td>\n",
              "      <td>0.446429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.063560</td>\n",
              "      <td>0.807484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>7037504</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>0.963008</td>\n",
              "      <td>0.653126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>EfficientNetV2S</td>\n",
              "      <td>20331360</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>7.168417</td>\n",
              "      <td>9.962553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Xception</td>\n",
              "      <td>20861480</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.767296</td>\n",
              "      <td>0.545874</td>\n",
              "      <td>0.877682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>21802784</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.691824</td>\n",
              "      <td>0.878346</td>\n",
              "      <td>1.367819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>23564800</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.710692</td>\n",
              "      <td>0.727831</td>\n",
              "      <td>1.128280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>23587712</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>2.267992</td>\n",
              "      <td>2.204662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>MobileNetV3Small</td>\n",
              "      <td>939120</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>1.261865</td>\n",
              "      <td>0.641164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>40960143</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>15.089182</td>\n",
              "      <td>0.647317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>42626560</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.666170</td>\n",
              "      <td>1.286456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>42658176</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.559748</td>\n",
              "      <td>2.067032</td>\n",
              "      <td>3.175153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>54336736</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.723270</td>\n",
              "      <td>0.674017</td>\n",
              "      <td>0.963250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>58331648</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.693627</td>\n",
              "      <td>1.022689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>20024384</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.851402</td>\n",
              "      <td>1.660126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>28513527</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>10.997963</td>\n",
              "      <td>13.233499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>17673823</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>6.457074</td>\n",
              "      <td>12.464493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>14714688</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>0.753402</td>\n",
              "      <td>0.936005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>EfficientNetV2B3</td>\n",
              "      <td>12930622</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.603774</td>\n",
              "      <td>5.956751</td>\n",
              "      <td>5.856483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>12642880</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.685535</td>\n",
              "      <td>1.069914</td>\n",
              "      <td>2.758311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>10783535</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.647799</td>\n",
              "      <td>4.238076</td>\n",
              "      <td>8.877633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>EfficientNetV2B2</td>\n",
              "      <td>8769374</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>4.251212</td>\n",
              "      <td>5.159295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>7768569</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>4.960029</td>\n",
              "      <td>5.174424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>EfficientNetV2B1</td>\n",
              "      <td>6931124</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>2.554912</td>\n",
              "      <td>1.696293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>6575239</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>3.755259</td>\n",
              "      <td>5.526180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>EfficientNetV2B0</td>\n",
              "      <td>5919312</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.616352</td>\n",
              "      <td>3.054909</td>\n",
              "      <td>1.487831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>4269716</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>1.742669</td>\n",
              "      <td>2.048274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>4049571</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.654088</td>\n",
              "      <td>2.758051</td>\n",
              "      <td>4.060661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>3228864</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.735849</td>\n",
              "      <td>0.772681</td>\n",
              "      <td>0.838050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>MobileNetV3Large</td>\n",
              "      <td>2996352</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>1.020504</td>\n",
              "      <td>1.215575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>2257984</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>3.931767</td>\n",
              "      <td>4.132025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>64097687</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.704403</td>\n",
              "      <td>12.874093</td>\n",
              "      <td>2.736376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>EfficientNetV2L</td>\n",
              "      <td>117746848</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.917149</td>\n",
              "      <td>3.508417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1006ed98-ab81-4ea8-bac6-3e5a4f77302d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1006ed98-ab81-4ea8-bac6-3e5a4f77302d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1006ed98-ab81-4ea8-bac6-3e5a4f77302d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_df.sort_values('train_accuracy')"
      ],
      "metadata": {
        "id": "JDU4WFy-HRbG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0df1cb81-02e2-4dfc-c83d-ccb6d51e7bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           model_name  num_model_params  validation_accuracy  train_accuracy  \\\n",
              "25          ResNet101          42658176             0.696429        0.559748   \n",
              "14   EfficientNetV2B3          12930622             0.696429        0.603774   \n",
              "23   MobileNetV3Small            939120             0.696429        0.610063   \n",
              "27          ResNet152          58370944             0.303571        0.610063   \n",
              "8      EfficientNetB5          28513527             0.696429        0.610063   \n",
              "7      EfficientNetB4          17673823             0.696429        0.610063   \n",
              "11   EfficientNetV2B0           5919312             0.696429        0.616352   \n",
              "17    EfficientNetV2S          20331360             0.696429        0.622642   \n",
              "9      EfficientNetB6          40960143             0.696429        0.628931   \n",
              "4      EfficientNetB1           6575239             0.696429        0.628931   \n",
              "13   EfficientNetV2B2           8769374             0.696429        0.635220   \n",
              "5      EfficientNetB2           7768569             0.696429        0.641509   \n",
              "22   MobileNetV3Large           2996352             0.696429        0.641509   \n",
              "29           ResNet50          23587712             0.696429        0.641509   \n",
              "6      EfficientNetB3          10783535             0.696429        0.647799   \n",
              "3      EfficientNetB0           4049571             0.696429        0.654088   \n",
              "31              VGG16          14714688             0.696429        0.660377   \n",
              "24       NASNetMobile           4269716             0.696429        0.660377   \n",
              "21        MobileNetV2           2257984             0.696429        0.666667   \n",
              "2         DenseNet201          18321984             0.446429        0.666667   \n",
              "12   EfficientNetV2B1           6931124             0.696429        0.679245   \n",
              "16    EfficientNetV2M          53150388             0.303571        0.679245   \n",
              "15    EfficientNetV2L         117746848             0.696429        0.679245   \n",
              "0         DenseNet121           7037504             0.642857        0.679245   \n",
              "1         DenseNet169          12642880             0.696429        0.685535   \n",
              "19        InceptionV3          21802784             0.696429        0.691824   \n",
              "10     EfficientNetB7          64097687             0.696429        0.704403   \n",
              "30         ResNet50V2          23564800             0.696429        0.710692   \n",
              "32              VGG19          20024384             0.696429        0.716981   \n",
              "28        ResNet152V2          58331648             0.696429        0.716981   \n",
              "18  InceptionResNetV2          54336736             0.696429        0.723270   \n",
              "20          MobileNet           3228864             0.696429        0.735849   \n",
              "26        ResNet101V2          42626560             0.696429        0.754717   \n",
              "33           Xception          20861480             0.696429        0.767296   \n",
              "\n",
              "    train_loss   val_loss  \n",
              "25    2.067032   3.175153  \n",
              "14    5.956751   5.856483  \n",
              "23    1.261865   0.641164  \n",
              "27    2.159722   0.907543  \n",
              "8    10.997963  13.233499  \n",
              "7     6.457074  12.464493  \n",
              "11    3.054909   1.487831  \n",
              "17    7.168417   9.962553  \n",
              "9    15.089182   0.647317  \n",
              "4     3.755259   5.526180  \n",
              "13    4.251212   5.159295  \n",
              "5     4.960029   5.174424  \n",
              "22    1.020504   1.215575  \n",
              "29    2.267992   2.204662  \n",
              "6     4.238076   8.877633  \n",
              "3     2.758051   4.060661  \n",
              "31    0.753402   0.936005  \n",
              "24    1.742669   2.048274  \n",
              "21    3.931767   4.132025  \n",
              "2     1.063560   0.807484  \n",
              "12    2.554912   1.696293  \n",
              "16    5.956095   8.798914  \n",
              "15    5.917149   3.508417  \n",
              "0     0.963008   0.653126  \n",
              "1     1.069914   2.758311  \n",
              "19    0.878346   1.367819  \n",
              "10   12.874093   2.736376  \n",
              "30    0.727831   1.128280  \n",
              "32    0.851402   1.660126  \n",
              "28    0.693627   1.022689  \n",
              "18    0.674017   0.963250  \n",
              "20    0.772681   0.838050  \n",
              "26    0.666170   1.286456  \n",
              "33    0.545874   0.877682  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8be5fe9-5987-4204-b387-9a8b8cb9774d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_model_params</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>42658176</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.559748</td>\n",
              "      <td>2.067032</td>\n",
              "      <td>3.175153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>EfficientNetV2B3</td>\n",
              "      <td>12930622</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.603774</td>\n",
              "      <td>5.956751</td>\n",
              "      <td>5.856483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>MobileNetV3Small</td>\n",
              "      <td>939120</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>1.261865</td>\n",
              "      <td>0.641164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>58370944</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>2.159722</td>\n",
              "      <td>0.907543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>28513527</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>10.997963</td>\n",
              "      <td>13.233499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>17673823</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>6.457074</td>\n",
              "      <td>12.464493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>EfficientNetV2B0</td>\n",
              "      <td>5919312</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.616352</td>\n",
              "      <td>3.054909</td>\n",
              "      <td>1.487831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>EfficientNetV2S</td>\n",
              "      <td>20331360</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>7.168417</td>\n",
              "      <td>9.962553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>40960143</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>15.089182</td>\n",
              "      <td>0.647317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>6575239</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>3.755259</td>\n",
              "      <td>5.526180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>EfficientNetV2B2</td>\n",
              "      <td>8769374</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>4.251212</td>\n",
              "      <td>5.159295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>7768569</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>4.960029</td>\n",
              "      <td>5.174424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>MobileNetV3Large</td>\n",
              "      <td>2996352</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>1.020504</td>\n",
              "      <td>1.215575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>23587712</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>2.267992</td>\n",
              "      <td>2.204662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>10783535</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.647799</td>\n",
              "      <td>4.238076</td>\n",
              "      <td>8.877633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>4049571</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.654088</td>\n",
              "      <td>2.758051</td>\n",
              "      <td>4.060661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>14714688</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>0.753402</td>\n",
              "      <td>0.936005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>4269716</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>1.742669</td>\n",
              "      <td>2.048274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>2257984</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>3.931767</td>\n",
              "      <td>4.132025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>18321984</td>\n",
              "      <td>0.446429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.063560</td>\n",
              "      <td>0.807484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>EfficientNetV2B1</td>\n",
              "      <td>6931124</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>2.554912</td>\n",
              "      <td>1.696293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetV2M</td>\n",
              "      <td>53150388</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.956095</td>\n",
              "      <td>8.798914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>EfficientNetV2L</td>\n",
              "      <td>117746848</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.917149</td>\n",
              "      <td>3.508417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>7037504</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>0.963008</td>\n",
              "      <td>0.653126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>12642880</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.685535</td>\n",
              "      <td>1.069914</td>\n",
              "      <td>2.758311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>21802784</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.691824</td>\n",
              "      <td>0.878346</td>\n",
              "      <td>1.367819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>64097687</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.704403</td>\n",
              "      <td>12.874093</td>\n",
              "      <td>2.736376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>23564800</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.710692</td>\n",
              "      <td>0.727831</td>\n",
              "      <td>1.128280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>20024384</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.851402</td>\n",
              "      <td>1.660126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>58331648</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.693627</td>\n",
              "      <td>1.022689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>54336736</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.723270</td>\n",
              "      <td>0.674017</td>\n",
              "      <td>0.963250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>3228864</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.735849</td>\n",
              "      <td>0.772681</td>\n",
              "      <td>0.838050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>42626560</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.666170</td>\n",
              "      <td>1.286456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Xception</td>\n",
              "      <td>20861480</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.767296</td>\n",
              "      <td>0.545874</td>\n",
              "      <td>0.877682</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8be5fe9-5987-4204-b387-9a8b8cb9774d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8be5fe9-5987-4204-b387-9a8b8cb9774d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8be5fe9-5987-4204-b387-9a8b8cb9774d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_df.sort_values('val_loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RozxPsZQKcTO",
        "outputId": "56eaf634-c94c-48ab-c31b-2f0ce8d2b04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           model_name  num_model_params  validation_accuracy  train_accuracy  \\\n",
              "23   MobileNetV3Small            939120             0.696429        0.610063   \n",
              "9      EfficientNetB6          40960143             0.696429        0.628931   \n",
              "0         DenseNet121           7037504             0.642857        0.679245   \n",
              "2         DenseNet201          18321984             0.446429        0.666667   \n",
              "20          MobileNet           3228864             0.696429        0.735849   \n",
              "33           Xception          20861480             0.696429        0.767296   \n",
              "27          ResNet152          58370944             0.303571        0.610063   \n",
              "31              VGG16          14714688             0.696429        0.660377   \n",
              "18  InceptionResNetV2          54336736             0.696429        0.723270   \n",
              "28        ResNet152V2          58331648             0.696429        0.716981   \n",
              "30         ResNet50V2          23564800             0.696429        0.710692   \n",
              "22   MobileNetV3Large           2996352             0.696429        0.641509   \n",
              "26        ResNet101V2          42626560             0.696429        0.754717   \n",
              "19        InceptionV3          21802784             0.696429        0.691824   \n",
              "11   EfficientNetV2B0           5919312             0.696429        0.616352   \n",
              "32              VGG19          20024384             0.696429        0.716981   \n",
              "12   EfficientNetV2B1           6931124             0.696429        0.679245   \n",
              "24       NASNetMobile           4269716             0.696429        0.660377   \n",
              "29           ResNet50          23587712             0.696429        0.641509   \n",
              "10     EfficientNetB7          64097687             0.696429        0.704403   \n",
              "1         DenseNet169          12642880             0.696429        0.685535   \n",
              "25          ResNet101          42658176             0.696429        0.559748   \n",
              "15    EfficientNetV2L         117746848             0.696429        0.679245   \n",
              "3      EfficientNetB0           4049571             0.696429        0.654088   \n",
              "21        MobileNetV2           2257984             0.696429        0.666667   \n",
              "13   EfficientNetV2B2           8769374             0.696429        0.635220   \n",
              "5      EfficientNetB2           7768569             0.696429        0.641509   \n",
              "4      EfficientNetB1           6575239             0.696429        0.628931   \n",
              "14   EfficientNetV2B3          12930622             0.696429        0.603774   \n",
              "16    EfficientNetV2M          53150388             0.303571        0.679245   \n",
              "6      EfficientNetB3          10783535             0.696429        0.647799   \n",
              "17    EfficientNetV2S          20331360             0.696429        0.622642   \n",
              "7      EfficientNetB4          17673823             0.696429        0.610063   \n",
              "8      EfficientNetB5          28513527             0.696429        0.610063   \n",
              "\n",
              "    train_loss   val_loss  \n",
              "23    1.261865   0.641164  \n",
              "9    15.089182   0.647317  \n",
              "0     0.963008   0.653126  \n",
              "2     1.063560   0.807484  \n",
              "20    0.772681   0.838050  \n",
              "33    0.545874   0.877682  \n",
              "27    2.159722   0.907543  \n",
              "31    0.753402   0.936005  \n",
              "18    0.674017   0.963250  \n",
              "28    0.693627   1.022689  \n",
              "30    0.727831   1.128280  \n",
              "22    1.020504   1.215575  \n",
              "26    0.666170   1.286456  \n",
              "19    0.878346   1.367819  \n",
              "11    3.054909   1.487831  \n",
              "32    0.851402   1.660126  \n",
              "12    2.554912   1.696293  \n",
              "24    1.742669   2.048274  \n",
              "29    2.267992   2.204662  \n",
              "10   12.874093   2.736376  \n",
              "1     1.069914   2.758311  \n",
              "25    2.067032   3.175153  \n",
              "15    5.917149   3.508417  \n",
              "3     2.758051   4.060661  \n",
              "21    3.931767   4.132025  \n",
              "13    4.251212   5.159295  \n",
              "5     4.960029   5.174424  \n",
              "4     3.755259   5.526180  \n",
              "14    5.956751   5.856483  \n",
              "16    5.956095   8.798914  \n",
              "6     4.238076   8.877633  \n",
              "17    7.168417   9.962553  \n",
              "7     6.457074  12.464493  \n",
              "8    10.997963  13.233499  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72f3340d-83eb-46a9-adbc-d2dd99b27513\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_model_params</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>MobileNetV3Small</td>\n",
              "      <td>939120</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>1.261865</td>\n",
              "      <td>0.641164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>40960143</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>15.089182</td>\n",
              "      <td>0.647317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>7037504</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>0.963008</td>\n",
              "      <td>0.653126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>18321984</td>\n",
              "      <td>0.446429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.063560</td>\n",
              "      <td>0.807484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>3228864</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.735849</td>\n",
              "      <td>0.772681</td>\n",
              "      <td>0.838050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Xception</td>\n",
              "      <td>20861480</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.767296</td>\n",
              "      <td>0.545874</td>\n",
              "      <td>0.877682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>58370944</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>2.159722</td>\n",
              "      <td>0.907543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>14714688</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>0.753402</td>\n",
              "      <td>0.936005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>54336736</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.723270</td>\n",
              "      <td>0.674017</td>\n",
              "      <td>0.963250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>58331648</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.693627</td>\n",
              "      <td>1.022689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>23564800</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.710692</td>\n",
              "      <td>0.727831</td>\n",
              "      <td>1.128280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>MobileNetV3Large</td>\n",
              "      <td>2996352</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>1.020504</td>\n",
              "      <td>1.215575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>42626560</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.666170</td>\n",
              "      <td>1.286456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>21802784</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.691824</td>\n",
              "      <td>0.878346</td>\n",
              "      <td>1.367819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>EfficientNetV2B0</td>\n",
              "      <td>5919312</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.616352</td>\n",
              "      <td>3.054909</td>\n",
              "      <td>1.487831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>20024384</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.851402</td>\n",
              "      <td>1.660126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>EfficientNetV2B1</td>\n",
              "      <td>6931124</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>2.554912</td>\n",
              "      <td>1.696293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>4269716</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>1.742669</td>\n",
              "      <td>2.048274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>23587712</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>2.267992</td>\n",
              "      <td>2.204662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>64097687</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.704403</td>\n",
              "      <td>12.874093</td>\n",
              "      <td>2.736376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>12642880</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.685535</td>\n",
              "      <td>1.069914</td>\n",
              "      <td>2.758311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>42658176</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.559748</td>\n",
              "      <td>2.067032</td>\n",
              "      <td>3.175153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>EfficientNetV2L</td>\n",
              "      <td>117746848</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.917149</td>\n",
              "      <td>3.508417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>4049571</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.654088</td>\n",
              "      <td>2.758051</td>\n",
              "      <td>4.060661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>2257984</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>3.931767</td>\n",
              "      <td>4.132025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>EfficientNetV2B2</td>\n",
              "      <td>8769374</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>4.251212</td>\n",
              "      <td>5.159295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>7768569</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>4.960029</td>\n",
              "      <td>5.174424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>6575239</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>3.755259</td>\n",
              "      <td>5.526180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>EfficientNetV2B3</td>\n",
              "      <td>12930622</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.603774</td>\n",
              "      <td>5.956751</td>\n",
              "      <td>5.856483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetV2M</td>\n",
              "      <td>53150388</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.956095</td>\n",
              "      <td>8.798914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>10783535</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.647799</td>\n",
              "      <td>4.238076</td>\n",
              "      <td>8.877633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>EfficientNetV2S</td>\n",
              "      <td>20331360</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>7.168417</td>\n",
              "      <td>9.962553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>17673823</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>6.457074</td>\n",
              "      <td>12.464493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>28513527</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>10.997963</td>\n",
              "      <td>13.233499</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72f3340d-83eb-46a9-adbc-d2dd99b27513')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72f3340d-83eb-46a9-adbc-d2dd99b27513 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72f3340d-83eb-46a9-adbc-d2dd99b27513');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_df.sort_values('train_loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t4G2JuAOK6z_",
        "outputId": "3ca160ba-3e1f-44e1-f2f2-61eeb7bb64fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           model_name  num_model_params  validation_accuracy  train_accuracy  \\\n",
              "33           Xception          20861480             0.696429        0.767296   \n",
              "26        ResNet101V2          42626560             0.696429        0.754717   \n",
              "18  InceptionResNetV2          54336736             0.696429        0.723270   \n",
              "28        ResNet152V2          58331648             0.696429        0.716981   \n",
              "30         ResNet50V2          23564800             0.696429        0.710692   \n",
              "31              VGG16          14714688             0.696429        0.660377   \n",
              "20          MobileNet           3228864             0.696429        0.735849   \n",
              "32              VGG19          20024384             0.696429        0.716981   \n",
              "19        InceptionV3          21802784             0.696429        0.691824   \n",
              "0         DenseNet121           7037504             0.642857        0.679245   \n",
              "22   MobileNetV3Large           2996352             0.696429        0.641509   \n",
              "2         DenseNet201          18321984             0.446429        0.666667   \n",
              "1         DenseNet169          12642880             0.696429        0.685535   \n",
              "23   MobileNetV3Small            939120             0.696429        0.610063   \n",
              "24       NASNetMobile           4269716             0.696429        0.660377   \n",
              "25          ResNet101          42658176             0.696429        0.559748   \n",
              "27          ResNet152          58370944             0.303571        0.610063   \n",
              "29           ResNet50          23587712             0.696429        0.641509   \n",
              "12   EfficientNetV2B1           6931124             0.696429        0.679245   \n",
              "3      EfficientNetB0           4049571             0.696429        0.654088   \n",
              "11   EfficientNetV2B0           5919312             0.696429        0.616352   \n",
              "4      EfficientNetB1           6575239             0.696429        0.628931   \n",
              "21        MobileNetV2           2257984             0.696429        0.666667   \n",
              "6      EfficientNetB3          10783535             0.696429        0.647799   \n",
              "13   EfficientNetV2B2           8769374             0.696429        0.635220   \n",
              "5      EfficientNetB2           7768569             0.696429        0.641509   \n",
              "15    EfficientNetV2L         117746848             0.696429        0.679245   \n",
              "16    EfficientNetV2M          53150388             0.303571        0.679245   \n",
              "14   EfficientNetV2B3          12930622             0.696429        0.603774   \n",
              "7      EfficientNetB4          17673823             0.696429        0.610063   \n",
              "17    EfficientNetV2S          20331360             0.696429        0.622642   \n",
              "8      EfficientNetB5          28513527             0.696429        0.610063   \n",
              "10     EfficientNetB7          64097687             0.696429        0.704403   \n",
              "9      EfficientNetB6          40960143             0.696429        0.628931   \n",
              "\n",
              "    train_loss   val_loss  \n",
              "33    0.545874   0.877682  \n",
              "26    0.666170   1.286456  \n",
              "18    0.674017   0.963250  \n",
              "28    0.693627   1.022689  \n",
              "30    0.727831   1.128280  \n",
              "31    0.753402   0.936005  \n",
              "20    0.772681   0.838050  \n",
              "32    0.851402   1.660126  \n",
              "19    0.878346   1.367819  \n",
              "0     0.963008   0.653126  \n",
              "22    1.020504   1.215575  \n",
              "2     1.063560   0.807484  \n",
              "1     1.069914   2.758311  \n",
              "23    1.261865   0.641164  \n",
              "24    1.742669   2.048274  \n",
              "25    2.067032   3.175153  \n",
              "27    2.159722   0.907543  \n",
              "29    2.267992   2.204662  \n",
              "12    2.554912   1.696293  \n",
              "3     2.758051   4.060661  \n",
              "11    3.054909   1.487831  \n",
              "4     3.755259   5.526180  \n",
              "21    3.931767   4.132025  \n",
              "6     4.238076   8.877633  \n",
              "13    4.251212   5.159295  \n",
              "5     4.960029   5.174424  \n",
              "15    5.917149   3.508417  \n",
              "16    5.956095   8.798914  \n",
              "14    5.956751   5.856483  \n",
              "7     6.457074  12.464493  \n",
              "17    7.168417   9.962553  \n",
              "8    10.997963  13.233499  \n",
              "10   12.874093   2.736376  \n",
              "9    15.089182   0.647317  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4dad41c-2c0e-4fc6-baa8-d0c49faffee4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>num_model_params</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Xception</td>\n",
              "      <td>20861480</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.767296</td>\n",
              "      <td>0.545874</td>\n",
              "      <td>0.877682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ResNet101V2</td>\n",
              "      <td>42626560</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.666170</td>\n",
              "      <td>1.286456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>InceptionResNetV2</td>\n",
              "      <td>54336736</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.723270</td>\n",
              "      <td>0.674017</td>\n",
              "      <td>0.963250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ResNet152V2</td>\n",
              "      <td>58331648</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.693627</td>\n",
              "      <td>1.022689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>ResNet50V2</td>\n",
              "      <td>23564800</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.710692</td>\n",
              "      <td>0.727831</td>\n",
              "      <td>1.128280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VGG16</td>\n",
              "      <td>14714688</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>0.753402</td>\n",
              "      <td>0.936005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MobileNet</td>\n",
              "      <td>3228864</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.735849</td>\n",
              "      <td>0.772681</td>\n",
              "      <td>0.838050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>VGG19</td>\n",
              "      <td>20024384</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.851402</td>\n",
              "      <td>1.660126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>InceptionV3</td>\n",
              "      <td>21802784</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.691824</td>\n",
              "      <td>0.878346</td>\n",
              "      <td>1.367819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DenseNet121</td>\n",
              "      <td>7037504</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>0.963008</td>\n",
              "      <td>0.653126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>MobileNetV3Large</td>\n",
              "      <td>2996352</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>1.020504</td>\n",
              "      <td>1.215575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DenseNet201</td>\n",
              "      <td>18321984</td>\n",
              "      <td>0.446429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.063560</td>\n",
              "      <td>0.807484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DenseNet169</td>\n",
              "      <td>12642880</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.685535</td>\n",
              "      <td>1.069914</td>\n",
              "      <td>2.758311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>MobileNetV3Small</td>\n",
              "      <td>939120</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>1.261865</td>\n",
              "      <td>0.641164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NASNetMobile</td>\n",
              "      <td>4269716</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>1.742669</td>\n",
              "      <td>2.048274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ResNet101</td>\n",
              "      <td>42658176</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.559748</td>\n",
              "      <td>2.067032</td>\n",
              "      <td>3.175153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ResNet152</td>\n",
              "      <td>58370944</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>2.159722</td>\n",
              "      <td>0.907543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ResNet50</td>\n",
              "      <td>23587712</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>2.267992</td>\n",
              "      <td>2.204662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>EfficientNetV2B1</td>\n",
              "      <td>6931124</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>2.554912</td>\n",
              "      <td>1.696293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EfficientNetB0</td>\n",
              "      <td>4049571</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.654088</td>\n",
              "      <td>2.758051</td>\n",
              "      <td>4.060661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>EfficientNetV2B0</td>\n",
              "      <td>5919312</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.616352</td>\n",
              "      <td>3.054909</td>\n",
              "      <td>1.487831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EfficientNetB1</td>\n",
              "      <td>6575239</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>3.755259</td>\n",
              "      <td>5.526180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MobileNetV2</td>\n",
              "      <td>2257984</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>3.931767</td>\n",
              "      <td>4.132025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EfficientNetB3</td>\n",
              "      <td>10783535</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.647799</td>\n",
              "      <td>4.238076</td>\n",
              "      <td>8.877633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>EfficientNetV2B2</td>\n",
              "      <td>8769374</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.635220</td>\n",
              "      <td>4.251212</td>\n",
              "      <td>5.159295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>EfficientNetB2</td>\n",
              "      <td>7768569</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.641509</td>\n",
              "      <td>4.960029</td>\n",
              "      <td>5.174424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>EfficientNetV2L</td>\n",
              "      <td>117746848</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.917149</td>\n",
              "      <td>3.508417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>EfficientNetV2M</td>\n",
              "      <td>53150388</td>\n",
              "      <td>0.303571</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>5.956095</td>\n",
              "      <td>8.798914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>EfficientNetV2B3</td>\n",
              "      <td>12930622</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.603774</td>\n",
              "      <td>5.956751</td>\n",
              "      <td>5.856483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EfficientNetB4</td>\n",
              "      <td>17673823</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>6.457074</td>\n",
              "      <td>12.464493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>EfficientNetV2S</td>\n",
              "      <td>20331360</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>7.168417</td>\n",
              "      <td>9.962553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>EfficientNetB5</td>\n",
              "      <td>28513527</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.610063</td>\n",
              "      <td>10.997963</td>\n",
              "      <td>13.233499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>EfficientNetB7</td>\n",
              "      <td>64097687</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.704403</td>\n",
              "      <td>12.874093</td>\n",
              "      <td>2.736376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>EfficientNetB6</td>\n",
              "      <td>40960143</td>\n",
              "      <td>0.696429</td>\n",
              "      <td>0.628931</td>\n",
              "      <td>15.089182</td>\n",
              "      <td>0.647317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4dad41c-2c0e-4fc6-baa8-d0c49faffee4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4dad41c-2c0e-4fc6-baa8-d0c49faffee4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4dad41c-2c0e-4fc6-baa8-d0c49faffee4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following article was referred to for writing the above lines of code\n",
        "# https://towardsdatascience.com/how-to-choose-the-best-keras-pre-trained-model-for-image-classification-b850ca4428d4\n",
        "##################################################################################################################\n"
      ],
      "metadata": {
        "id": "uWPOY07NStk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNet V2"
      ],
      "metadata": {
        "id": "m6i8Q7Y0LGVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "gJxE2EUW27jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_func = tensorflow.keras.applications.mobilenet_v2.preprocess_input"
      ],
      "metadata": {
        "id": "h487ZQJ-27jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "train_gen = train_datagen.flow_from_directory('/content/train', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(train_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1fbbb9-0a6f-4123-af7b-3a92e33160d5",
        "id": "kfXvgtCu27jE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 163 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "test_gen = train_datagen.flow_from_directory('/content/test', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(test_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e13b57-b157-4643-fb64-80737c8bc37e",
        "id": "jZvneyip27jE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 56 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.python.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "\n",
        "# num_classes = 2\n",
        "\n",
        "pretrained_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = Sequential()\n",
        "model.add(pretrained_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "mrvJaW8gggS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001,\n",
        "    # learning_rate=0.00001,\n",
        "    learning_rate=0.000001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optim)"
      ],
      "metadata": {
        "id": "Jzf_REXit88c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  print(layer.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiBystQqudlF",
        "outputId": "18361cc6-b085-4a52-dbd2-d4e88ca14ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#With 0.0001 lr.\n",
        "n_epochs = 1000\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaEkEn3bLlHf",
        "outputId": "b666b73e-c624-4ca5-f258-e3132d9c2cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - 11s 293ms/step - loss: 0.9759 - accuracy: 0.2313 - val_loss: 0.8950 - val_accuracy: 0.3036\n",
            "Epoch 2/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.9352 - accuracy: 0.2250 - val_loss: 0.8590 - val_accuracy: 0.3036\n",
            "Epoch 3/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.8876 - accuracy: 0.2366 - val_loss: 0.8254 - val_accuracy: 0.3036\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.8554 - accuracy: 0.2214 - val_loss: 0.7947 - val_accuracy: 0.3036\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.8056 - accuracy: 0.2519 - val_loss: 0.7677 - val_accuracy: 0.3036\n",
            "Epoch 6/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.7815 - accuracy: 0.2313 - val_loss: 0.7434 - val_accuracy: 0.3036\n",
            "Epoch 7/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.7558 - accuracy: 0.2214 - val_loss: 0.7219 - val_accuracy: 0.3036\n",
            "Epoch 8/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.7249 - accuracy: 0.2366 - val_loss: 0.7039 - val_accuracy: 0.3036\n",
            "Epoch 9/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.7046 - accuracy: 0.2519 - val_loss: 0.6882 - val_accuracy: 0.6964\n",
            "Epoch 10/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.6799 - accuracy: 0.7710 - val_loss: 0.6738 - val_accuracy: 0.6964\n",
            "Epoch 11/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.6600 - accuracy: 0.7750 - val_loss: 0.6615 - val_accuracy: 0.6964\n",
            "Epoch 12/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.6434 - accuracy: 0.7710 - val_loss: 0.6512 - val_accuracy: 0.6964\n",
            "Epoch 13/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.6260 - accuracy: 0.7786 - val_loss: 0.6423 - val_accuracy: 0.6964\n",
            "Epoch 14/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.6237 - accuracy: 0.7481 - val_loss: 0.6349 - val_accuracy: 0.6964\n",
            "Epoch 15/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.6074 - accuracy: 0.7634 - val_loss: 0.6293 - val_accuracy: 0.6964\n",
            "Epoch 16/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5881 - accuracy: 0.7863 - val_loss: 0.6248 - val_accuracy: 0.6964\n",
            "Epoch 17/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5982 - accuracy: 0.7481 - val_loss: 0.6212 - val_accuracy: 0.6964\n",
            "Epoch 18/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5738 - accuracy: 0.7786 - val_loss: 0.6185 - val_accuracy: 0.6964\n",
            "Epoch 19/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5765 - accuracy: 0.7634 - val_loss: 0.6169 - val_accuracy: 0.6964\n",
            "Epoch 20/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5658 - accuracy: 0.7750 - val_loss: 0.6155 - val_accuracy: 0.6964\n",
            "Epoch 21/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5587 - accuracy: 0.7786 - val_loss: 0.6142 - val_accuracy: 0.6964\n",
            "Epoch 22/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5765 - accuracy: 0.7481 - val_loss: 0.6134 - val_accuracy: 0.6964\n",
            "Epoch 23/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5611 - accuracy: 0.7634 - val_loss: 0.6131 - val_accuracy: 0.6964\n",
            "Epoch 24/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5386 - accuracy: 0.7863 - val_loss: 0.6132 - val_accuracy: 0.6964\n",
            "Epoch 25/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5548 - accuracy: 0.7634 - val_loss: 0.6138 - val_accuracy: 0.6964\n",
            "Epoch 26/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5584 - accuracy: 0.7557 - val_loss: 0.6145 - val_accuracy: 0.6964\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5403 - accuracy: 0.7750 - val_loss: 0.6155 - val_accuracy: 0.6964\n",
            "Epoch 28/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5451 - accuracy: 0.7688 - val_loss: 0.6164 - val_accuracy: 0.6964\n",
            "Epoch 29/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5261 - accuracy: 0.7863 - val_loss: 0.6166 - val_accuracy: 0.6964\n",
            "Epoch 30/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5347 - accuracy: 0.7786 - val_loss: 0.6170 - val_accuracy: 0.6964\n",
            "Epoch 31/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5335 - accuracy: 0.7786 - val_loss: 0.6175 - val_accuracy: 0.6964\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5413 - accuracy: 0.7710 - val_loss: 0.6177 - val_accuracy: 0.6964\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5431 - accuracy: 0.7688 - val_loss: 0.6178 - val_accuracy: 0.6964\n",
            "Epoch 34/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5407 - accuracy: 0.7710 - val_loss: 0.6182 - val_accuracy: 0.6964\n",
            "Epoch 35/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5082 - accuracy: 0.8015 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
            "Epoch 36/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.4737 - accuracy: 0.8321 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 37/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5389 - accuracy: 0.7710 - val_loss: 0.6211 - val_accuracy: 0.6964\n",
            "Epoch 38/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5215 - accuracy: 0.7863 - val_loss: 0.6222 - val_accuracy: 0.6964\n",
            "Epoch 39/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.4868 - accuracy: 0.8168 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 40/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5289 - accuracy: 0.7786 - val_loss: 0.6248 - val_accuracy: 0.6964\n",
            "Epoch 41/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5405 - accuracy: 0.7688 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 42/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5194 - accuracy: 0.7863 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 43/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5379 - accuracy: 0.7710 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 44/1000\n",
            "5/5 [==============================] - 0s 106ms/step - loss: 0.5375 - accuracy: 0.7710 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 45/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5557 - accuracy: 0.7557 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 46/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5257 - accuracy: 0.7812 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 47/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5473 - accuracy: 0.7634 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 48/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5111 - accuracy: 0.7939 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 49/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5557 - accuracy: 0.7557 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 50/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5330 - accuracy: 0.7750 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 51/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5011 - accuracy: 0.8015 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 52/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5009 - accuracy: 0.8015 - val_loss: 0.6285 - val_accuracy: 0.6964\n",
            "Epoch 53/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5103 - accuracy: 0.7939 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 54/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.4994 - accuracy: 0.8015 - val_loss: 0.6315 - val_accuracy: 0.6964\n",
            "Epoch 55/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6328 - val_accuracy: 0.6964\n",
            "Epoch 56/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5379 - accuracy: 0.7710 - val_loss: 0.6343 - val_accuracy: 0.6964\n",
            "Epoch 57/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5185 - accuracy: 0.7863 - val_loss: 0.6354 - val_accuracy: 0.6964\n",
            "Epoch 58/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5413 - accuracy: 0.7688 - val_loss: 0.6361 - val_accuracy: 0.6964\n",
            "Epoch 59/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5690 - accuracy: 0.7481 - val_loss: 0.6368 - val_accuracy: 0.6964\n",
            "Epoch 60/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5253 - accuracy: 0.7812 - val_loss: 0.6373 - val_accuracy: 0.6964\n",
            "Epoch 61/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5185 - accuracy: 0.7863 - val_loss: 0.6357 - val_accuracy: 0.6964\n",
            "Epoch 62/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5389 - accuracy: 0.7710 - val_loss: 0.6344 - val_accuracy: 0.6964\n",
            "Epoch 63/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5283 - accuracy: 0.7786 - val_loss: 0.6333 - val_accuracy: 0.6964\n",
            "Epoch 64/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5384 - accuracy: 0.7710 - val_loss: 0.6329 - val_accuracy: 0.6964\n",
            "Epoch 65/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5575 - accuracy: 0.7557 - val_loss: 0.6332 - val_accuracy: 0.6964\n",
            "Epoch 66/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5388 - accuracy: 0.7710 - val_loss: 0.6340 - val_accuracy: 0.6964\n",
            "Epoch 67/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5673 - accuracy: 0.7481 - val_loss: 0.6337 - val_accuracy: 0.6964\n",
            "Epoch 68/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5475 - accuracy: 0.7634 - val_loss: 0.6325 - val_accuracy: 0.6964\n",
            "Epoch 69/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5087 - accuracy: 0.7939 - val_loss: 0.6315 - val_accuracy: 0.6964\n",
            "Epoch 70/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5665 - accuracy: 0.7481 - val_loss: 0.6313 - val_accuracy: 0.6964\n",
            "Epoch 71/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5408 - accuracy: 0.7688 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 72/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5378 - accuracy: 0.7710 - val_loss: 0.6306 - val_accuracy: 0.6964\n",
            "Epoch 73/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5093 - accuracy: 0.7939 - val_loss: 0.6306 - val_accuracy: 0.6964\n",
            "Epoch 74/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5374 - accuracy: 0.7710 - val_loss: 0.6314 - val_accuracy: 0.6964\n",
            "Epoch 75/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5091 - accuracy: 0.7939 - val_loss: 0.6336 - val_accuracy: 0.6964\n",
            "Epoch 76/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5383 - accuracy: 0.7710 - val_loss: 0.6353 - val_accuracy: 0.6964\n",
            "Epoch 77/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5382 - accuracy: 0.7710 - val_loss: 0.6374 - val_accuracy: 0.6964\n",
            "Epoch 78/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5494 - accuracy: 0.7634 - val_loss: 0.6380 - val_accuracy: 0.6964\n",
            "Epoch 79/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5334 - accuracy: 0.7750 - val_loss: 0.6375 - val_accuracy: 0.6964\n",
            "Epoch 80/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6377 - val_accuracy: 0.6964\n",
            "Epoch 81/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5391 - accuracy: 0.7710 - val_loss: 0.6373 - val_accuracy: 0.6964\n",
            "Epoch 82/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5485 - accuracy: 0.7634 - val_loss: 0.6355 - val_accuracy: 0.6964\n",
            "Epoch 83/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5578 - accuracy: 0.7557 - val_loss: 0.6346 - val_accuracy: 0.6964\n",
            "Epoch 84/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5331 - accuracy: 0.7750 - val_loss: 0.6333 - val_accuracy: 0.6964\n",
            "Epoch 85/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5328 - accuracy: 0.7750 - val_loss: 0.6325 - val_accuracy: 0.6964\n",
            "Epoch 86/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5327 - accuracy: 0.7750 - val_loss: 0.6317 - val_accuracy: 0.6964\n",
            "Epoch 87/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5276 - accuracy: 0.7786 - val_loss: 0.6313 - val_accuracy: 0.6964\n",
            "Epoch 88/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.5281 - accuracy: 0.7786 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 89/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5758 - accuracy: 0.7405 - val_loss: 0.6279 - val_accuracy: 0.6964\n",
            "Epoch 90/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4924 - accuracy: 0.8092 - val_loss: 0.6274 - val_accuracy: 0.6964\n",
            "Epoch 91/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5194 - accuracy: 0.7863 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 92/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.4825 - accuracy: 0.8168 - val_loss: 0.6291 - val_accuracy: 0.6964\n",
            "Epoch 93/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5372 - accuracy: 0.7710 - val_loss: 0.6316 - val_accuracy: 0.6964\n",
            "Epoch 94/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5475 - accuracy: 0.7634 - val_loss: 0.6340 - val_accuracy: 0.6964\n",
            "Epoch 95/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5287 - accuracy: 0.7786 - val_loss: 0.6365 - val_accuracy: 0.6964\n",
            "Epoch 96/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5288 - accuracy: 0.7786 - val_loss: 0.6381 - val_accuracy: 0.6964\n",
            "Epoch 97/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6392 - val_accuracy: 0.6964\n",
            "Epoch 98/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5189 - accuracy: 0.7863 - val_loss: 0.6402 - val_accuracy: 0.6964\n",
            "Epoch 99/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5395 - accuracy: 0.7710 - val_loss: 0.6407 - val_accuracy: 0.6964\n",
            "Epoch 100/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6428 - val_accuracy: 0.6964\n",
            "Epoch 101/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5295 - accuracy: 0.7786 - val_loss: 0.6429 - val_accuracy: 0.6964\n",
            "Epoch 102/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.4865 - accuracy: 0.8092 - val_loss: 0.6438 - val_accuracy: 0.6964\n",
            "Epoch 103/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5824 - accuracy: 0.7405 - val_loss: 0.6435 - val_accuracy: 0.6964\n",
            "Epoch 104/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6425 - val_accuracy: 0.6964\n",
            "Epoch 105/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5608 - accuracy: 0.7557 - val_loss: 0.6431 - val_accuracy: 0.6964\n",
            "Epoch 106/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5399 - accuracy: 0.7710 - val_loss: 0.6420 - val_accuracy: 0.6964\n",
            "Epoch 107/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5489 - accuracy: 0.7634 - val_loss: 0.6386 - val_accuracy: 0.6964\n",
            "Epoch 108/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5390 - accuracy: 0.7710 - val_loss: 0.6356 - val_accuracy: 0.6964\n",
            "Epoch 109/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5480 - accuracy: 0.7634 - val_loss: 0.6333 - val_accuracy: 0.6964\n",
            "Epoch 110/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5378 - accuracy: 0.7710 - val_loss: 0.6326 - val_accuracy: 0.6964\n",
            "Epoch 111/1000\n",
            "5/5 [==============================] - 1s 101ms/step - loss: 0.5406 - accuracy: 0.7688 - val_loss: 0.6317 - val_accuracy: 0.6964\n",
            "Epoch 112/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5086 - accuracy: 0.7939 - val_loss: 0.6312 - val_accuracy: 0.6964\n",
            "Epoch 113/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5192 - accuracy: 0.7863 - val_loss: 0.6320 - val_accuracy: 0.6964\n",
            "Epoch 114/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5283 - accuracy: 0.7786 - val_loss: 0.6341 - val_accuracy: 0.6964\n",
            "Epoch 115/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5186 - accuracy: 0.7863 - val_loss: 0.6366 - val_accuracy: 0.6964\n",
            "Epoch 116/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5279 - accuracy: 0.7786 - val_loss: 0.6376 - val_accuracy: 0.6964\n",
            "Epoch 117/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.4985 - accuracy: 0.8015 - val_loss: 0.6376 - val_accuracy: 0.6964\n",
            "Epoch 118/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5389 - accuracy: 0.7710 - val_loss: 0.6385 - val_accuracy: 0.6964\n",
            "Epoch 119/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5296 - accuracy: 0.7786 - val_loss: 0.6391 - val_accuracy: 0.6964\n",
            "Epoch 120/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5338 - accuracy: 0.7750 - val_loss: 0.6399 - val_accuracy: 0.6964\n",
            "Epoch 121/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5707 - accuracy: 0.7481 - val_loss: 0.6395 - val_accuracy: 0.6964\n",
            "Epoch 122/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5701 - accuracy: 0.7481 - val_loss: 0.6401 - val_accuracy: 0.6964\n",
            "Epoch 123/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5493 - accuracy: 0.7634 - val_loss: 0.6407 - val_accuracy: 0.6964\n",
            "Epoch 124/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5601 - accuracy: 0.7557 - val_loss: 0.6411 - val_accuracy: 0.6964\n",
            "Epoch 125/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5497 - accuracy: 0.7634 - val_loss: 0.6415 - val_accuracy: 0.6964\n",
            "Epoch 126/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5292 - accuracy: 0.7786 - val_loss: 0.6419 - val_accuracy: 0.6964\n",
            "Epoch 127/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.4990 - accuracy: 0.8015 - val_loss: 0.6415 - val_accuracy: 0.6964\n",
            "Epoch 128/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5188 - accuracy: 0.7863 - val_loss: 0.6425 - val_accuracy: 0.6964\n",
            "Epoch 129/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5608 - accuracy: 0.7557 - val_loss: 0.6428 - val_accuracy: 0.6964\n",
            "Epoch 130/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6437 - val_accuracy: 0.6964\n",
            "Epoch 131/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5509 - accuracy: 0.7634 - val_loss: 0.6437 - val_accuracy: 0.6964\n",
            "Epoch 132/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.5617 - accuracy: 0.7557 - val_loss: 0.6420 - val_accuracy: 0.6964\n",
            "Epoch 133/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6411 - val_accuracy: 0.6964\n",
            "Epoch 134/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5707 - accuracy: 0.7481 - val_loss: 0.6393 - val_accuracy: 0.6964\n",
            "Epoch 135/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5687 - accuracy: 0.7481 - val_loss: 0.6349 - val_accuracy: 0.6964\n",
            "Epoch 136/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5473 - accuracy: 0.7634 - val_loss: 0.6315 - val_accuracy: 0.6964\n",
            "Epoch 137/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5327 - accuracy: 0.7750 - val_loss: 0.6295 - val_accuracy: 0.6964\n",
            "Epoch 138/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5279 - accuracy: 0.7786 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 139/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5103 - accuracy: 0.7939 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 140/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5478 - accuracy: 0.7634 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 141/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5404 - accuracy: 0.7688 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 142/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5118 - accuracy: 0.7939 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 143/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5332 - accuracy: 0.7750 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 144/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5474 - accuracy: 0.7634 - val_loss: 0.6253 - val_accuracy: 0.6964\n",
            "Epoch 145/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5328 - accuracy: 0.7750 - val_loss: 0.6232 - val_accuracy: 0.6964\n",
            "Epoch 146/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5205 - accuracy: 0.7863 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 147/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5408 - accuracy: 0.7688 - val_loss: 0.6233 - val_accuracy: 0.6964\n",
            "Epoch 148/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5406 - accuracy: 0.7688 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 149/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5405 - accuracy: 0.7688 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 150/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5295 - accuracy: 0.7786 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 151/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5643 - accuracy: 0.7481 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 152/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5197 - accuracy: 0.7863 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 153/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5101 - accuracy: 0.7939 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 154/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5191 - accuracy: 0.7863 - val_loss: 0.6280 - val_accuracy: 0.6964\n",
            "Epoch 155/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.4818 - accuracy: 0.8168 - val_loss: 0.6299 - val_accuracy: 0.6964\n",
            "Epoch 156/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5404 - accuracy: 0.7688 - val_loss: 0.6314 - val_accuracy: 0.6964\n",
            "Epoch 157/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5326 - accuracy: 0.7750 - val_loss: 0.6322 - val_accuracy: 0.6964\n",
            "Epoch 158/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5381 - accuracy: 0.7710 - val_loss: 0.6318 - val_accuracy: 0.6964\n",
            "Epoch 159/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5181 - accuracy: 0.7863 - val_loss: 0.6319 - val_accuracy: 0.6964\n",
            "Epoch 160/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5273 - accuracy: 0.7786 - val_loss: 0.6314 - val_accuracy: 0.6964\n",
            "Epoch 161/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5088 - accuracy: 0.7939 - val_loss: 0.6308 - val_accuracy: 0.6964\n",
            "Epoch 162/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5407 - accuracy: 0.7688 - val_loss: 0.6295 - val_accuracy: 0.6964\n",
            "Epoch 163/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5091 - accuracy: 0.7939 - val_loss: 0.6302 - val_accuracy: 0.6964\n",
            "Epoch 164/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5372 - accuracy: 0.7710 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 165/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5093 - accuracy: 0.7939 - val_loss: 0.6296 - val_accuracy: 0.6964\n",
            "Epoch 166/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5762 - accuracy: 0.7405 - val_loss: 0.6302 - val_accuracy: 0.6964\n",
            "Epoch 167/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5001 - accuracy: 0.8015 - val_loss: 0.6306 - val_accuracy: 0.6964\n",
            "Epoch 168/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5277 - accuracy: 0.7786 - val_loss: 0.6301 - val_accuracy: 0.6964\n",
            "Epoch 169/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5404 - accuracy: 0.7688 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 170/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5566 - accuracy: 0.7557 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 171/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5470 - accuracy: 0.7634 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 172/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5094 - accuracy: 0.7939 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 173/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5281 - accuracy: 0.7786 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 174/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5402 - accuracy: 0.7688 - val_loss: 0.6287 - val_accuracy: 0.6964\n",
            "Epoch 175/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5288 - accuracy: 0.7786 - val_loss: 0.6296 - val_accuracy: 0.6964\n",
            "Epoch 176/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5377 - accuracy: 0.7710 - val_loss: 0.6302 - val_accuracy: 0.6964\n",
            "Epoch 177/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5371 - accuracy: 0.7710 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 178/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5564 - accuracy: 0.7557 - val_loss: 0.6301 - val_accuracy: 0.6964\n",
            "Epoch 179/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5329 - accuracy: 0.7750 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 180/1000\n",
            "5/5 [==============================] - 0s 106ms/step - loss: 0.5566 - accuracy: 0.7557 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 181/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5193 - accuracy: 0.7863 - val_loss: 0.6272 - val_accuracy: 0.6964\n",
            "Epoch 182/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5467 - accuracy: 0.7634 - val_loss: 0.6242 - val_accuracy: 0.6964\n",
            "Epoch 183/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5373 - accuracy: 0.7710 - val_loss: 0.6221 - val_accuracy: 0.6964\n",
            "Epoch 184/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5638 - accuracy: 0.7481 - val_loss: 0.6216 - val_accuracy: 0.6964\n",
            "Epoch 185/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5215 - accuracy: 0.7863 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 186/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5555 - accuracy: 0.7557 - val_loss: 0.6216 - val_accuracy: 0.6964\n",
            "Epoch 187/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5382 - accuracy: 0.7710 - val_loss: 0.6215 - val_accuracy: 0.6964\n",
            "Epoch 188/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5297 - accuracy: 0.7786 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 189/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5108 - accuracy: 0.7939 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 190/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5464 - accuracy: 0.7634 - val_loss: 0.6278 - val_accuracy: 0.6964\n",
            "Epoch 191/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5188 - accuracy: 0.7863 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 192/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5377 - accuracy: 0.7710 - val_loss: 0.6286 - val_accuracy: 0.6964\n",
            "Epoch 193/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5276 - accuracy: 0.7786 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 194/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5377 - accuracy: 0.7710 - val_loss: 0.6296 - val_accuracy: 0.6964\n",
            "Epoch 195/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.4997 - accuracy: 0.8015 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 196/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5466 - accuracy: 0.7634 - val_loss: 0.6305 - val_accuracy: 0.6964\n",
            "Epoch 197/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5181 - accuracy: 0.7863 - val_loss: 0.6308 - val_accuracy: 0.6964\n",
            "Epoch 198/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5186 - accuracy: 0.7863 - val_loss: 0.6316 - val_accuracy: 0.6964\n",
            "Epoch 199/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5184 - accuracy: 0.7863 - val_loss: 0.6309 - val_accuracy: 0.6964\n",
            "Epoch 200/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5089 - accuracy: 0.7939 - val_loss: 0.6312 - val_accuracy: 0.6964\n",
            "Epoch 201/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5663 - accuracy: 0.7481 - val_loss: 0.6296 - val_accuracy: 0.6964\n",
            "Epoch 202/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5568 - accuracy: 0.7557 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 203/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5005 - accuracy: 0.8015 - val_loss: 0.6289 - val_accuracy: 0.6964\n",
            "Epoch 204/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5325 - accuracy: 0.7750 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 205/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5177 - accuracy: 0.7863 - val_loss: 0.6305 - val_accuracy: 0.6964\n",
            "Epoch 206/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5377 - accuracy: 0.7710 - val_loss: 0.6294 - val_accuracy: 0.6964\n",
            "Epoch 207/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5284 - accuracy: 0.7786 - val_loss: 0.6288 - val_accuracy: 0.6964\n",
            "Epoch 208/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5375 - accuracy: 0.7710 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 209/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5381 - accuracy: 0.7710 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 210/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5572 - accuracy: 0.7557 - val_loss: 0.6313 - val_accuracy: 0.6964\n",
            "Epoch 211/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5570 - accuracy: 0.7557 - val_loss: 0.6318 - val_accuracy: 0.6964\n",
            "Epoch 212/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5567 - accuracy: 0.7557 - val_loss: 0.6312 - val_accuracy: 0.6964\n",
            "Epoch 213/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5473 - accuracy: 0.7634 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 214/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4995 - accuracy: 0.8015 - val_loss: 0.6304 - val_accuracy: 0.6964\n",
            "Epoch 215/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5275 - accuracy: 0.7786 - val_loss: 0.6306 - val_accuracy: 0.6964\n",
            "Epoch 216/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5468 - accuracy: 0.7634 - val_loss: 0.6304 - val_accuracy: 0.6964\n",
            "Epoch 217/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5559 - accuracy: 0.7557 - val_loss: 0.6284 - val_accuracy: 0.6964\n",
            "Epoch 218/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5323 - accuracy: 0.7750 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 219/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5466 - accuracy: 0.7634 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 220/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5104 - accuracy: 0.7939 - val_loss: 0.6270 - val_accuracy: 0.6964\n",
            "Epoch 221/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5098 - accuracy: 0.7939 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 222/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5324 - accuracy: 0.7750 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 223/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5563 - accuracy: 0.7557 - val_loss: 0.6287 - val_accuracy: 0.6964\n",
            "Epoch 224/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5335 - accuracy: 0.7750 - val_loss: 0.6306 - val_accuracy: 0.6964\n",
            "Epoch 225/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5403 - accuracy: 0.7688 - val_loss: 0.6309 - val_accuracy: 0.6964\n",
            "Epoch 226/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.4999 - accuracy: 0.8015 - val_loss: 0.6310 - val_accuracy: 0.6964\n",
            "Epoch 227/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5092 - accuracy: 0.7939 - val_loss: 0.6317 - val_accuracy: 0.6964\n",
            "Epoch 228/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.5379 - accuracy: 0.7710 - val_loss: 0.6332 - val_accuracy: 0.6964\n",
            "Epoch 229/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5478 - accuracy: 0.7634 - val_loss: 0.6355 - val_accuracy: 0.6964\n",
            "Epoch 230/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5184 - accuracy: 0.7863 - val_loss: 0.6367 - val_accuracy: 0.6964\n",
            "Epoch 231/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5176 - accuracy: 0.7863 - val_loss: 0.6376 - val_accuracy: 0.6964\n",
            "Epoch 232/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5488 - accuracy: 0.7634 - val_loss: 0.6366 - val_accuracy: 0.6964\n",
            "Epoch 233/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5383 - accuracy: 0.7710 - val_loss: 0.6356 - val_accuracy: 0.6964\n",
            "Epoch 234/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5577 - accuracy: 0.7557 - val_loss: 0.6339 - val_accuracy: 0.6964\n",
            "Epoch 235/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5410 - accuracy: 0.7688 - val_loss: 0.6316 - val_accuracy: 0.6964\n",
            "Epoch 236/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5478 - accuracy: 0.7634 - val_loss: 0.6299 - val_accuracy: 0.6964\n",
            "Epoch 237/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5658 - accuracy: 0.7481 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 238/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6281 - val_accuracy: 0.6964\n",
            "Epoch 239/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5328 - accuracy: 0.7750 - val_loss: 0.6286 - val_accuracy: 0.6964\n",
            "Epoch 240/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5747 - accuracy: 0.7405 - val_loss: 0.6278 - val_accuracy: 0.6964\n",
            "Epoch 241/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5653 - accuracy: 0.7481 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 242/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5326 - accuracy: 0.7750 - val_loss: 0.6272 - val_accuracy: 0.6964\n",
            "Epoch 243/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5101 - accuracy: 0.7939 - val_loss: 0.6272 - val_accuracy: 0.6964\n",
            "Epoch 244/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5104 - accuracy: 0.7939 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 245/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5286 - accuracy: 0.7786 - val_loss: 0.6242 - val_accuracy: 0.6964\n",
            "Epoch 246/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5563 - accuracy: 0.7557 - val_loss: 0.6203 - val_accuracy: 0.6964\n",
            "Epoch 247/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5300 - accuracy: 0.7786 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
            "Epoch 248/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5476 - accuracy: 0.7634 - val_loss: 0.6184 - val_accuracy: 0.6964\n",
            "Epoch 249/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5633 - accuracy: 0.7481 - val_loss: 0.6184 - val_accuracy: 0.6964\n",
            "Epoch 250/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5144 - accuracy: 0.7939 - val_loss: 0.6198 - val_accuracy: 0.6964\n",
            "Epoch 251/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5472 - accuracy: 0.7634 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 252/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5379 - accuracy: 0.7710 - val_loss: 0.6246 - val_accuracy: 0.6964\n",
            "Epoch 253/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5727 - accuracy: 0.7405 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 254/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5379 - accuracy: 0.7710 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 255/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5462 - accuracy: 0.7634 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 256/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5375 - accuracy: 0.7710 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 257/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5282 - accuracy: 0.7786 - val_loss: 0.6276 - val_accuracy: 0.6964\n",
            "Epoch 258/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5279 - accuracy: 0.7786 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 259/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5401 - accuracy: 0.7688 - val_loss: 0.6295 - val_accuracy: 0.6964\n",
            "Epoch 260/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5097 - accuracy: 0.7939 - val_loss: 0.6313 - val_accuracy: 0.6964\n",
            "Epoch 261/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5191 - accuracy: 0.7863 - val_loss: 0.6333 - val_accuracy: 0.6964\n",
            "Epoch 262/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5408 - accuracy: 0.7688 - val_loss: 0.6346 - val_accuracy: 0.6964\n",
            "Epoch 263/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5327 - accuracy: 0.7750 - val_loss: 0.6347 - val_accuracy: 0.6964\n",
            "Epoch 264/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5328 - accuracy: 0.7750 - val_loss: 0.6346 - val_accuracy: 0.6964\n",
            "Epoch 265/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5408 - accuracy: 0.7688 - val_loss: 0.6343 - val_accuracy: 0.6964\n",
            "Epoch 266/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5774 - accuracy: 0.7405 - val_loss: 0.6342 - val_accuracy: 0.6964\n",
            "Epoch 267/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5275 - accuracy: 0.7786 - val_loss: 0.6347 - val_accuracy: 0.6964\n",
            "Epoch 268/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5676 - accuracy: 0.7481 - val_loss: 0.6343 - val_accuracy: 0.6964\n",
            "Epoch 269/1000\n",
            "5/5 [==============================] - 0s 107ms/step - loss: 0.5282 - accuracy: 0.7786 - val_loss: 0.6326 - val_accuracy: 0.6964\n",
            "Epoch 270/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5481 - accuracy: 0.7634 - val_loss: 0.6301 - val_accuracy: 0.6964\n",
            "Epoch 271/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5654 - accuracy: 0.7481 - val_loss: 0.6281 - val_accuracy: 0.6964\n",
            "Epoch 272/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5467 - accuracy: 0.7634 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 273/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5248 - accuracy: 0.7812 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 274/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5284 - accuracy: 0.7786 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 275/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5737 - accuracy: 0.7405 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 276/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5285 - accuracy: 0.7786 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 277/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5376 - accuracy: 0.7710 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 278/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5401 - accuracy: 0.7688 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 279/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5550 - accuracy: 0.7557 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 280/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5285 - accuracy: 0.7786 - val_loss: 0.6248 - val_accuracy: 0.6964\n",
            "Epoch 281/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5373 - accuracy: 0.7710 - val_loss: 0.6253 - val_accuracy: 0.6964\n",
            "Epoch 282/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5400 - accuracy: 0.7688 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 283/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5400 - accuracy: 0.7688 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 284/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5283 - accuracy: 0.7786 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 285/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5647 - accuracy: 0.7481 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 286/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.4741 - accuracy: 0.8244 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 287/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5292 - accuracy: 0.7786 - val_loss: 0.6237 - val_accuracy: 0.6964\n",
            "Epoch 288/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5554 - accuracy: 0.7557 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
            "Epoch 289/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5465 - accuracy: 0.7634 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 290/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5552 - accuracy: 0.7557 - val_loss: 0.6232 - val_accuracy: 0.6964\n",
            "Epoch 291/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5026 - accuracy: 0.8015 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 292/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5285 - accuracy: 0.7786 - val_loss: 0.6233 - val_accuracy: 0.6964\n",
            "Epoch 293/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5115 - accuracy: 0.7939 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 294/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5466 - accuracy: 0.7634 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 295/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5327 - accuracy: 0.7750 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 296/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5556 - accuracy: 0.7557 - val_loss: 0.6274 - val_accuracy: 0.6964\n",
            "Epoch 297/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5366 - accuracy: 0.7710 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 298/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5247 - accuracy: 0.7812 - val_loss: 0.6294 - val_accuracy: 0.6964\n",
            "Epoch 299/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5558 - accuracy: 0.7557 - val_loss: 0.6291 - val_accuracy: 0.6964\n",
            "Epoch 300/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4806 - accuracy: 0.8168 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 301/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6289 - val_accuracy: 0.6964\n",
            "Epoch 302/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4989 - accuracy: 0.8015 - val_loss: 0.6303 - val_accuracy: 0.6964\n",
            "Epoch 303/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5372 - accuracy: 0.7710 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 304/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5402 - accuracy: 0.7688 - val_loss: 0.6308 - val_accuracy: 0.6964\n",
            "Epoch 305/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5660 - accuracy: 0.7481 - val_loss: 0.6312 - val_accuracy: 0.6964\n",
            "Epoch 306/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5373 - accuracy: 0.7710 - val_loss: 0.6313 - val_accuracy: 0.6964\n",
            "Epoch 307/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5278 - accuracy: 0.7786 - val_loss: 0.6305 - val_accuracy: 0.6964\n",
            "Epoch 308/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5468 - accuracy: 0.7634 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 309/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5464 - accuracy: 0.7634 - val_loss: 0.6285 - val_accuracy: 0.6964\n",
            "Epoch 310/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5095 - accuracy: 0.7939 - val_loss: 0.6294 - val_accuracy: 0.6964\n",
            "Epoch 311/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5466 - accuracy: 0.7634 - val_loss: 0.6304 - val_accuracy: 0.6964\n",
            "Epoch 312/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5565 - accuracy: 0.7557 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 313/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5185 - accuracy: 0.7863 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 314/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5557 - accuracy: 0.7557 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 315/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5322 - accuracy: 0.7750 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 316/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5466 - accuracy: 0.7634 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 317/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5399 - accuracy: 0.7688 - val_loss: 0.6254 - val_accuracy: 0.6964\n",
            "Epoch 318/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5281 - accuracy: 0.7786 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 319/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5553 - accuracy: 0.7557 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 320/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5324 - accuracy: 0.7750 - val_loss: 0.6285 - val_accuracy: 0.6964\n",
            "Epoch 321/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5561 - accuracy: 0.7557 - val_loss: 0.6280 - val_accuracy: 0.6964\n",
            "Epoch 322/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5273 - accuracy: 0.7786 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 323/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5372 - accuracy: 0.7710 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 324/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5375 - accuracy: 0.7710 - val_loss: 0.6255 - val_accuracy: 0.6964\n",
            "Epoch 325/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5553 - accuracy: 0.7557 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 326/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5376 - accuracy: 0.7710 - val_loss: 0.6218 - val_accuracy: 0.6964\n",
            "Epoch 327/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5055 - accuracy: 0.8015 - val_loss: 0.6197 - val_accuracy: 0.6964\n",
            "Epoch 328/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5222 - accuracy: 0.7863 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 329/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5298 - accuracy: 0.7786 - val_loss: 0.6210 - val_accuracy: 0.6964\n",
            "Epoch 330/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5121 - accuracy: 0.7939 - val_loss: 0.6219 - val_accuracy: 0.6964\n",
            "Epoch 331/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5201 - accuracy: 0.7863 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
            "Epoch 332/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5638 - accuracy: 0.7481 - val_loss: 0.6245 - val_accuracy: 0.6964\n",
            "Epoch 333/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5465 - accuracy: 0.7634 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 334/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5459 - accuracy: 0.7634 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
            "Epoch 335/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5637 - accuracy: 0.7481 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 336/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5329 - accuracy: 0.7750 - val_loss: 0.6226 - val_accuracy: 0.6964\n",
            "Epoch 337/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5369 - accuracy: 0.7710 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
            "Epoch 338/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5325 - accuracy: 0.7750 - val_loss: 0.6245 - val_accuracy: 0.6964\n",
            "Epoch 339/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5465 - accuracy: 0.7634 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 340/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5195 - accuracy: 0.7863 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 341/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5561 - accuracy: 0.7557 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 342/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5469 - accuracy: 0.7634 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 343/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5092 - accuracy: 0.7939 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 344/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 345/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5322 - accuracy: 0.7750 - val_loss: 0.6253 - val_accuracy: 0.6964\n",
            "Epoch 346/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5819 - accuracy: 0.7328 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 347/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5397 - accuracy: 0.7688 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
            "Epoch 348/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5326 - accuracy: 0.7750 - val_loss: 0.6237 - val_accuracy: 0.6964\n",
            "Epoch 349/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5461 - accuracy: 0.7634 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 350/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5331 - accuracy: 0.7750 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 351/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5325 - accuracy: 0.7750 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 352/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5101 - accuracy: 0.7939 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 353/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 354/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.4834 - accuracy: 0.8168 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 355/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5553 - accuracy: 0.7557 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 356/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5323 - accuracy: 0.7750 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 357/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5399 - accuracy: 0.7688 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 358/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5368 - accuracy: 0.7710 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 359/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5378 - accuracy: 0.7710 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 360/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5279 - accuracy: 0.7786 - val_loss: 0.6254 - val_accuracy: 0.6964\n",
            "Epoch 361/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5456 - accuracy: 0.7634 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 362/1000\n",
            "5/5 [==============================] - 0s 111ms/step - loss: 0.5198 - accuracy: 0.7863 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 363/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5549 - accuracy: 0.7557 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 364/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5324 - accuracy: 0.7750 - val_loss: 0.6246 - val_accuracy: 0.6964\n",
            "Epoch 365/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5009 - accuracy: 0.8015 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 366/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5557 - accuracy: 0.7557 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 367/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5180 - accuracy: 0.7863 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 368/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5371 - accuracy: 0.7710 - val_loss: 0.6304 - val_accuracy: 0.6964\n",
            "Epoch 369/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5180 - accuracy: 0.7863 - val_loss: 0.6310 - val_accuracy: 0.6964\n",
            "Epoch 370/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5278 - accuracy: 0.7786 - val_loss: 0.6317 - val_accuracy: 0.6964\n",
            "Epoch 371/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5758 - accuracy: 0.7405 - val_loss: 0.6318 - val_accuracy: 0.6964\n",
            "Epoch 372/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5665 - accuracy: 0.7481 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 373/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.4983 - accuracy: 0.8015 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 374/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5375 - accuracy: 0.7710 - val_loss: 0.6272 - val_accuracy: 0.6964\n",
            "Epoch 375/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5401 - accuracy: 0.7688 - val_loss: 0.6253 - val_accuracy: 0.6964\n",
            "Epoch 376/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5548 - accuracy: 0.7557 - val_loss: 0.6226 - val_accuracy: 0.6964\n",
            "Epoch 377/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5402 - accuracy: 0.7688 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 378/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5031 - accuracy: 0.8015 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 379/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5107 - accuracy: 0.7939 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 380/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5550 - accuracy: 0.7557 - val_loss: 0.6253 - val_accuracy: 0.6964\n",
            "Epoch 381/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5473 - accuracy: 0.7634 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 382/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5553 - accuracy: 0.7557 - val_loss: 0.6287 - val_accuracy: 0.6964\n",
            "Epoch 383/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5461 - accuracy: 0.7634 - val_loss: 0.6294 - val_accuracy: 0.6964\n",
            "Epoch 384/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5379 - accuracy: 0.7710 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 385/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5272 - accuracy: 0.7786 - val_loss: 0.6289 - val_accuracy: 0.6964\n",
            "Epoch 386/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5174 - accuracy: 0.7863 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 387/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5286 - accuracy: 0.7786 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 388/1000\n",
            "5/5 [==============================] - 0s 109ms/step - loss: 0.5377 - accuracy: 0.7710 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 389/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5373 - accuracy: 0.7710 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 390/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5108 - accuracy: 0.7939 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 391/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5367 - accuracy: 0.7710 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 392/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5189 - accuracy: 0.7863 - val_loss: 0.6274 - val_accuracy: 0.6964\n",
            "Epoch 393/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5324 - accuracy: 0.7750 - val_loss: 0.6295 - val_accuracy: 0.6964\n",
            "Epoch 394/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5650 - accuracy: 0.7481 - val_loss: 0.6291 - val_accuracy: 0.6964\n",
            "Epoch 395/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5272 - accuracy: 0.7786 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 396/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5278 - accuracy: 0.7786 - val_loss: 0.6303 - val_accuracy: 0.6964\n",
            "Epoch 397/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5470 - accuracy: 0.7634 - val_loss: 0.6276 - val_accuracy: 0.6964\n",
            "Epoch 398/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5363 - accuracy: 0.7710 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 399/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5367 - accuracy: 0.7710 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 400/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5181 - accuracy: 0.7863 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 401/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5396 - accuracy: 0.7688 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 402/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5321 - accuracy: 0.7750 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 403/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 404/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5373 - accuracy: 0.7710 - val_loss: 0.6249 - val_accuracy: 0.6964\n",
            "Epoch 405/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5638 - accuracy: 0.7481 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 406/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5281 - accuracy: 0.7786 - val_loss: 0.6232 - val_accuracy: 0.6964\n",
            "Epoch 407/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5553 - accuracy: 0.7557 - val_loss: 0.6216 - val_accuracy: 0.6964\n",
            "Epoch 408/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5401 - accuracy: 0.7688 - val_loss: 0.6205 - val_accuracy: 0.6964\n",
            "Epoch 409/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5126 - accuracy: 0.7939 - val_loss: 0.6201 - val_accuracy: 0.6964\n",
            "Epoch 410/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5376 - accuracy: 0.7710 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 411/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5131 - accuracy: 0.7939 - val_loss: 0.6213 - val_accuracy: 0.6964\n",
            "Epoch 412/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5401 - accuracy: 0.7688 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 413/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5110 - accuracy: 0.7939 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 414/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5320 - accuracy: 0.7750 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 415/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5367 - accuracy: 0.7710 - val_loss: 0.6249 - val_accuracy: 0.6964\n",
            "Epoch 416/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5825 - accuracy: 0.7328 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 417/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5322 - accuracy: 0.7750 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 418/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5554 - accuracy: 0.7557 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 419/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5321 - accuracy: 0.7750 - val_loss: 0.6254 - val_accuracy: 0.6964\n",
            "Epoch 420/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5001 - accuracy: 0.8015 - val_loss: 0.6270 - val_accuracy: 0.6964\n",
            "Epoch 421/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5321 - accuracy: 0.7750 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 422/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5464 - accuracy: 0.7634 - val_loss: 0.6286 - val_accuracy: 0.6964\n",
            "Epoch 423/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5364 - accuracy: 0.7710 - val_loss: 0.6281 - val_accuracy: 0.6964\n",
            "Epoch 424/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5314 - accuracy: 0.7750 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 425/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5729 - accuracy: 0.7405 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 426/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5199 - accuracy: 0.7863 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 427/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5400 - accuracy: 0.7688 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 428/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5379 - accuracy: 0.7710 - val_loss: 0.6213 - val_accuracy: 0.6964\n",
            "Epoch 429/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5371 - accuracy: 0.7710 - val_loss: 0.6212 - val_accuracy: 0.6964\n",
            "Epoch 430/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5288 - accuracy: 0.7786 - val_loss: 0.6223 - val_accuracy: 0.6964\n",
            "Epoch 431/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5252 - accuracy: 0.7812 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 432/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5730 - accuracy: 0.7405 - val_loss: 0.6246 - val_accuracy: 0.6964\n",
            "Epoch 433/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 434/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5278 - accuracy: 0.7786 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 435/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5324 - accuracy: 0.7750 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 436/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5644 - accuracy: 0.7481 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 437/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5010 - accuracy: 0.8015 - val_loss: 0.6284 - val_accuracy: 0.6964\n",
            "Epoch 438/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5270 - accuracy: 0.7786 - val_loss: 0.6288 - val_accuracy: 0.6964\n",
            "Epoch 439/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5371 - accuracy: 0.7710 - val_loss: 0.6278 - val_accuracy: 0.6964\n",
            "Epoch 440/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5460 - accuracy: 0.7634 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 441/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5455 - accuracy: 0.7634 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 442/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5552 - accuracy: 0.7557 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 443/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5324 - accuracy: 0.7750 - val_loss: 0.6255 - val_accuracy: 0.6964\n",
            "Epoch 444/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5547 - accuracy: 0.7557 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 445/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 446/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5394 - accuracy: 0.7688 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 447/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5093 - accuracy: 0.7939 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 448/1000\n",
            "5/5 [==============================] - 0s 108ms/step - loss: 0.5553 - accuracy: 0.7557 - val_loss: 0.6233 - val_accuracy: 0.6964\n",
            "Epoch 449/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5284 - accuracy: 0.7786 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 450/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5104 - accuracy: 0.7939 - val_loss: 0.6220 - val_accuracy: 0.6964\n",
            "Epoch 451/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5456 - accuracy: 0.7634 - val_loss: 0.6210 - val_accuracy: 0.6964\n",
            "Epoch 452/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5381 - accuracy: 0.7710 - val_loss: 0.6197 - val_accuracy: 0.6964\n",
            "Epoch 453/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5404 - accuracy: 0.7688 - val_loss: 0.6195 - val_accuracy: 0.6964\n",
            "Epoch 454/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5293 - accuracy: 0.7786 - val_loss: 0.6194 - val_accuracy: 0.6964\n",
            "Epoch 455/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5466 - accuracy: 0.7634 - val_loss: 0.6189 - val_accuracy: 0.6964\n",
            "Epoch 456/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5122 - accuracy: 0.7939 - val_loss: 0.6205 - val_accuracy: 0.6964\n",
            "Epoch 457/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5289 - accuracy: 0.7786 - val_loss: 0.6203 - val_accuracy: 0.6964\n",
            "Epoch 458/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5260 - accuracy: 0.7812 - val_loss: 0.6209 - val_accuracy: 0.6964\n",
            "Epoch 459/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5202 - accuracy: 0.7863 - val_loss: 0.6225 - val_accuracy: 0.6964\n",
            "Epoch 460/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5279 - accuracy: 0.7786 - val_loss: 0.6246 - val_accuracy: 0.6964\n",
            "Epoch 461/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5645 - accuracy: 0.7481 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 462/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5183 - accuracy: 0.7863 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 463/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5556 - accuracy: 0.7557 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 464/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5644 - accuracy: 0.7481 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 465/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5278 - accuracy: 0.7786 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 466/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5362 - accuracy: 0.7710 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 467/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.4999 - accuracy: 0.8015 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 468/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5367 - accuracy: 0.7710 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 469/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5003 - accuracy: 0.8015 - val_loss: 0.6249 - val_accuracy: 0.6964\n",
            "Epoch 470/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5369 - accuracy: 0.7710 - val_loss: 0.6245 - val_accuracy: 0.6964\n",
            "Epoch 471/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5103 - accuracy: 0.7939 - val_loss: 0.6255 - val_accuracy: 0.6964\n",
            "Epoch 472/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5372 - accuracy: 0.7710 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 473/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5394 - accuracy: 0.7688 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 474/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5637 - accuracy: 0.7481 - val_loss: 0.6255 - val_accuracy: 0.6964\n",
            "Epoch 475/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5454 - accuracy: 0.7634 - val_loss: 0.6232 - val_accuracy: 0.6964\n",
            "Epoch 476/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5457 - accuracy: 0.7634 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 477/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5548 - accuracy: 0.7557 - val_loss: 0.6165 - val_accuracy: 0.6964\n",
            "Epoch 478/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5088 - accuracy: 0.8015 - val_loss: 0.6154 - val_accuracy: 0.6964\n",
            "Epoch 479/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5486 - accuracy: 0.7634 - val_loss: 0.6149 - val_accuracy: 0.6964\n",
            "Epoch 480/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5412 - accuracy: 0.7710 - val_loss: 0.6146 - val_accuracy: 0.6964\n",
            "Epoch 481/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5567 - accuracy: 0.7557 - val_loss: 0.6151 - val_accuracy: 0.6964\n",
            "Epoch 482/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5389 - accuracy: 0.7710 - val_loss: 0.6165 - val_accuracy: 0.6964\n",
            "Epoch 483/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5705 - accuracy: 0.7405 - val_loss: 0.6186 - val_accuracy: 0.6964\n",
            "Epoch 484/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5288 - accuracy: 0.7786 - val_loss: 0.6203 - val_accuracy: 0.6964\n",
            "Epoch 485/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5374 - accuracy: 0.7710 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 486/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5374 - accuracy: 0.7710 - val_loss: 0.6226 - val_accuracy: 0.6964\n",
            "Epoch 487/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5455 - accuracy: 0.7634 - val_loss: 0.6224 - val_accuracy: 0.6964\n",
            "Epoch 488/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5460 - accuracy: 0.7634 - val_loss: 0.6233 - val_accuracy: 0.6964\n",
            "Epoch 489/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5395 - accuracy: 0.7688 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 490/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5096 - accuracy: 0.7939 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 491/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5452 - accuracy: 0.7634 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 492/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5314 - accuracy: 0.7750 - val_loss: 0.6288 - val_accuracy: 0.6964\n",
            "Epoch 493/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5361 - accuracy: 0.7710 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 494/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5272 - accuracy: 0.7786 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 495/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5265 - accuracy: 0.7786 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 496/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5554 - accuracy: 0.7557 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 497/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 498/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5328 - accuracy: 0.7750 - val_loss: 0.6212 - val_accuracy: 0.6964\n",
            "Epoch 499/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5207 - accuracy: 0.7863 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 500/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5280 - accuracy: 0.7786 - val_loss: 0.6221 - val_accuracy: 0.6964\n",
            "Epoch 501/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5323 - accuracy: 0.7750 - val_loss: 0.6229 - val_accuracy: 0.6964\n",
            "Epoch 502/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5362 - accuracy: 0.7710 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 503/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5317 - accuracy: 0.7750 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 504/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5319 - accuracy: 0.7750 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 505/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5243 - accuracy: 0.7812 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 506/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5272 - accuracy: 0.7786 - val_loss: 0.6280 - val_accuracy: 0.6964\n",
            "Epoch 507/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5270 - accuracy: 0.7786 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 508/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5367 - accuracy: 0.7710 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 509/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 510/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5554 - accuracy: 0.7557 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 511/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5184 - accuracy: 0.7863 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 512/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5318 - accuracy: 0.7750 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 513/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5731 - accuracy: 0.7405 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 514/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5318 - accuracy: 0.7750 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 515/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5180 - accuracy: 0.7863 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 516/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5453 - accuracy: 0.7634 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 517/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5546 - accuracy: 0.7557 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 518/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5364 - accuracy: 0.7710 - val_loss: 0.6253 - val_accuracy: 0.6964\n",
            "Epoch 519/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 520/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5014 - accuracy: 0.8015 - val_loss: 0.6219 - val_accuracy: 0.6964\n",
            "Epoch 521/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5448 - accuracy: 0.7634 - val_loss: 0.6230 - val_accuracy: 0.6964\n",
            "Epoch 522/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5364 - accuracy: 0.7710 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 523/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5638 - accuracy: 0.7481 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 524/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5242 - accuracy: 0.7812 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 525/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5463 - accuracy: 0.7634 - val_loss: 0.6281 - val_accuracy: 0.6964\n",
            "Epoch 526/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5647 - accuracy: 0.7481 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 527/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5372 - accuracy: 0.7710 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 528/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5241 - accuracy: 0.7812 - val_loss: 0.6255 - val_accuracy: 0.6964\n",
            "Epoch 529/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5320 - accuracy: 0.7750 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 530/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5181 - accuracy: 0.7863 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 531/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5460 - accuracy: 0.7634 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 532/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5392 - accuracy: 0.7688 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 533/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5369 - accuracy: 0.7710 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 534/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5639 - accuracy: 0.7481 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 535/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5185 - accuracy: 0.7863 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 536/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5454 - accuracy: 0.7634 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 537/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5315 - accuracy: 0.7750 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 538/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5373 - accuracy: 0.7710 - val_loss: 0.6225 - val_accuracy: 0.6964\n",
            "Epoch 539/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5452 - accuracy: 0.7634 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 540/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5454 - accuracy: 0.7634 - val_loss: 0.6213 - val_accuracy: 0.6964\n",
            "Epoch 541/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5286 - accuracy: 0.7786 - val_loss: 0.6211 - val_accuracy: 0.6964\n",
            "Epoch 542/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5286 - accuracy: 0.7786 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 543/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6248 - val_accuracy: 0.6964\n",
            "Epoch 544/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5549 - accuracy: 0.7557 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 545/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5393 - accuracy: 0.7688 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 546/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5182 - accuracy: 0.7863 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 547/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5277 - accuracy: 0.7786 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 548/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5263 - accuracy: 0.7786 - val_loss: 0.6280 - val_accuracy: 0.6964\n",
            "Epoch 549/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5275 - accuracy: 0.7786 - val_loss: 0.6296 - val_accuracy: 0.6964\n",
            "Epoch 550/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.4889 - accuracy: 0.8092 - val_loss: 0.6315 - val_accuracy: 0.6964\n",
            "Epoch 551/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5569 - accuracy: 0.7557 - val_loss: 0.6322 - val_accuracy: 0.6964\n",
            "Epoch 552/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5369 - accuracy: 0.7710 - val_loss: 0.6327 - val_accuracy: 0.6964\n",
            "Epoch 553/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5570 - accuracy: 0.7557 - val_loss: 0.6306 - val_accuracy: 0.6964\n",
            "Epoch 554/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5562 - accuracy: 0.7557 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 555/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5082 - accuracy: 0.7939 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 556/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5319 - accuracy: 0.7750 - val_loss: 0.6285 - val_accuracy: 0.6964\n",
            "Epoch 557/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5271 - accuracy: 0.7786 - val_loss: 0.6284 - val_accuracy: 0.6964\n",
            "Epoch 558/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5083 - accuracy: 0.7939 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 559/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5178 - accuracy: 0.7863 - val_loss: 0.6280 - val_accuracy: 0.6964\n",
            "Epoch 560/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5391 - accuracy: 0.7688 - val_loss: 0.6276 - val_accuracy: 0.6964\n",
            "Epoch 561/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5363 - accuracy: 0.7710 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 562/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5554 - accuracy: 0.7557 - val_loss: 0.6254 - val_accuracy: 0.6964\n",
            "Epoch 563/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5091 - accuracy: 0.7939 - val_loss: 0.6246 - val_accuracy: 0.6964\n",
            "Epoch 564/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5449 - accuracy: 0.7634 - val_loss: 0.6224 - val_accuracy: 0.6964\n",
            "Epoch 565/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5107 - accuracy: 0.7939 - val_loss: 0.6212 - val_accuracy: 0.6964\n",
            "Epoch 566/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5326 - accuracy: 0.7750 - val_loss: 0.6206 - val_accuracy: 0.6964\n",
            "Epoch 567/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5371 - accuracy: 0.7710 - val_loss: 0.6210 - val_accuracy: 0.6964\n",
            "Epoch 568/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5279 - accuracy: 0.7786 - val_loss: 0.6218 - val_accuracy: 0.6964\n",
            "Epoch 569/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5392 - accuracy: 0.7688 - val_loss: 0.6226 - val_accuracy: 0.6964\n",
            "Epoch 570/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5190 - accuracy: 0.7863 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 571/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5316 - accuracy: 0.7750 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 572/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5276 - accuracy: 0.7786 - val_loss: 0.6279 - val_accuracy: 0.6964\n",
            "Epoch 573/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5168 - accuracy: 0.7863 - val_loss: 0.6299 - val_accuracy: 0.6964\n",
            "Epoch 574/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5312 - accuracy: 0.7750 - val_loss: 0.6316 - val_accuracy: 0.6964\n",
            "Epoch 575/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5317 - accuracy: 0.7750 - val_loss: 0.6327 - val_accuracy: 0.6964\n",
            "Epoch 576/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5180 - accuracy: 0.7863 - val_loss: 0.6350 - val_accuracy: 0.6964\n",
            "Epoch 577/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5474 - accuracy: 0.7634 - val_loss: 0.6360 - val_accuracy: 0.6964\n",
            "Epoch 578/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5271 - accuracy: 0.7786 - val_loss: 0.6356 - val_accuracy: 0.6964\n",
            "Epoch 579/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5270 - accuracy: 0.7786 - val_loss: 0.6365 - val_accuracy: 0.6964\n",
            "Epoch 580/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5280 - accuracy: 0.7786 - val_loss: 0.6363 - val_accuracy: 0.6964\n",
            "Epoch 581/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5321 - accuracy: 0.7750 - val_loss: 0.6358 - val_accuracy: 0.6964\n",
            "Epoch 582/1000\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 0.5240 - accuracy: 0.7812 - val_loss: 0.6345 - val_accuracy: 0.6964\n",
            "Epoch 583/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5074 - accuracy: 0.7939 - val_loss: 0.6356 - val_accuracy: 0.6964\n",
            "Epoch 584/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5471 - accuracy: 0.7634 - val_loss: 0.6357 - val_accuracy: 0.6964\n",
            "Epoch 585/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5579 - accuracy: 0.7557 - val_loss: 0.6352 - val_accuracy: 0.6964\n",
            "Epoch 586/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5066 - accuracy: 0.7939 - val_loss: 0.6362 - val_accuracy: 0.6964\n",
            "Epoch 587/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5406 - accuracy: 0.7688 - val_loss: 0.6364 - val_accuracy: 0.6964\n",
            "Epoch 588/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5373 - accuracy: 0.7710 - val_loss: 0.6354 - val_accuracy: 0.6964\n",
            "Epoch 589/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5170 - accuracy: 0.7863 - val_loss: 0.6340 - val_accuracy: 0.6964\n",
            "Epoch 590/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5075 - accuracy: 0.7939 - val_loss: 0.6339 - val_accuracy: 0.6964\n",
            "Epoch 591/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5078 - accuracy: 0.7939 - val_loss: 0.6351 - val_accuracy: 0.6964\n",
            "Epoch 592/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5070 - accuracy: 0.7939 - val_loss: 0.6370 - val_accuracy: 0.6964\n",
            "Epoch 593/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5068 - accuracy: 0.7939 - val_loss: 0.6387 - val_accuracy: 0.6964\n",
            "Epoch 594/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5180 - accuracy: 0.7863 - val_loss: 0.6392 - val_accuracy: 0.6964\n",
            "Epoch 595/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5069 - accuracy: 0.7939 - val_loss: 0.6419 - val_accuracy: 0.6964\n",
            "Epoch 596/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5389 - accuracy: 0.7710 - val_loss: 0.6433 - val_accuracy: 0.6964\n",
            "Epoch 597/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5396 - accuracy: 0.7710 - val_loss: 0.6436 - val_accuracy: 0.6964\n",
            "Epoch 598/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5395 - accuracy: 0.7710 - val_loss: 0.6415 - val_accuracy: 0.6964\n",
            "Epoch 599/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5335 - accuracy: 0.7750 - val_loss: 0.6386 - val_accuracy: 0.6964\n",
            "Epoch 600/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5480 - accuracy: 0.7634 - val_loss: 0.6345 - val_accuracy: 0.6964\n",
            "Epoch 601/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5365 - accuracy: 0.7710 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 602/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5458 - accuracy: 0.7634 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 603/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5456 - accuracy: 0.7634 - val_loss: 0.6291 - val_accuracy: 0.6964\n",
            "Epoch 604/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5273 - accuracy: 0.7786 - val_loss: 0.6287 - val_accuracy: 0.6964\n",
            "Epoch 605/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5235 - accuracy: 0.7812 - val_loss: 0.6284 - val_accuracy: 0.6964\n",
            "Epoch 606/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.4990 - accuracy: 0.8015 - val_loss: 0.6294 - val_accuracy: 0.6964\n",
            "Epoch 607/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5457 - accuracy: 0.7634 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 608/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5365 - accuracy: 0.7710 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 609/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5081 - accuracy: 0.7939 - val_loss: 0.6295 - val_accuracy: 0.6964\n",
            "Epoch 610/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5392 - accuracy: 0.7688 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 611/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5463 - accuracy: 0.7634 - val_loss: 0.6308 - val_accuracy: 0.6964\n",
            "Epoch 612/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5170 - accuracy: 0.7863 - val_loss: 0.6317 - val_accuracy: 0.6964\n",
            "Epoch 613/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5314 - accuracy: 0.7750 - val_loss: 0.6314 - val_accuracy: 0.6964\n",
            "Epoch 614/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5462 - accuracy: 0.7634 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 615/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5177 - accuracy: 0.7863 - val_loss: 0.6302 - val_accuracy: 0.6964\n",
            "Epoch 616/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5267 - accuracy: 0.7786 - val_loss: 0.6318 - val_accuracy: 0.6964\n",
            "Epoch 617/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5360 - accuracy: 0.7710 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 618/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5545 - accuracy: 0.7557 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 619/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5454 - accuracy: 0.7634 - val_loss: 0.6242 - val_accuracy: 0.6964\n",
            "Epoch 620/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5465 - accuracy: 0.7634 - val_loss: 0.6222 - val_accuracy: 0.6964\n",
            "Epoch 621/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5321 - accuracy: 0.7750 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 622/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5371 - accuracy: 0.7710 - val_loss: 0.6210 - val_accuracy: 0.6964\n",
            "Epoch 623/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5451 - accuracy: 0.7634 - val_loss: 0.6204 - val_accuracy: 0.6964\n",
            "Epoch 624/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5033 - accuracy: 0.8015 - val_loss: 0.6206 - val_accuracy: 0.6964\n",
            "Epoch 625/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5032 - accuracy: 0.8015 - val_loss: 0.6213 - val_accuracy: 0.6964\n",
            "Epoch 626/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5195 - accuracy: 0.7863 - val_loss: 0.6218 - val_accuracy: 0.6964\n",
            "Epoch 627/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5456 - accuracy: 0.7634 - val_loss: 0.6229 - val_accuracy: 0.6964\n",
            "Epoch 628/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5362 - accuracy: 0.7710 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
            "Epoch 629/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5089 - accuracy: 0.7939 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 630/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5641 - accuracy: 0.7481 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 631/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5238 - accuracy: 0.7812 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 632/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5451 - accuracy: 0.7634 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 633/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5461 - accuracy: 0.7634 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 634/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5448 - accuracy: 0.7634 - val_loss: 0.6242 - val_accuracy: 0.6964\n",
            "Epoch 635/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5362 - accuracy: 0.7710 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
            "Epoch 636/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5390 - accuracy: 0.7688 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 637/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5316 - accuracy: 0.7750 - val_loss: 0.6248 - val_accuracy: 0.6964\n",
            "Epoch 638/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5366 - accuracy: 0.7710 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 639/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5365 - accuracy: 0.7710 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 640/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4999 - accuracy: 0.8015 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 641/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5462 - accuracy: 0.7634 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 642/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5388 - accuracy: 0.7688 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 643/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5234 - accuracy: 0.7812 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 644/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5730 - accuracy: 0.7405 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 645/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5392 - accuracy: 0.7688 - val_loss: 0.6245 - val_accuracy: 0.6964\n",
            "Epoch 646/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5389 - accuracy: 0.7688 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 647/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6225 - val_accuracy: 0.6964\n",
            "Epoch 648/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5096 - accuracy: 0.7939 - val_loss: 0.6219 - val_accuracy: 0.6964\n",
            "Epoch 649/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5186 - accuracy: 0.7863 - val_loss: 0.6216 - val_accuracy: 0.6964\n",
            "Epoch 650/1000\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5016 - accuracy: 0.8015 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 651/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5318 - accuracy: 0.7750 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 652/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5447 - accuracy: 0.7634 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 653/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5456 - accuracy: 0.7634 - val_loss: 0.6242 - val_accuracy: 0.6964\n",
            "Epoch 654/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5541 - accuracy: 0.7557 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
            "Epoch 655/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5633 - accuracy: 0.7481 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 656/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5277 - accuracy: 0.7786 - val_loss: 0.6198 - val_accuracy: 0.6964\n",
            "Epoch 657/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5452 - accuracy: 0.7634 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 658/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5200 - accuracy: 0.7863 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
            "Epoch 659/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5390 - accuracy: 0.7688 - val_loss: 0.6220 - val_accuracy: 0.6964\n",
            "Epoch 660/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5445 - accuracy: 0.7634 - val_loss: 0.6225 - val_accuracy: 0.6964\n",
            "Epoch 661/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5281 - accuracy: 0.7786 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 662/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5359 - accuracy: 0.7710 - val_loss: 0.6248 - val_accuracy: 0.6964\n",
            "Epoch 663/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.4990 - accuracy: 0.8015 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 664/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5359 - accuracy: 0.7710 - val_loss: 0.6285 - val_accuracy: 0.6964\n",
            "Epoch 665/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5268 - accuracy: 0.7786 - val_loss: 0.6305 - val_accuracy: 0.6964\n",
            "Epoch 666/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5164 - accuracy: 0.7863 - val_loss: 0.6325 - val_accuracy: 0.6964\n",
            "Epoch 667/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5561 - accuracy: 0.7557 - val_loss: 0.6332 - val_accuracy: 0.6964\n",
            "Epoch 668/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5472 - accuracy: 0.7634 - val_loss: 0.6333 - val_accuracy: 0.6964\n",
            "Epoch 669/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5271 - accuracy: 0.7786 - val_loss: 0.6325 - val_accuracy: 0.6964\n",
            "Epoch 670/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5315 - accuracy: 0.7750 - val_loss: 0.6319 - val_accuracy: 0.6964\n",
            "Epoch 671/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5365 - accuracy: 0.7710 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 672/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5756 - accuracy: 0.7405 - val_loss: 0.6303 - val_accuracy: 0.6964\n",
            "Epoch 673/1000\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.5462 - accuracy: 0.7634 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 674/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.5264 - accuracy: 0.7786 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 675/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5449 - accuracy: 0.7634 - val_loss: 0.6248 - val_accuracy: 0.6964\n",
            "Epoch 676/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5359 - accuracy: 0.7710 - val_loss: 0.6254 - val_accuracy: 0.6964\n",
            "Epoch 677/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5090 - accuracy: 0.7939 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 678/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5388 - accuracy: 0.7688 - val_loss: 0.6278 - val_accuracy: 0.6964\n",
            "Epoch 679/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5454 - accuracy: 0.7634 - val_loss: 0.6286 - val_accuracy: 0.6964\n",
            "Epoch 680/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5646 - accuracy: 0.7481 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 681/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.4980 - accuracy: 0.8015 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 682/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5271 - accuracy: 0.7786 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 683/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5452 - accuracy: 0.7634 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 684/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5542 - accuracy: 0.7557 - val_loss: 0.6242 - val_accuracy: 0.6964\n",
            "Epoch 685/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5364 - accuracy: 0.7710 - val_loss: 0.6253 - val_accuracy: 0.6964\n",
            "Epoch 686/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5389 - accuracy: 0.7688 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 687/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5359 - accuracy: 0.7710 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 688/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5082 - accuracy: 0.7939 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 689/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5275 - accuracy: 0.7786 - val_loss: 0.6271 - val_accuracy: 0.6964\n",
            "Epoch 690/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5553 - accuracy: 0.7557 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 691/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5550 - accuracy: 0.7557 - val_loss: 0.6284 - val_accuracy: 0.6964\n",
            "Epoch 692/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5454 - accuracy: 0.7634 - val_loss: 0.6301 - val_accuracy: 0.6964\n",
            "Epoch 693/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5364 - accuracy: 0.7710 - val_loss: 0.6327 - val_accuracy: 0.6964\n",
            "Epoch 694/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4974 - accuracy: 0.8015 - val_loss: 0.6349 - val_accuracy: 0.6964\n",
            "Epoch 695/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6358 - val_accuracy: 0.6964\n",
            "Epoch 696/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5672 - accuracy: 0.7481 - val_loss: 0.6319 - val_accuracy: 0.6964\n",
            "Epoch 697/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5171 - accuracy: 0.7863 - val_loss: 0.6284 - val_accuracy: 0.6964\n",
            "Epoch 698/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5359 - accuracy: 0.7710 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 699/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5265 - accuracy: 0.7786 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 700/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5644 - accuracy: 0.7481 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 701/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5455 - accuracy: 0.7634 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 702/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5449 - accuracy: 0.7634 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
            "Epoch 703/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5365 - accuracy: 0.7710 - val_loss: 0.6237 - val_accuracy: 0.6964\n",
            "Epoch 704/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5182 - accuracy: 0.7863 - val_loss: 0.6237 - val_accuracy: 0.6964\n",
            "Epoch 705/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5270 - accuracy: 0.7786 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
            "Epoch 706/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5360 - accuracy: 0.7710 - val_loss: 0.6232 - val_accuracy: 0.6964\n",
            "Epoch 707/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5546 - accuracy: 0.7557 - val_loss: 0.6224 - val_accuracy: 0.6964\n",
            "Epoch 708/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5314 - accuracy: 0.7750 - val_loss: 0.6223 - val_accuracy: 0.6964\n",
            "Epoch 709/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5362 - accuracy: 0.7710 - val_loss: 0.6229 - val_accuracy: 0.6964\n",
            "Epoch 710/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5273 - accuracy: 0.7786 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 711/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5265 - accuracy: 0.7786 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 712/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5268 - accuracy: 0.7786 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 713/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5460 - accuracy: 0.7634 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 714/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5083 - accuracy: 0.7939 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 715/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5560 - accuracy: 0.7557 - val_loss: 0.6310 - val_accuracy: 0.6964\n",
            "Epoch 716/1000\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.5566 - accuracy: 0.7557 - val_loss: 0.6322 - val_accuracy: 0.6964\n",
            "Epoch 717/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5568 - accuracy: 0.7557 - val_loss: 0.6338 - val_accuracy: 0.6964\n",
            "Epoch 718/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5173 - accuracy: 0.7863 - val_loss: 0.6364 - val_accuracy: 0.6964\n",
            "Epoch 719/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5240 - accuracy: 0.7812 - val_loss: 0.6379 - val_accuracy: 0.6964\n",
            "Epoch 720/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5068 - accuracy: 0.7939 - val_loss: 0.6387 - val_accuracy: 0.6964\n",
            "Epoch 721/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5480 - accuracy: 0.7634 - val_loss: 0.6388 - val_accuracy: 0.6964\n",
            "Epoch 722/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5581 - accuracy: 0.7557 - val_loss: 0.6369 - val_accuracy: 0.6964\n",
            "Epoch 723/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5167 - accuracy: 0.7863 - val_loss: 0.6348 - val_accuracy: 0.6964\n",
            "Epoch 724/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.4975 - accuracy: 0.8015 - val_loss: 0.6324 - val_accuracy: 0.6964\n",
            "Epoch 725/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5457 - accuracy: 0.7634 - val_loss: 0.6309 - val_accuracy: 0.6964\n",
            "Epoch 726/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5456 - accuracy: 0.7634 - val_loss: 0.6285 - val_accuracy: 0.6964\n",
            "Epoch 727/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5546 - accuracy: 0.7557 - val_loss: 0.6276 - val_accuracy: 0.6964\n",
            "Epoch 728/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5356 - accuracy: 0.7710 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 729/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5537 - accuracy: 0.7557 - val_loss: 0.6232 - val_accuracy: 0.6964\n",
            "Epoch 730/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5276 - accuracy: 0.7786 - val_loss: 0.6209 - val_accuracy: 0.6964\n",
            "Epoch 731/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5369 - accuracy: 0.7710 - val_loss: 0.6202 - val_accuracy: 0.6964\n",
            "Epoch 732/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5707 - accuracy: 0.7405 - val_loss: 0.6198 - val_accuracy: 0.6964\n",
            "Epoch 733/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5370 - accuracy: 0.7710 - val_loss: 0.6184 - val_accuracy: 0.6964\n",
            "Epoch 734/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5203 - accuracy: 0.7863 - val_loss: 0.6182 - val_accuracy: 0.6964\n",
            "Epoch 735/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5624 - accuracy: 0.7481 - val_loss: 0.6183 - val_accuracy: 0.6964\n",
            "Epoch 736/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5453 - accuracy: 0.7634 - val_loss: 0.6202 - val_accuracy: 0.6964\n",
            "Epoch 737/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5251 - accuracy: 0.7812 - val_loss: 0.6219 - val_accuracy: 0.6964\n",
            "Epoch 738/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5535 - accuracy: 0.7557 - val_loss: 0.6225 - val_accuracy: 0.6964\n",
            "Epoch 739/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5356 - accuracy: 0.7710 - val_loss: 0.6229 - val_accuracy: 0.6964\n",
            "Epoch 740/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5451 - accuracy: 0.7634 - val_loss: 0.6223 - val_accuracy: 0.6964\n",
            "Epoch 741/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5358 - accuracy: 0.7710 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 742/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5186 - accuracy: 0.7863 - val_loss: 0.6222 - val_accuracy: 0.6964\n",
            "Epoch 743/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5542 - accuracy: 0.7557 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 744/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5385 - accuracy: 0.7688 - val_loss: 0.6249 - val_accuracy: 0.6964\n",
            "Epoch 745/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5074 - accuracy: 0.7939 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 746/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5261 - accuracy: 0.7786 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 747/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5368 - accuracy: 0.7710 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 748/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.4984 - accuracy: 0.8015 - val_loss: 0.6324 - val_accuracy: 0.6964\n",
            "Epoch 749/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5318 - accuracy: 0.7750 - val_loss: 0.6344 - val_accuracy: 0.6964\n",
            "Epoch 750/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5375 - accuracy: 0.7710 - val_loss: 0.6367 - val_accuracy: 0.6964\n",
            "Epoch 751/1000\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.5166 - accuracy: 0.7863 - val_loss: 0.6375 - val_accuracy: 0.6964\n",
            "Epoch 752/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5069 - accuracy: 0.7939 - val_loss: 0.6407 - val_accuracy: 0.6964\n",
            "Epoch 753/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5418 - accuracy: 0.7688 - val_loss: 0.6413 - val_accuracy: 0.6964\n",
            "Epoch 754/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5487 - accuracy: 0.7634 - val_loss: 0.6395 - val_accuracy: 0.6964\n",
            "Epoch 755/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.4964 - accuracy: 0.8015 - val_loss: 0.6382 - val_accuracy: 0.6964\n",
            "Epoch 756/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5282 - accuracy: 0.7786 - val_loss: 0.6320 - val_accuracy: 0.6964\n",
            "Epoch 757/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5160 - accuracy: 0.7863 - val_loss: 0.6283 - val_accuracy: 0.6964\n",
            "Epoch 758/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5276 - accuracy: 0.7786 - val_loss: 0.6237 - val_accuracy: 0.6964\n",
            "Epoch 759/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5356 - accuracy: 0.7710 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 760/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5387 - accuracy: 0.7688 - val_loss: 0.6203 - val_accuracy: 0.6964\n",
            "Epoch 761/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5450 - accuracy: 0.7634 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
            "Epoch 762/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5393 - accuracy: 0.7688 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
            "Epoch 763/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.4943 - accuracy: 0.8092 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
            "Epoch 764/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5250 - accuracy: 0.7812 - val_loss: 0.6198 - val_accuracy: 0.6964\n",
            "Epoch 765/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5389 - accuracy: 0.7688 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
            "Epoch 766/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5449 - accuracy: 0.7634 - val_loss: 0.6206 - val_accuracy: 0.6964\n",
            "Epoch 767/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6211 - val_accuracy: 0.6964\n",
            "Epoch 768/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5183 - accuracy: 0.7863 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 769/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5314 - accuracy: 0.7750 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 770/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5437 - accuracy: 0.7634 - val_loss: 0.6245 - val_accuracy: 0.6964\n",
            "Epoch 771/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.5538 - accuracy: 0.7557 - val_loss: 0.6249 - val_accuracy: 0.6964\n",
            "Epoch 772/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.4992 - accuracy: 0.8015 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 773/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5388 - accuracy: 0.7688 - val_loss: 0.6291 - val_accuracy: 0.6964\n",
            "Epoch 774/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5455 - accuracy: 0.7634 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 775/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5268 - accuracy: 0.7786 - val_loss: 0.6318 - val_accuracy: 0.6964\n",
            "Epoch 776/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5395 - accuracy: 0.7688 - val_loss: 0.6309 - val_accuracy: 0.6964\n",
            "Epoch 777/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5757 - accuracy: 0.7405 - val_loss: 0.6299 - val_accuracy: 0.6964\n",
            "Epoch 778/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.4883 - accuracy: 0.8092 - val_loss: 0.6289 - val_accuracy: 0.6964\n",
            "Epoch 779/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5458 - accuracy: 0.7634 - val_loss: 0.6273 - val_accuracy: 0.6964\n",
            "Epoch 780/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5088 - accuracy: 0.7939 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 781/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5176 - accuracy: 0.7863 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
            "Epoch 782/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5175 - accuracy: 0.7863 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 783/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5452 - accuracy: 0.7634 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 784/1000\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 0.5391 - accuracy: 0.7688 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 785/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5270 - accuracy: 0.7786 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 786/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5308 - accuracy: 0.7750 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 787/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5310 - accuracy: 0.7750 - val_loss: 0.6255 - val_accuracy: 0.6964\n",
            "Epoch 788/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5273 - accuracy: 0.7786 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 789/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5264 - accuracy: 0.7786 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 790/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5462 - accuracy: 0.7634 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 791/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5266 - accuracy: 0.7786 - val_loss: 0.6271 - val_accuracy: 0.6964\n",
            "Epoch 792/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5356 - accuracy: 0.7710 - val_loss: 0.6272 - val_accuracy: 0.6964\n",
            "Epoch 793/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5366 - accuracy: 0.7710 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 794/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5450 - accuracy: 0.7634 - val_loss: 0.6231 - val_accuracy: 0.6964\n",
            "Epoch 795/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6205 - val_accuracy: 0.6964\n",
            "Epoch 796/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5280 - accuracy: 0.7786 - val_loss: 0.6196 - val_accuracy: 0.6964\n",
            "Epoch 797/1000\n",
            "5/5 [==============================] - 0s 106ms/step - loss: 0.5451 - accuracy: 0.7634 - val_loss: 0.6210 - val_accuracy: 0.6964\n",
            "Epoch 798/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5361 - accuracy: 0.7710 - val_loss: 0.6222 - val_accuracy: 0.6964\n",
            "Epoch 799/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5266 - accuracy: 0.7786 - val_loss: 0.6231 - val_accuracy: 0.6964\n",
            "Epoch 800/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5173 - accuracy: 0.7863 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 801/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5541 - accuracy: 0.7557 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 802/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5258 - accuracy: 0.7786 - val_loss: 0.6279 - val_accuracy: 0.6964\n",
            "Epoch 803/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5458 - accuracy: 0.7634 - val_loss: 0.6295 - val_accuracy: 0.6964\n",
            "Epoch 804/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5230 - accuracy: 0.7812 - val_loss: 0.6310 - val_accuracy: 0.6964\n",
            "Epoch 805/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5257 - accuracy: 0.7786 - val_loss: 0.6332 - val_accuracy: 0.6964\n",
            "Epoch 806/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5314 - accuracy: 0.7750 - val_loss: 0.6343 - val_accuracy: 0.6964\n",
            "Epoch 807/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.4963 - accuracy: 0.8015 - val_loss: 0.6347 - val_accuracy: 0.6964\n",
            "Epoch 808/1000\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.5172 - accuracy: 0.7863 - val_loss: 0.6354 - val_accuracy: 0.6964\n",
            "Epoch 809/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5469 - accuracy: 0.7634 - val_loss: 0.6354 - val_accuracy: 0.6964\n",
            "Epoch 810/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5399 - accuracy: 0.7688 - val_loss: 0.6338 - val_accuracy: 0.6964\n",
            "Epoch 811/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5464 - accuracy: 0.7634 - val_loss: 0.6337 - val_accuracy: 0.6964\n",
            "Epoch 812/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.4966 - accuracy: 0.8015 - val_loss: 0.6350 - val_accuracy: 0.6964\n",
            "Epoch 813/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.4860 - accuracy: 0.8092 - val_loss: 0.6352 - val_accuracy: 0.6964\n",
            "Epoch 814/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5167 - accuracy: 0.7863 - val_loss: 0.6345 - val_accuracy: 0.6964\n",
            "Epoch 815/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5566 - accuracy: 0.7557 - val_loss: 0.6341 - val_accuracy: 0.6964\n",
            "Epoch 816/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5233 - accuracy: 0.7812 - val_loss: 0.6340 - val_accuracy: 0.6964\n",
            "Epoch 817/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5665 - accuracy: 0.7481 - val_loss: 0.6331 - val_accuracy: 0.6964\n",
            "Epoch 818/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5667 - accuracy: 0.7481 - val_loss: 0.6308 - val_accuracy: 0.6964\n",
            "Epoch 819/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5168 - accuracy: 0.7863 - val_loss: 0.6304 - val_accuracy: 0.6964\n",
            "Epoch 820/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5270 - accuracy: 0.7786 - val_loss: 0.6286 - val_accuracy: 0.6964\n",
            "Epoch 821/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5361 - accuracy: 0.7710 - val_loss: 0.6279 - val_accuracy: 0.6964\n",
            "Epoch 822/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5361 - accuracy: 0.7710 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 823/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5267 - accuracy: 0.7786 - val_loss: 0.6251 - val_accuracy: 0.6964\n",
            "Epoch 824/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.5455 - accuracy: 0.7634 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
            "Epoch 825/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5095 - accuracy: 0.7939 - val_loss: 0.6225 - val_accuracy: 0.6964\n",
            "Epoch 826/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5451 - accuracy: 0.7634 - val_loss: 0.6219 - val_accuracy: 0.6964\n",
            "Epoch 827/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5621 - accuracy: 0.7481 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
            "Epoch 828/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5529 - accuracy: 0.7557 - val_loss: 0.6188 - val_accuracy: 0.6964\n",
            "Epoch 829/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5111 - accuracy: 0.7939 - val_loss: 0.6184 - val_accuracy: 0.6964\n",
            "Epoch 830/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5447 - accuracy: 0.7634 - val_loss: 0.6184 - val_accuracy: 0.6964\n",
            "Epoch 831/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5464 - accuracy: 0.7634 - val_loss: 0.6174 - val_accuracy: 0.6964\n",
            "Epoch 832/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5290 - accuracy: 0.7786 - val_loss: 0.6168 - val_accuracy: 0.6964\n",
            "Epoch 833/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5205 - accuracy: 0.7863 - val_loss: 0.6165 - val_accuracy: 0.6964\n",
            "Epoch 834/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5056 - accuracy: 0.8015 - val_loss: 0.6168 - val_accuracy: 0.6964\n",
            "Epoch 835/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5298 - accuracy: 0.7786 - val_loss: 0.6178 - val_accuracy: 0.6964\n",
            "Epoch 836/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5526 - accuracy: 0.7557 - val_loss: 0.6191 - val_accuracy: 0.6964\n",
            "Epoch 837/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5390 - accuracy: 0.7688 - val_loss: 0.6205 - val_accuracy: 0.6964\n",
            "Epoch 838/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5177 - accuracy: 0.7863 - val_loss: 0.6220 - val_accuracy: 0.6964\n",
            "Epoch 839/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5539 - accuracy: 0.7557 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 840/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5355 - accuracy: 0.7710 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 841/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5538 - accuracy: 0.7557 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 842/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5384 - accuracy: 0.7688 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 843/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4987 - accuracy: 0.8015 - val_loss: 0.6241 - val_accuracy: 0.6964\n",
            "Epoch 844/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5175 - accuracy: 0.7863 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 845/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5642 - accuracy: 0.7481 - val_loss: 0.6287 - val_accuracy: 0.6964\n",
            "Epoch 846/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5448 - accuracy: 0.7634 - val_loss: 0.6296 - val_accuracy: 0.6964\n",
            "Epoch 847/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5462 - accuracy: 0.7634 - val_loss: 0.6304 - val_accuracy: 0.6964\n",
            "Epoch 848/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5456 - accuracy: 0.7634 - val_loss: 0.6318 - val_accuracy: 0.6964\n",
            "Epoch 849/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5069 - accuracy: 0.7939 - val_loss: 0.6323 - val_accuracy: 0.6964\n",
            "Epoch 850/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5054 - accuracy: 0.7939 - val_loss: 0.6329 - val_accuracy: 0.6964\n",
            "Epoch 851/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5360 - accuracy: 0.7710 - val_loss: 0.6338 - val_accuracy: 0.6964\n",
            "Epoch 852/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5265 - accuracy: 0.7786 - val_loss: 0.6343 - val_accuracy: 0.6964\n",
            "Epoch 853/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5370 - accuracy: 0.7710 - val_loss: 0.6325 - val_accuracy: 0.6964\n",
            "Epoch 854/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5756 - accuracy: 0.7405 - val_loss: 0.6310 - val_accuracy: 0.6964\n",
            "Epoch 855/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.4968 - accuracy: 0.8015 - val_loss: 0.6294 - val_accuracy: 0.6964\n",
            "Epoch 856/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5263 - accuracy: 0.7786 - val_loss: 0.6298 - val_accuracy: 0.6964\n",
            "Epoch 857/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5257 - accuracy: 0.7786 - val_loss: 0.6303 - val_accuracy: 0.6964\n",
            "Epoch 858/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5369 - accuracy: 0.7710 - val_loss: 0.6320 - val_accuracy: 0.6964\n",
            "Epoch 859/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5063 - accuracy: 0.7939 - val_loss: 0.6337 - val_accuracy: 0.6964\n",
            "Epoch 860/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5323 - accuracy: 0.7750 - val_loss: 0.6364 - val_accuracy: 0.6964\n",
            "Epoch 861/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5269 - accuracy: 0.7786 - val_loss: 0.6382 - val_accuracy: 0.6964\n",
            "Epoch 862/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5382 - accuracy: 0.7710 - val_loss: 0.6368 - val_accuracy: 0.6964\n",
            "Epoch 863/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5157 - accuracy: 0.7863 - val_loss: 0.6335 - val_accuracy: 0.6964\n",
            "Epoch 864/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5066 - accuracy: 0.7939 - val_loss: 0.6313 - val_accuracy: 0.6964\n",
            "Epoch 865/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5754 - accuracy: 0.7405 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 866/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5648 - accuracy: 0.7481 - val_loss: 0.6259 - val_accuracy: 0.6964\n",
            "Epoch 867/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5347 - accuracy: 0.7710 - val_loss: 0.6231 - val_accuracy: 0.6964\n",
            "Epoch 868/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5617 - accuracy: 0.7481 - val_loss: 0.6203 - val_accuracy: 0.6964\n",
            "Epoch 869/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5367 - accuracy: 0.7710 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
            "Epoch 870/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5319 - accuracy: 0.7750 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
            "Epoch 871/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5189 - accuracy: 0.7863 - val_loss: 0.6202 - val_accuracy: 0.6964\n",
            "Epoch 872/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5447 - accuracy: 0.7634 - val_loss: 0.6215 - val_accuracy: 0.6964\n",
            "Epoch 873/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5617 - accuracy: 0.7481 - val_loss: 0.6216 - val_accuracy: 0.6964\n",
            "Epoch 874/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5612 - accuracy: 0.7481 - val_loss: 0.6209 - val_accuracy: 0.6964\n",
            "Epoch 875/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5389 - accuracy: 0.7688 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
            "Epoch 876/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5192 - accuracy: 0.7863 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
            "Epoch 877/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5187 - accuracy: 0.7863 - val_loss: 0.6198 - val_accuracy: 0.6964\n",
            "Epoch 878/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5282 - accuracy: 0.7786 - val_loss: 0.6189 - val_accuracy: 0.6964\n",
            "Epoch 879/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5447 - accuracy: 0.7634 - val_loss: 0.6170 - val_accuracy: 0.6964\n",
            "Epoch 880/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5288 - accuracy: 0.7786 - val_loss: 0.6171 - val_accuracy: 0.6964\n",
            "Epoch 881/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5123 - accuracy: 0.7939 - val_loss: 0.6180 - val_accuracy: 0.6964\n",
            "Epoch 882/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5457 - accuracy: 0.7634 - val_loss: 0.6183 - val_accuracy: 0.6964\n",
            "Epoch 883/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5367 - accuracy: 0.7710 - val_loss: 0.6175 - val_accuracy: 0.6964\n",
            "Epoch 884/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5293 - accuracy: 0.7786 - val_loss: 0.6182 - val_accuracy: 0.6964\n",
            "Epoch 885/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.4928 - accuracy: 0.8092 - val_loss: 0.6204 - val_accuracy: 0.6964\n",
            "Epoch 886/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5180 - accuracy: 0.7863 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
            "Epoch 887/1000\n",
            "5/5 [==============================] - 0s 106ms/step - loss: 0.5352 - accuracy: 0.7710 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 888/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5078 - accuracy: 0.7939 - val_loss: 0.6267 - val_accuracy: 0.6964\n",
            "Epoch 889/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5357 - accuracy: 0.7710 - val_loss: 0.6270 - val_accuracy: 0.6964\n",
            "Epoch 890/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5076 - accuracy: 0.7939 - val_loss: 0.6272 - val_accuracy: 0.6964\n",
            "Epoch 891/1000\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.5174 - accuracy: 0.7863 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 892/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5363 - accuracy: 0.7710 - val_loss: 0.6250 - val_accuracy: 0.6964\n",
            "Epoch 893/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5642 - accuracy: 0.7481 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 894/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5810 - accuracy: 0.7328 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 895/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5174 - accuracy: 0.7863 - val_loss: 0.6233 - val_accuracy: 0.6964\n",
            "Epoch 896/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5083 - accuracy: 0.7939 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 897/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5447 - accuracy: 0.7634 - val_loss: 0.6226 - val_accuracy: 0.6964\n",
            "Epoch 898/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5548 - accuracy: 0.7557 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 899/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5276 - accuracy: 0.7786 - val_loss: 0.6195 - val_accuracy: 0.6964\n",
            "Epoch 900/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6201 - val_accuracy: 0.6964\n",
            "Epoch 901/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6198 - val_accuracy: 0.6964\n",
            "Epoch 902/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5270 - accuracy: 0.7786 - val_loss: 0.6197 - val_accuracy: 0.6964\n",
            "Epoch 903/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5387 - accuracy: 0.7688 - val_loss: 0.6197 - val_accuracy: 0.6964\n",
            "Epoch 904/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5278 - accuracy: 0.7786 - val_loss: 0.6214 - val_accuracy: 0.6964\n",
            "Epoch 905/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5639 - accuracy: 0.7481 - val_loss: 0.6239 - val_accuracy: 0.6964\n",
            "Epoch 906/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5363 - accuracy: 0.7710 - val_loss: 0.6262 - val_accuracy: 0.6964\n",
            "Epoch 907/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5384 - accuracy: 0.7688 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 908/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5073 - accuracy: 0.7939 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 909/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5262 - accuracy: 0.7786 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 910/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5463 - accuracy: 0.7634 - val_loss: 0.6339 - val_accuracy: 0.6964\n",
            "Epoch 911/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5232 - accuracy: 0.7812 - val_loss: 0.6351 - val_accuracy: 0.6964\n",
            "Epoch 912/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5470 - accuracy: 0.7634 - val_loss: 0.6367 - val_accuracy: 0.6964\n",
            "Epoch 913/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5678 - accuracy: 0.7481 - val_loss: 0.6378 - val_accuracy: 0.6964\n",
            "Epoch 914/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5404 - accuracy: 0.7688 - val_loss: 0.6368 - val_accuracy: 0.6964\n",
            "Epoch 915/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5784 - accuracy: 0.7405 - val_loss: 0.6346 - val_accuracy: 0.6964\n",
            "Epoch 916/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5234 - accuracy: 0.7812 - val_loss: 0.6319 - val_accuracy: 0.6964\n",
            "Epoch 917/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5460 - accuracy: 0.7634 - val_loss: 0.6296 - val_accuracy: 0.6964\n",
            "Epoch 918/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5650 - accuracy: 0.7481 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 919/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5448 - accuracy: 0.7634 - val_loss: 0.6285 - val_accuracy: 0.6964\n",
            "Epoch 920/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5355 - accuracy: 0.7710 - val_loss: 0.6279 - val_accuracy: 0.6964\n",
            "Epoch 921/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5071 - accuracy: 0.7939 - val_loss: 0.6286 - val_accuracy: 0.6964\n",
            "Epoch 922/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5227 - accuracy: 0.7812 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 923/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5460 - accuracy: 0.7634 - val_loss: 0.6293 - val_accuracy: 0.6964\n",
            "Epoch 924/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5258 - accuracy: 0.7786 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 925/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.4985 - accuracy: 0.8015 - val_loss: 0.6233 - val_accuracy: 0.6964\n",
            "Epoch 926/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5381 - accuracy: 0.7688 - val_loss: 0.6219 - val_accuracy: 0.6964\n",
            "Epoch 927/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5534 - accuracy: 0.7557 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 928/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5381 - accuracy: 0.7688 - val_loss: 0.6220 - val_accuracy: 0.6964\n",
            "Epoch 929/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5089 - accuracy: 0.7939 - val_loss: 0.6221 - val_accuracy: 0.6964\n",
            "Epoch 930/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5307 - accuracy: 0.7750 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 931/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4993 - accuracy: 0.8015 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 932/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5615 - accuracy: 0.7481 - val_loss: 0.6210 - val_accuracy: 0.6964\n",
            "Epoch 933/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5537 - accuracy: 0.7557 - val_loss: 0.6177 - val_accuracy: 0.6964\n",
            "Epoch 934/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5364 - accuracy: 0.7710 - val_loss: 0.6167 - val_accuracy: 0.6964\n",
            "Epoch 935/1000\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.5376 - accuracy: 0.7710 - val_loss: 0.6173 - val_accuracy: 0.6964\n",
            "Epoch 936/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5253 - accuracy: 0.7812 - val_loss: 0.6183 - val_accuracy: 0.6964\n",
            "Epoch 937/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5365 - accuracy: 0.7710 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
            "Epoch 938/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5451 - accuracy: 0.7634 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
            "Epoch 939/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5387 - accuracy: 0.7688 - val_loss: 0.6196 - val_accuracy: 0.6964\n",
            "Epoch 940/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5195 - accuracy: 0.7863 - val_loss: 0.6202 - val_accuracy: 0.6964\n",
            "Epoch 941/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5359 - accuracy: 0.7710 - val_loss: 0.6205 - val_accuracy: 0.6964\n",
            "Epoch 942/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5356 - accuracy: 0.7710 - val_loss: 0.6208 - val_accuracy: 0.6964\n",
            "Epoch 943/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5179 - accuracy: 0.7863 - val_loss: 0.6224 - val_accuracy: 0.6964\n",
            "Epoch 944/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5184 - accuracy: 0.7863 - val_loss: 0.6230 - val_accuracy: 0.6964\n",
            "Epoch 945/1000\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.5355 - accuracy: 0.7710 - val_loss: 0.6217 - val_accuracy: 0.6964\n",
            "Epoch 946/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5089 - accuracy: 0.7939 - val_loss: 0.6210 - val_accuracy: 0.6964\n",
            "Epoch 947/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5315 - accuracy: 0.7750 - val_loss: 0.6204 - val_accuracy: 0.6964\n",
            "Epoch 948/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5385 - accuracy: 0.7688 - val_loss: 0.6208 - val_accuracy: 0.6964\n",
            "Epoch 949/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5360 - accuracy: 0.7710 - val_loss: 0.6220 - val_accuracy: 0.6964\n",
            "Epoch 950/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5355 - accuracy: 0.7710 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 951/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5259 - accuracy: 0.7786 - val_loss: 0.6256 - val_accuracy: 0.6964\n",
            "Epoch 952/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5263 - accuracy: 0.7786 - val_loss: 0.6271 - val_accuracy: 0.6964\n",
            "Epoch 953/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5306 - accuracy: 0.7750 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 954/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5385 - accuracy: 0.7688 - val_loss: 0.6281 - val_accuracy: 0.6964\n",
            "Epoch 955/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4873 - accuracy: 0.8092 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 956/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5071 - accuracy: 0.7939 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 957/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.5261 - accuracy: 0.7786 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 958/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5379 - accuracy: 0.7688 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 959/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5256 - accuracy: 0.7786 - val_loss: 0.6265 - val_accuracy: 0.6964\n",
            "Epoch 960/1000\n",
            "5/5 [==============================] - 1s 101ms/step - loss: 0.5304 - accuracy: 0.7750 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 961/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5306 - accuracy: 0.7750 - val_loss: 0.6280 - val_accuracy: 0.6964\n",
            "Epoch 962/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5634 - accuracy: 0.7481 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 963/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5353 - accuracy: 0.7710 - val_loss: 0.6303 - val_accuracy: 0.6964\n",
            "Epoch 964/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5387 - accuracy: 0.7688 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 965/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5751 - accuracy: 0.7405 - val_loss: 0.6309 - val_accuracy: 0.6964\n",
            "Epoch 966/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5309 - accuracy: 0.7750 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 967/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5462 - accuracy: 0.7634 - val_loss: 0.6319 - val_accuracy: 0.6964\n",
            "Epoch 968/1000\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5390 - accuracy: 0.7688 - val_loss: 0.6321 - val_accuracy: 0.6964\n",
            "Epoch 969/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5158 - accuracy: 0.7863 - val_loss: 0.6333 - val_accuracy: 0.6964\n",
            "Epoch 970/1000\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5370 - accuracy: 0.7710 - val_loss: 0.6341 - val_accuracy: 0.6964\n",
            "Epoch 971/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5572 - accuracy: 0.7557 - val_loss: 0.6336 - val_accuracy: 0.6964\n",
            "Epoch 972/1000\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.5470 - accuracy: 0.7634 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 973/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5463 - accuracy: 0.7634 - val_loss: 0.6271 - val_accuracy: 0.6964\n",
            "Epoch 974/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5073 - accuracy: 0.7939 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 975/1000\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.5303 - accuracy: 0.7750 - val_loss: 0.6258 - val_accuracy: 0.6964\n",
            "Epoch 976/1000\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.4888 - accuracy: 0.8092 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 977/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5557 - accuracy: 0.7557 - val_loss: 0.6230 - val_accuracy: 0.6964\n",
            "Epoch 978/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5182 - accuracy: 0.7863 - val_loss: 0.6211 - val_accuracy: 0.6964\n",
            "Epoch 979/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5196 - accuracy: 0.7863 - val_loss: 0.6185 - val_accuracy: 0.6964\n",
            "Epoch 980/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5384 - accuracy: 0.7688 - val_loss: 0.6177 - val_accuracy: 0.6964\n",
            "Epoch 981/1000\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5447 - accuracy: 0.7634 - val_loss: 0.6173 - val_accuracy: 0.6964\n",
            "Epoch 982/1000\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5111 - accuracy: 0.7939 - val_loss: 0.6175 - val_accuracy: 0.6964\n",
            "Epoch 983/1000\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.5390 - accuracy: 0.7688 - val_loss: 0.6189 - val_accuracy: 0.6964\n",
            "Epoch 984/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5614 - accuracy: 0.7481 - val_loss: 0.6196 - val_accuracy: 0.6964\n",
            "Epoch 985/1000\n",
            "5/5 [==============================] - 0s 106ms/step - loss: 0.5185 - accuracy: 0.7863 - val_loss: 0.6223 - val_accuracy: 0.6964\n",
            "Epoch 986/1000\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.5085 - accuracy: 0.7939 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 987/1000\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.5449 - accuracy: 0.7634 - val_loss: 0.6226 - val_accuracy: 0.6964\n",
            "Epoch 988/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5084 - accuracy: 0.7939 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 989/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5352 - accuracy: 0.7710 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 990/1000\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 0.5384 - accuracy: 0.7688 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 991/1000\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.4889 - accuracy: 0.8092 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 992/1000\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5733 - accuracy: 0.7405 - val_loss: 0.6287 - val_accuracy: 0.6964\n",
            "Epoch 993/1000\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.5077 - accuracy: 0.7939 - val_loss: 0.6275 - val_accuracy: 0.6964\n",
            "Epoch 994/1000\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.4879 - accuracy: 0.8092 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 995/1000\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5351 - accuracy: 0.7710 - val_loss: 0.6281 - val_accuracy: 0.6964\n",
            "Epoch 996/1000\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.5550 - accuracy: 0.7557 - val_loss: 0.6289 - val_accuracy: 0.6964\n",
            "Epoch 997/1000\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.5385 - accuracy: 0.7688 - val_loss: 0.6290 - val_accuracy: 0.6964\n",
            "Epoch 998/1000\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5306 - accuracy: 0.7750 - val_loss: 0.6284 - val_accuracy: 0.6964\n",
            "Epoch 999/1000\n",
            "5/5 [==============================] - 0s 107ms/step - loss: 0.5451 - accuracy: 0.7634 - val_loss: 0.6291 - val_accuracy: 0.6964\n",
            "Epoch 1000/1000\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4975 - accuracy: 0.8015 - val_loss: 0.6288 - val_accuracy: 0.6964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "#With 0.001 lr.\n",
        "n_epochs = 100\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODGuQMrKuAat",
        "outputId": "27675ca7-4cf2-46f0-e286-d0f2e4df2e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 12s 298ms/step - loss: 2.5252 - accuracy: 0.2137 - val_loss: 1.1788 - val_accuracy: 0.3036\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.8841 - accuracy: 0.2137 - val_loss: 0.8746 - val_accuracy: 0.3036\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5555 - accuracy: 0.2061 - val_loss: 0.9770 - val_accuracy: 0.3036\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.8629 - accuracy: 0.2366 - val_loss: 1.1501 - val_accuracy: 0.3036\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.9240 - accuracy: 0.2519 - val_loss: 1.1078 - val_accuracy: 0.3036\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7571 - accuracy: 0.2519 - val_loss: 0.6849 - val_accuracy: 0.3036\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5780 - accuracy: 0.2137 - val_loss: 0.6061 - val_accuracy: 0.3036\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 1.7647 - accuracy: 0.2366 - val_loss: 2.8459 - val_accuracy: 0.3036\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 1.7249 - accuracy: 0.2313 - val_loss: 1.7976 - val_accuracy: 0.3036\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.4020 - accuracy: 0.2137 - val_loss: 1.5527 - val_accuracy: 0.3036\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.6104 - accuracy: 0.1985 - val_loss: 0.6008 - val_accuracy: 0.3036\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5203 - accuracy: 0.2214 - val_loss: 0.6451 - val_accuracy: 0.3036\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6243 - accuracy: 0.2061 - val_loss: 0.7600 - val_accuracy: 0.3036\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6123 - accuracy: 0.2519 - val_loss: 0.6592 - val_accuracy: 0.3036\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6178 - accuracy: 0.2366 - val_loss: 0.8678 - val_accuracy: 0.3036\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6570 - accuracy: 0.2443 - val_loss: 0.6392 - val_accuracy: 0.3036\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.7378 - accuracy: 0.2313 - val_loss: 0.6851 - val_accuracy: 0.3036\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5734 - accuracy: 0.2250 - val_loss: 0.6401 - val_accuracy: 0.3036\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5622 - accuracy: 0.2188 - val_loss: 0.7683 - val_accuracy: 0.3036\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6070 - accuracy: 0.2061 - val_loss: 0.5989 - val_accuracy: 0.3036\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5106 - accuracy: 0.2214 - val_loss: 0.7647 - val_accuracy: 0.3036\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.6578 - accuracy: 0.2313 - val_loss: 0.9309 - val_accuracy: 0.3036\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.7444 - accuracy: 0.2137 - val_loss: 0.8866 - val_accuracy: 0.3036\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5462 - accuracy: 0.2137 - val_loss: 0.6077 - val_accuracy: 0.3036\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5579 - accuracy: 0.2137 - val_loss: 0.5979 - val_accuracy: 0.3036\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.6628 - accuracy: 0.2443 - val_loss: 1.0824 - val_accuracy: 0.3036\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.8410 - accuracy: 0.2443 - val_loss: 1.2277 - val_accuracy: 0.3036\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.1125 - accuracy: 0.1985 - val_loss: 2.5288 - val_accuracy: 0.3036\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 2.9988 - accuracy: 0.2519 - val_loss: 1.3394 - val_accuracy: 0.3036\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 2.4833 - accuracy: 0.2313 - val_loss: 3.4372 - val_accuracy: 0.3036\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 1.6976 - accuracy: 0.2214 - val_loss: 0.6103 - val_accuracy: 0.3036\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.9626 - accuracy: 0.2188 - val_loss: 0.6042 - val_accuracy: 0.3036\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7416 - accuracy: 0.2290 - val_loss: 2.0917 - val_accuracy: 0.3036\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.4749 - accuracy: 0.2214 - val_loss: 1.4386 - val_accuracy: 0.3036\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 1.8756 - accuracy: 0.2137 - val_loss: 2.8180 - val_accuracy: 0.3036\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.8693 - accuracy: 0.2595 - val_loss: 0.7581 - val_accuracy: 0.3036\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 2.6692 - accuracy: 0.2214 - val_loss: 4.2503 - val_accuracy: 0.3036\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 2.3249 - accuracy: 0.2137 - val_loss: 2.5406 - val_accuracy: 0.3036\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 1.4054 - accuracy: 0.2250 - val_loss: 3.2612 - val_accuracy: 0.3036\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.8438 - accuracy: 0.2290 - val_loss: 0.8093 - val_accuracy: 0.3036\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 1.6918 - accuracy: 0.2137 - val_loss: 2.5726 - val_accuracy: 0.3036\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.3355 - accuracy: 0.2443 - val_loss: 0.6171 - val_accuracy: 0.3036\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.8752 - accuracy: 0.2137 - val_loss: 0.8464 - val_accuracy: 0.3036\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6831 - accuracy: 0.2137 - val_loss: 0.6562 - val_accuracy: 0.3036\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6582 - accuracy: 0.2443 - val_loss: 0.7016 - val_accuracy: 0.3036\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.6479 - accuracy: 0.2214 - val_loss: 1.3041 - val_accuracy: 0.3036\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.7763 - accuracy: 0.2214 - val_loss: 0.9464 - val_accuracy: 0.3036\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.7247 - accuracy: 0.2061 - val_loss: 1.5538 - val_accuracy: 0.3036\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.9089 - accuracy: 0.2214 - val_loss: 0.8669 - val_accuracy: 0.3036\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.6223 - accuracy: 0.2250 - val_loss: 0.5929 - val_accuracy: 0.3036\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5873 - accuracy: 0.1985 - val_loss: 1.3367 - val_accuracy: 0.3036\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.9446 - accuracy: 0.2290 - val_loss: 0.9629 - val_accuracy: 0.3036\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.9787 - accuracy: 0.2290 - val_loss: 0.8605 - val_accuracy: 0.3036\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.8550 - accuracy: 0.1985 - val_loss: 1.2659 - val_accuracy: 0.3036\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.8397 - accuracy: 0.2214 - val_loss: 1.1460 - val_accuracy: 0.3036\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.8276 - accuracy: 0.2313 - val_loss: 0.6800 - val_accuracy: 0.3036\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.8478 - accuracy: 0.2290 - val_loss: 0.5947 - val_accuracy: 0.3036\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6099 - accuracy: 0.2137 - val_loss: 0.7485 - val_accuracy: 0.3036\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.6756 - accuracy: 0.2313 - val_loss: 0.8480 - val_accuracy: 0.3036\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7620 - accuracy: 0.2214 - val_loss: 0.9635 - val_accuracy: 0.3036\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 1.2220 - accuracy: 0.2188 - val_loss: 1.0795 - val_accuracy: 0.3036\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.8222 - accuracy: 0.2250 - val_loss: 1.6727 - val_accuracy: 0.3036\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 1.2033 - accuracy: 0.2214 - val_loss: 0.6738 - val_accuracy: 0.3036\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.7002 - accuracy: 0.2290 - val_loss: 1.1449 - val_accuracy: 0.3036\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.9738 - accuracy: 0.2290 - val_loss: 0.6930 - val_accuracy: 0.3036\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 1.2418 - accuracy: 0.2443 - val_loss: 1.4237 - val_accuracy: 0.3036\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.8688 - accuracy: 0.2137 - val_loss: 0.8611 - val_accuracy: 0.3036\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6499 - accuracy: 0.2366 - val_loss: 0.8378 - val_accuracy: 0.3036\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.6575 - accuracy: 0.2214 - val_loss: 1.3530 - val_accuracy: 0.3036\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.3722 - accuracy: 0.2519 - val_loss: 1.4888 - val_accuracy: 0.3036\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.7553 - accuracy: 0.1908 - val_loss: 0.9094 - val_accuracy: 0.3036\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6080 - accuracy: 0.2443 - val_loss: 0.9103 - val_accuracy: 0.3036\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.7119 - accuracy: 0.2519 - val_loss: 0.7323 - val_accuracy: 0.3036\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6990 - accuracy: 0.2214 - val_loss: 0.5927 - val_accuracy: 0.3036\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6678 - accuracy: 0.2214 - val_loss: 0.5927 - val_accuracy: 0.3036\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.7607 - accuracy: 0.2214 - val_loss: 0.7566 - val_accuracy: 0.3036\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.6965 - accuracy: 0.2214 - val_loss: 1.8025 - val_accuracy: 0.3036\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 1.0241 - accuracy: 0.2443 - val_loss: 0.5987 - val_accuracy: 0.3036\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5799 - accuracy: 0.2443 - val_loss: 0.6736 - val_accuracy: 0.3036\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5368 - accuracy: 0.2313 - val_loss: 0.5947 - val_accuracy: 0.3036\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5582 - accuracy: 0.2290 - val_loss: 0.9694 - val_accuracy: 0.3036\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.7515 - accuracy: 0.2061 - val_loss: 0.6340 - val_accuracy: 0.3036\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6618 - accuracy: 0.2443 - val_loss: 0.7202 - val_accuracy: 0.3036\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6470 - accuracy: 0.2290 - val_loss: 0.9875 - val_accuracy: 0.3036\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.6741 - accuracy: 0.2061 - val_loss: 0.6063 - val_accuracy: 0.3036\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5656 - accuracy: 0.2366 - val_loss: 0.5977 - val_accuracy: 0.3036\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5418 - accuracy: 0.2519 - val_loss: 0.6450 - val_accuracy: 0.3036\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5422 - accuracy: 0.2443 - val_loss: 0.5979 - val_accuracy: 0.3036\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.6564 - accuracy: 0.2214 - val_loss: 0.6868 - val_accuracy: 0.3036\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5817 - accuracy: 0.2214 - val_loss: 0.7939 - val_accuracy: 0.3036\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5540 - accuracy: 0.2214 - val_loss: 0.6724 - val_accuracy: 0.3036\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.7064 - accuracy: 0.2519 - val_loss: 2.1639 - val_accuracy: 0.3036\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 1.2786 - accuracy: 0.2290 - val_loss: 1.1236 - val_accuracy: 0.3036\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 1.1066 - accuracy: 0.2366 - val_loss: 1.0895 - val_accuracy: 0.3036\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.7642 - accuracy: 0.2061 - val_loss: 1.6766 - val_accuracy: 0.3036\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.4231 - accuracy: 0.2290 - val_loss: 0.7620 - val_accuracy: 0.3036\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.9252 - accuracy: 0.2250 - val_loss: 1.5716 - val_accuracy: 0.3036\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.9210 - accuracy: 0.2137 - val_loss: 1.3814 - val_accuracy: 0.3036\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 2.1326 - accuracy: 0.2061 - val_loss: 2.9113 - val_accuracy: 0.3036\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.4859 - accuracy: 0.2443 - val_loss: 0.9264 - val_accuracy: 0.3036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xception"
      ],
      "metadata": {
        "id": "ABIV6nhh0cYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "9_IVjH-D0cYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_func = tensorflow.keras.applications.xception.preprocess_input"
      ],
      "metadata": {
        "id": "ZX8JZQGW0cYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "train_gen = train_datagen.flow_from_directory('/content/train', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(train_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab06f7d-1350-4df1-eea6-79080bfa1b16",
        "id": "LVrMCy-R0cYO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 163 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "test_gen = train_datagen.flow_from_directory('/content/test', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(test_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b891700-461b-42a5-9e03-b25ce1cc7a75",
        "id": "7bbBv0EK0cYO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 56 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.python.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "\n",
        "\n",
        "pretrained_model = Xception(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "pretrained_model.trainable = False\n",
        "model = Sequential()\n",
        "model.add(pretrained_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "Tw305hXO0cYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001,\n",
        "    # learning_rate=0.00001,\n",
        "    learning_rate=0.0001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optim)"
      ],
      "metadata": {
        "id": "3In_whw10cYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  print(layer.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b738ef5e-a98d-4ce6-cda7-0ec0d04d8735",
        "id": "8-M6Rn1i0cYP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#With 0.0001 lr.\n",
        "n_epochs = 1000\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95b9ccb3-2405-48dd-baf1-a839528a7099",
        "id": "FCKiXUFS0cYP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - 4s 351ms/step - loss: 0.6495 - accuracy: 0.6336 - val_loss: 0.6188 - val_accuracy: 0.6964\n",
            "Epoch 2/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5437 - accuracy: 0.7863 - val_loss: 0.6212 - val_accuracy: 0.6964\n",
            "Epoch 3/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5089 - accuracy: 0.7939 - val_loss: 0.6554 - val_accuracy: 0.6964\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5631 - accuracy: 0.7634 - val_loss: 0.6747 - val_accuracy: 0.6964\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5898 - accuracy: 0.7481 - val_loss: 0.6548 - val_accuracy: 0.6964\n",
            "Epoch 6/1000\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.4974 - accuracy: 0.8015 - val_loss: 0.6468 - val_accuracy: 0.6964\n",
            "Epoch 7/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5353 - accuracy: 0.7750 - val_loss: 0.6351 - val_accuracy: 0.6964\n",
            "Epoch 8/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5274 - accuracy: 0.7786 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 9/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5405 - accuracy: 0.7710 - val_loss: 0.6171 - val_accuracy: 0.6964\n",
            "Epoch 10/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5139 - accuracy: 0.7939 - val_loss: 0.6172 - val_accuracy: 0.6964\n",
            "Epoch 11/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5040 - accuracy: 0.8015 - val_loss: 0.6179 - val_accuracy: 0.6964\n",
            "Epoch 12/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5017 - accuracy: 0.8015 - val_loss: 0.6224 - val_accuracy: 0.6964\n",
            "Epoch 13/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5580 - accuracy: 0.7557 - val_loss: 0.6307 - val_accuracy: 0.6964\n",
            "Epoch 14/1000\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5094 - accuracy: 0.7939 - val_loss: 0.6299 - val_accuracy: 0.6964\n",
            "Epoch 15/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5079 - accuracy: 0.7939 - val_loss: 0.6291 - val_accuracy: 0.6964\n",
            "Epoch 16/1000\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5317 - accuracy: 0.7750 - val_loss: 0.6263 - val_accuracy: 0.6964\n",
            "Epoch 17/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5263 - accuracy: 0.7786 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 18/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5360 - accuracy: 0.7710 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 19/1000\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5242 - accuracy: 0.7812 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 20/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5737 - accuracy: 0.7405 - val_loss: 0.6196 - val_accuracy: 0.6964\n",
            "Epoch 21/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5276 - accuracy: 0.7786 - val_loss: 0.6196 - val_accuracy: 0.6964\n",
            "Epoch 22/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5668 - accuracy: 0.7481 - val_loss: 0.6299 - val_accuracy: 0.6964\n",
            "Epoch 23/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5372 - accuracy: 0.7710 - val_loss: 0.6339 - val_accuracy: 0.6964\n",
            "Epoch 24/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5592 - accuracy: 0.7557 - val_loss: 0.6403 - val_accuracy: 0.6964\n",
            "Epoch 25/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5193 - accuracy: 0.7863 - val_loss: 0.6468 - val_accuracy: 0.6964\n",
            "Epoch 26/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5784 - accuracy: 0.7481 - val_loss: 0.6502 - val_accuracy: 0.6964\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5655 - accuracy: 0.7557 - val_loss: 0.6362 - val_accuracy: 0.6964\n",
            "Epoch 28/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5782 - accuracy: 0.7405 - val_loss: 0.6221 - val_accuracy: 0.6964\n",
            "Epoch 29/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5464 - accuracy: 0.7634 - val_loss: 0.6128 - val_accuracy: 0.6964\n",
            "Epoch 30/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5516 - accuracy: 0.7634 - val_loss: 0.6111 - val_accuracy: 0.6964\n",
            "Epoch 31/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5393 - accuracy: 0.7786 - val_loss: 0.6115 - val_accuracy: 0.6964\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5398 - accuracy: 0.7710 - val_loss: 0.6118 - val_accuracy: 0.6964\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5250 - accuracy: 0.7863 - val_loss: 0.6140 - val_accuracy: 0.6964\n",
            "Epoch 34/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5303 - accuracy: 0.7750 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 35/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5205 - accuracy: 0.7863 - val_loss: 0.6388 - val_accuracy: 0.6964\n",
            "Epoch 36/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5675 - accuracy: 0.7557 - val_loss: 0.6470 - val_accuracy: 0.6964\n",
            "Epoch 37/1000\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.5406 - accuracy: 0.7710 - val_loss: 0.6357 - val_accuracy: 0.6964\n",
            "Epoch 38/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5370 - accuracy: 0.7710 - val_loss: 0.6299 - val_accuracy: 0.6964\n",
            "Epoch 39/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5573 - accuracy: 0.7557 - val_loss: 0.6176 - val_accuracy: 0.6964\n",
            "Epoch 40/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5258 - accuracy: 0.7786 - val_loss: 0.6175 - val_accuracy: 0.6964\n",
            "Epoch 41/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5448 - accuracy: 0.7634 - val_loss: 0.6199 - val_accuracy: 0.6964\n",
            "Epoch 42/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5536 - accuracy: 0.7557 - val_loss: 0.6223 - val_accuracy: 0.6964\n",
            "Epoch 43/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5336 - accuracy: 0.7710 - val_loss: 0.6249 - val_accuracy: 0.6964\n",
            "Epoch 44/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5207 - accuracy: 0.7812 - val_loss: 0.6288 - val_accuracy: 0.6964\n",
            "Epoch 45/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5466 - accuracy: 0.7634 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 46/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5254 - accuracy: 0.7786 - val_loss: 0.6213 - val_accuracy: 0.6964\n",
            "Epoch 47/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5502 - accuracy: 0.7557 - val_loss: 0.6121 - val_accuracy: 0.6964\n",
            "Epoch 48/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5493 - accuracy: 0.7634 - val_loss: 0.6104 - val_accuracy: 0.6964\n",
            "Epoch 49/1000\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5452 - accuracy: 0.7688 - val_loss: 0.6104 - val_accuracy: 0.6964\n",
            "Epoch 50/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5318 - accuracy: 0.7786 - val_loss: 0.6127 - val_accuracy: 0.6964\n",
            "Epoch 51/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5340 - accuracy: 0.7710 - val_loss: 0.6197 - val_accuracy: 0.6964\n",
            "Epoch 52/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4951 - accuracy: 0.8015 - val_loss: 0.6311 - val_accuracy: 0.6964\n",
            "Epoch 53/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5401 - accuracy: 0.7710 - val_loss: 0.6441 - val_accuracy: 0.6964\n",
            "Epoch 54/1000\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.5465 - accuracy: 0.7710 - val_loss: 0.6530 - val_accuracy: 0.6964\n",
            "Epoch 55/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5202 - accuracy: 0.7863 - val_loss: 0.6434 - val_accuracy: 0.6964\n",
            "Epoch 56/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5267 - accuracy: 0.7786 - val_loss: 0.6331 - val_accuracy: 0.6964\n",
            "Epoch 57/1000\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5380 - accuracy: 0.7688 - val_loss: 0.6268 - val_accuracy: 0.6964\n",
            "Epoch 58/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5122 - accuracy: 0.7863 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 59/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5146 - accuracy: 0.7863 - val_loss: 0.6269 - val_accuracy: 0.6964\n",
            "Epoch 60/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5568 - accuracy: 0.7557 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
            "Epoch 61/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5207 - accuracy: 0.7812 - val_loss: 0.6163 - val_accuracy: 0.6964\n",
            "Epoch 62/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5337 - accuracy: 0.7710 - val_loss: 0.6149 - val_accuracy: 0.6964\n",
            "Epoch 63/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5680 - accuracy: 0.7405 - val_loss: 0.6140 - val_accuracy: 0.6964\n",
            "Epoch 64/1000\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5283 - accuracy: 0.7750 - val_loss: 0.6181 - val_accuracy: 0.6964\n",
            "Epoch 65/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5324 - accuracy: 0.7710 - val_loss: 0.6266 - val_accuracy: 0.6964\n",
            "Epoch 66/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4725 - accuracy: 0.8168 - val_loss: 0.6426 - val_accuracy: 0.6964\n",
            "Epoch 67/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5193 - accuracy: 0.7863 - val_loss: 0.6491 - val_accuracy: 0.6964\n",
            "Epoch 68/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5518 - accuracy: 0.7634 - val_loss: 0.6350 - val_accuracy: 0.6964\n",
            "Epoch 69/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5540 - accuracy: 0.7557 - val_loss: 0.6169 - val_accuracy: 0.6964\n",
            "Epoch 70/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5230 - accuracy: 0.7786 - val_loss: 0.6134 - val_accuracy: 0.6964\n",
            "Epoch 71/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5335 - accuracy: 0.7710 - val_loss: 0.6147 - val_accuracy: 0.6964\n",
            "Epoch 72/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5334 - accuracy: 0.7710 - val_loss: 0.6218 - val_accuracy: 0.6964\n",
            "Epoch 73/1000\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.5607 - accuracy: 0.7481 - val_loss: 0.6230 - val_accuracy: 0.6964\n",
            "Epoch 74/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5412 - accuracy: 0.7634 - val_loss: 0.6278 - val_accuracy: 0.6964\n",
            "Epoch 75/1000\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.5525 - accuracy: 0.7557 - val_loss: 0.6315 - val_accuracy: 0.6964\n",
            "Epoch 76/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5145 - accuracy: 0.7863 - val_loss: 0.6415 - val_accuracy: 0.6964\n",
            "Epoch 77/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5165 - accuracy: 0.7863 - val_loss: 0.6532 - val_accuracy: 0.6964\n",
            "Epoch 78/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5537 - accuracy: 0.7634 - val_loss: 0.6469 - val_accuracy: 0.6964\n",
            "Epoch 79/1000\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5607 - accuracy: 0.7557 - val_loss: 0.6219 - val_accuracy: 0.6964\n",
            "Epoch 80/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5681 - accuracy: 0.7405 - val_loss: 0.6134 - val_accuracy: 0.6964\n",
            "Epoch 81/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5657 - accuracy: 0.7405 - val_loss: 0.6092 - val_accuracy: 0.6964\n",
            "Epoch 82/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5584 - accuracy: 0.7557 - val_loss: 0.6092 - val_accuracy: 0.6964\n",
            "Epoch 83/1000\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5361 - accuracy: 0.7710 - val_loss: 0.6099 - val_accuracy: 0.6964\n",
            "Epoch 84/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5329 - accuracy: 0.7710 - val_loss: 0.6143 - val_accuracy: 0.6964\n",
            "Epoch 85/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5306 - accuracy: 0.7710 - val_loss: 0.6194 - val_accuracy: 0.6964\n",
            "Epoch 86/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5099 - accuracy: 0.7863 - val_loss: 0.6205 - val_accuracy: 0.6964\n",
            "Epoch 87/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5219 - accuracy: 0.7786 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
            "Epoch 88/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5310 - accuracy: 0.7710 - val_loss: 0.6167 - val_accuracy: 0.6964\n",
            "Epoch 89/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5498 - accuracy: 0.7557 - val_loss: 0.6111 - val_accuracy: 0.6964\n",
            "Epoch 90/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5355 - accuracy: 0.7688 - val_loss: 0.6088 - val_accuracy: 0.6964\n",
            "Epoch 91/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5130 - accuracy: 0.7939 - val_loss: 0.6097 - val_accuracy: 0.6964\n",
            "Epoch 92/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5530 - accuracy: 0.7481 - val_loss: 0.6153 - val_accuracy: 0.6964\n",
            "Epoch 93/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5026 - accuracy: 0.7939 - val_loss: 0.6329 - val_accuracy: 0.6964\n",
            "Epoch 94/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5453 - accuracy: 0.7634 - val_loss: 0.6478 - val_accuracy: 0.6964\n",
            "Epoch 95/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.4985 - accuracy: 0.8015 - val_loss: 0.6675 - val_accuracy: 0.6964\n",
            "Epoch 96/1000\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.5634 - accuracy: 0.7634 - val_loss: 0.6602 - val_accuracy: 0.6964\n",
            "Epoch 97/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5583 - accuracy: 0.7634 - val_loss: 0.6413 - val_accuracy: 0.6964\n",
            "Epoch 98/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5574 - accuracy: 0.7557 - val_loss: 0.6245 - val_accuracy: 0.6964\n",
            "Epoch 99/1000\n",
            "5/5 [==============================] - 1s 124ms/step - loss: 0.5148 - accuracy: 0.7863 - val_loss: 0.6178 - val_accuracy: 0.6964\n",
            "Epoch 100/1000\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5509 - accuracy: 0.7557 - val_loss: 0.6234 - val_accuracy: 0.6964\n",
            "Epoch 101/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5148 - accuracy: 0.7863 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 102/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5373 - accuracy: 0.7634 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 103/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5128 - accuracy: 0.7863 - val_loss: 0.6188 - val_accuracy: 0.6964\n",
            "Epoch 104/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5156 - accuracy: 0.7863 - val_loss: 0.6168 - val_accuracy: 0.6964\n",
            "Epoch 105/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5324 - accuracy: 0.7688 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 106/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5375 - accuracy: 0.7634 - val_loss: 0.6109 - val_accuracy: 0.6964\n",
            "Epoch 107/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5504 - accuracy: 0.7557 - val_loss: 0.6088 - val_accuracy: 0.6964\n",
            "Epoch 108/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5192 - accuracy: 0.7863 - val_loss: 0.6086 - val_accuracy: 0.6964\n",
            "Epoch 109/1000\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5116 - accuracy: 0.7939 - val_loss: 0.6094 - val_accuracy: 0.6964\n",
            "Epoch 110/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5118 - accuracy: 0.7863 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
            "Epoch 111/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5338 - accuracy: 0.7688 - val_loss: 0.6300 - val_accuracy: 0.6964\n",
            "Epoch 112/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5364 - accuracy: 0.7688 - val_loss: 0.6309 - val_accuracy: 0.6964\n",
            "Epoch 113/1000\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5349 - accuracy: 0.7688 - val_loss: 0.6245 - val_accuracy: 0.6964\n",
            "Epoch 114/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5200 - accuracy: 0.7786 - val_loss: 0.6183 - val_accuracy: 0.6964\n",
            "Epoch 115/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5082 - accuracy: 0.7863 - val_loss: 0.6226 - val_accuracy: 0.6964\n",
            "Epoch 116/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5323 - accuracy: 0.7710 - val_loss: 0.6274 - val_accuracy: 0.6964\n",
            "Epoch 117/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5422 - accuracy: 0.7634 - val_loss: 0.6232 - val_accuracy: 0.6964\n",
            "Epoch 118/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5274 - accuracy: 0.7710 - val_loss: 0.6197 - val_accuracy: 0.6964\n",
            "Epoch 119/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5423 - accuracy: 0.7634 - val_loss: 0.6171 - val_accuracy: 0.6964\n",
            "Epoch 120/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5232 - accuracy: 0.7786 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 121/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5217 - accuracy: 0.7786 - val_loss: 0.6198 - val_accuracy: 0.6964\n",
            "Epoch 122/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5269 - accuracy: 0.7710 - val_loss: 0.6220 - val_accuracy: 0.6964\n",
            "Epoch 123/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.4994 - accuracy: 0.7939 - val_loss: 0.6362 - val_accuracy: 0.6964\n",
            "Epoch 124/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5745 - accuracy: 0.7481 - val_loss: 0.6421 - val_accuracy: 0.6964\n",
            "Epoch 125/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5401 - accuracy: 0.7688 - val_loss: 0.6374 - val_accuracy: 0.6964\n",
            "Epoch 126/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5453 - accuracy: 0.7634 - val_loss: 0.6277 - val_accuracy: 0.6964\n",
            "Epoch 127/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5679 - accuracy: 0.7481 - val_loss: 0.6140 - val_accuracy: 0.6964\n",
            "Epoch 128/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5467 - accuracy: 0.7557 - val_loss: 0.6120 - val_accuracy: 0.6964\n",
            "Epoch 129/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5446 - accuracy: 0.7557 - val_loss: 0.6132 - val_accuracy: 0.6964\n",
            "Epoch 130/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5200 - accuracy: 0.7786 - val_loss: 0.6181 - val_accuracy: 0.6964\n",
            "Epoch 131/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5170 - accuracy: 0.7786 - val_loss: 0.6254 - val_accuracy: 0.6964\n",
            "Epoch 132/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5517 - accuracy: 0.7557 - val_loss: 0.6223 - val_accuracy: 0.6964\n",
            "Epoch 133/1000\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5311 - accuracy: 0.7688 - val_loss: 0.6156 - val_accuracy: 0.6964\n",
            "Epoch 134/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.4938 - accuracy: 0.8015 - val_loss: 0.6136 - val_accuracy: 0.6964\n",
            "Epoch 135/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5303 - accuracy: 0.7710 - val_loss: 0.6094 - val_accuracy: 0.6964\n",
            "Epoch 136/1000\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.5183 - accuracy: 0.7863 - val_loss: 0.6078 - val_accuracy: 0.6964\n",
            "Epoch 137/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5437 - accuracy: 0.7634 - val_loss: 0.6078 - val_accuracy: 0.6964\n",
            "Epoch 138/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5278 - accuracy: 0.7750 - val_loss: 0.6094 - val_accuracy: 0.6964\n",
            "Epoch 139/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5253 - accuracy: 0.7786 - val_loss: 0.6088 - val_accuracy: 0.6964\n",
            "Epoch 140/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5089 - accuracy: 0.7939 - val_loss: 0.6112 - val_accuracy: 0.6964\n",
            "Epoch 141/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.5029 - accuracy: 0.7863 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 142/1000\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5419 - accuracy: 0.7634 - val_loss: 0.6345 - val_accuracy: 0.6964\n",
            "Epoch 143/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5255 - accuracy: 0.7786 - val_loss: 0.6281 - val_accuracy: 0.6964\n",
            "Epoch 144/1000\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5284 - accuracy: 0.7710 - val_loss: 0.6080 - val_accuracy: 0.6964\n",
            "Epoch 145/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5341 - accuracy: 0.7750 - val_loss: 0.6091 - val_accuracy: 0.6964\n",
            "Epoch 146/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5453 - accuracy: 0.7634 - val_loss: 0.6081 - val_accuracy: 0.6964\n",
            "Epoch 147/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.4928 - accuracy: 0.8015 - val_loss: 0.6208 - val_accuracy: 0.6964\n",
            "Epoch 148/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.4526 - accuracy: 0.8244 - val_loss: 0.6470 - val_accuracy: 0.6964\n",
            "Epoch 149/1000\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5982 - accuracy: 0.7405 - val_loss: 0.6683 - val_accuracy: 0.6964\n",
            "Epoch 150/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5672 - accuracy: 0.7557 - val_loss: 0.6480 - val_accuracy: 0.6964\n",
            "Epoch 151/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5381 - accuracy: 0.7710 - val_loss: 0.6279 - val_accuracy: 0.6964\n",
            "Epoch 152/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.5064 - accuracy: 0.7863 - val_loss: 0.6183 - val_accuracy: 0.6964\n",
            "Epoch 153/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5379 - accuracy: 0.7634 - val_loss: 0.6155 - val_accuracy: 0.6964\n",
            "Epoch 154/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4912 - accuracy: 0.8015 - val_loss: 0.6110 - val_accuracy: 0.6964\n",
            "Epoch 155/1000\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5257 - accuracy: 0.7710 - val_loss: 0.6183 - val_accuracy: 0.6964\n",
            "Epoch 156/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.4988 - accuracy: 0.7939 - val_loss: 0.6276 - val_accuracy: 0.6964\n",
            "Epoch 157/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5249 - accuracy: 0.7750 - val_loss: 0.6302 - val_accuracy: 0.6964\n",
            "Epoch 158/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5241 - accuracy: 0.7750 - val_loss: 0.6230 - val_accuracy: 0.6964\n",
            "Epoch 159/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5370 - accuracy: 0.7634 - val_loss: 0.6227 - val_accuracy: 0.6964\n",
            "Epoch 160/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5145 - accuracy: 0.7812 - val_loss: 0.6218 - val_accuracy: 0.6964\n",
            "Epoch 161/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5461 - accuracy: 0.7557 - val_loss: 0.6164 - val_accuracy: 0.6964\n",
            "Epoch 162/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.5177 - accuracy: 0.7786 - val_loss: 0.6122 - val_accuracy: 0.6964\n",
            "Epoch 163/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5536 - accuracy: 0.7481 - val_loss: 0.6142 - val_accuracy: 0.6964\n",
            "Epoch 164/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5282 - accuracy: 0.7710 - val_loss: 0.6188 - val_accuracy: 0.6964\n",
            "Epoch 165/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5178 - accuracy: 0.7786 - val_loss: 0.6177 - val_accuracy: 0.6964\n",
            "Epoch 166/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5534 - accuracy: 0.7481 - val_loss: 0.6151 - val_accuracy: 0.6964\n",
            "Epoch 167/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5164 - accuracy: 0.7786 - val_loss: 0.6182 - val_accuracy: 0.6964\n",
            "Epoch 168/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5203 - accuracy: 0.7786 - val_loss: 0.6147 - val_accuracy: 0.6964\n",
            "Epoch 169/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5074 - accuracy: 0.7863 - val_loss: 0.6168 - val_accuracy: 0.6964\n",
            "Epoch 170/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5076 - accuracy: 0.7863 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 171/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5401 - accuracy: 0.7634 - val_loss: 0.6231 - val_accuracy: 0.6964\n",
            "Epoch 172/1000\n",
            "5/5 [==============================] - 1s 124ms/step - loss: 0.4953 - accuracy: 0.7939 - val_loss: 0.6177 - val_accuracy: 0.6964\n",
            "Epoch 173/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5213 - accuracy: 0.7750 - val_loss: 0.6123 - val_accuracy: 0.6964\n",
            "Epoch 174/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.4983 - accuracy: 0.7939 - val_loss: 0.6170 - val_accuracy: 0.6964\n",
            "Epoch 175/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.4866 - accuracy: 0.8015 - val_loss: 0.6297 - val_accuracy: 0.6964\n",
            "Epoch 176/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5332 - accuracy: 0.7688 - val_loss: 0.6322 - val_accuracy: 0.6964\n",
            "Epoch 177/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5427 - accuracy: 0.7634 - val_loss: 0.6244 - val_accuracy: 0.6964\n",
            "Epoch 178/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5323 - accuracy: 0.7634 - val_loss: 0.6138 - val_accuracy: 0.6964\n",
            "Epoch 179/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5417 - accuracy: 0.7557 - val_loss: 0.6080 - val_accuracy: 0.6964\n",
            "Epoch 180/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5328 - accuracy: 0.7710 - val_loss: 0.6074 - val_accuracy: 0.6964\n",
            "Epoch 181/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5169 - accuracy: 0.7863 - val_loss: 0.6075 - val_accuracy: 0.6964\n",
            "Epoch 182/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5303 - accuracy: 0.7750 - val_loss: 0.6088 - val_accuracy: 0.6964\n",
            "Epoch 183/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5295 - accuracy: 0.7786 - val_loss: 0.6077 - val_accuracy: 0.6964\n",
            "Epoch 184/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.5151 - accuracy: 0.7863 - val_loss: 0.6104 - val_accuracy: 0.6964\n",
            "Epoch 185/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5187 - accuracy: 0.7710 - val_loss: 0.6204 - val_accuracy: 0.6964\n",
            "Epoch 186/1000\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5365 - accuracy: 0.7634 - val_loss: 0.6229 - val_accuracy: 0.6964\n",
            "Epoch 187/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5141 - accuracy: 0.7786 - val_loss: 0.6243 - val_accuracy: 0.6964\n",
            "Epoch 188/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5147 - accuracy: 0.7786 - val_loss: 0.6306 - val_accuracy: 0.6964\n",
            "Epoch 189/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5167 - accuracy: 0.7786 - val_loss: 0.6339 - val_accuracy: 0.6964\n",
            "Epoch 190/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5077 - accuracy: 0.7863 - val_loss: 0.6292 - val_accuracy: 0.6964\n",
            "Epoch 191/1000\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5079 - accuracy: 0.7863 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
            "Epoch 192/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5099 - accuracy: 0.7863 - val_loss: 0.6116 - val_accuracy: 0.6964\n",
            "Epoch 193/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5040 - accuracy: 0.7863 - val_loss: 0.6148 - val_accuracy: 0.6964\n",
            "Epoch 194/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.5384 - accuracy: 0.7634 - val_loss: 0.6155 - val_accuracy: 0.6964\n",
            "Epoch 195/1000\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.4839 - accuracy: 0.8015 - val_loss: 0.6135 - val_accuracy: 0.6964\n",
            "Epoch 196/1000\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5177 - accuracy: 0.7786 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
            "Epoch 197/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5457 - accuracy: 0.7557 - val_loss: 0.6181 - val_accuracy: 0.6964\n",
            "Epoch 198/1000\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5037 - accuracy: 0.7863 - val_loss: 0.6208 - val_accuracy: 0.6964\n",
            "Epoch 199/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5130 - accuracy: 0.7786 - val_loss: 0.6147 - val_accuracy: 0.6964\n",
            "Epoch 200/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5149 - accuracy: 0.7786 - val_loss: 0.6103 - val_accuracy: 0.6964\n",
            "Epoch 201/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5410 - accuracy: 0.7557 - val_loss: 0.6077 - val_accuracy: 0.6964\n",
            "Epoch 202/1000\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5410 - accuracy: 0.7557 - val_loss: 0.6081 - val_accuracy: 0.6964\n",
            "Epoch 203/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5264 - accuracy: 0.7710 - val_loss: 0.6156 - val_accuracy: 0.6964\n",
            "Epoch 204/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5293 - accuracy: 0.7634 - val_loss: 0.6171 - val_accuracy: 0.6964\n",
            "Epoch 205/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5186 - accuracy: 0.7750 - val_loss: 0.6162 - val_accuracy: 0.6964\n",
            "Epoch 206/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5149 - accuracy: 0.7786 - val_loss: 0.6090 - val_accuracy: 0.6964\n",
            "Epoch 207/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5313 - accuracy: 0.7634 - val_loss: 0.6079 - val_accuracy: 0.6964\n",
            "Epoch 208/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5267 - accuracy: 0.7710 - val_loss: 0.6128 - val_accuracy: 0.6964\n",
            "Epoch 209/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5240 - accuracy: 0.7710 - val_loss: 0.6240 - val_accuracy: 0.6964\n",
            "Epoch 210/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5445 - accuracy: 0.7557 - val_loss: 0.6264 - val_accuracy: 0.6964\n",
            "Epoch 211/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5079 - accuracy: 0.7863 - val_loss: 0.6246 - val_accuracy: 0.6964\n",
            "Epoch 212/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5019 - accuracy: 0.7863 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 213/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5452 - accuracy: 0.7557 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
            "Epoch 214/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5299 - accuracy: 0.7634 - val_loss: 0.6152 - val_accuracy: 0.6964\n",
            "Epoch 215/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5166 - accuracy: 0.7786 - val_loss: 0.6131 - val_accuracy: 0.6964\n",
            "Epoch 216/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5370 - accuracy: 0.7557 - val_loss: 0.6102 - val_accuracy: 0.6964\n",
            "Epoch 217/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5504 - accuracy: 0.7557 - val_loss: 0.6082 - val_accuracy: 0.6964\n",
            "Epoch 218/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5202 - accuracy: 0.7863 - val_loss: 0.6081 - val_accuracy: 0.6964\n",
            "Epoch 219/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5380 - accuracy: 0.7634 - val_loss: 0.6081 - val_accuracy: 0.6964\n",
            "Epoch 220/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5374 - accuracy: 0.7634 - val_loss: 0.6205 - val_accuracy: 0.6964\n",
            "Epoch 221/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5192 - accuracy: 0.7750 - val_loss: 0.6336 - val_accuracy: 0.6964\n",
            "Epoch 222/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5253 - accuracy: 0.7710 - val_loss: 0.6282 - val_accuracy: 0.6964\n",
            "Epoch 223/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.4930 - accuracy: 0.7939 - val_loss: 0.6252 - val_accuracy: 0.6964\n",
            "Epoch 224/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5467 - accuracy: 0.7557 - val_loss: 0.6215 - val_accuracy: 0.6964\n",
            "Epoch 225/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5305 - accuracy: 0.7634 - val_loss: 0.6170 - val_accuracy: 0.6964\n",
            "Epoch 226/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5418 - accuracy: 0.7557 - val_loss: 0.6130 - val_accuracy: 0.6964\n",
            "Epoch 227/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5234 - accuracy: 0.7688 - val_loss: 0.6106 - val_accuracy: 0.6964\n",
            "Epoch 228/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5490 - accuracy: 0.7481 - val_loss: 0.6109 - val_accuracy: 0.6964\n",
            "Epoch 229/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5243 - accuracy: 0.7688 - val_loss: 0.6137 - val_accuracy: 0.6964\n",
            "Epoch 230/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5570 - accuracy: 0.7405 - val_loss: 0.6151 - val_accuracy: 0.6964\n",
            "Epoch 231/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5131 - accuracy: 0.7786 - val_loss: 0.6167 - val_accuracy: 0.6964\n",
            "Epoch 232/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5396 - accuracy: 0.7557 - val_loss: 0.6103 - val_accuracy: 0.6964\n",
            "Epoch 233/1000\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.4912 - accuracy: 0.8015 - val_loss: 0.6118 - val_accuracy: 0.6964\n",
            "Epoch 234/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5001 - accuracy: 0.7863 - val_loss: 0.6139 - val_accuracy: 0.6964\n",
            "Epoch 235/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5407 - accuracy: 0.7557 - val_loss: 0.6141 - val_accuracy: 0.6964\n",
            "Epoch 236/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5396 - accuracy: 0.7557 - val_loss: 0.6170 - val_accuracy: 0.6964\n",
            "Epoch 237/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5128 - accuracy: 0.7786 - val_loss: 0.6157 - val_accuracy: 0.6964\n",
            "Epoch 238/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.4964 - accuracy: 0.7939 - val_loss: 0.6164 - val_accuracy: 0.6964\n",
            "Epoch 239/1000\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5329 - accuracy: 0.7634 - val_loss: 0.6172 - val_accuracy: 0.6964\n",
            "Epoch 240/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5067 - accuracy: 0.7863 - val_loss: 0.6140 - val_accuracy: 0.6964\n",
            "Epoch 241/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.4928 - accuracy: 0.7939 - val_loss: 0.6146 - val_accuracy: 0.6964\n",
            "Epoch 242/1000\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5174 - accuracy: 0.7750 - val_loss: 0.6197 - val_accuracy: 0.6964\n",
            "Epoch 243/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5210 - accuracy: 0.7710 - val_loss: 0.6176 - val_accuracy: 0.6964\n",
            "Epoch 244/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5090 - accuracy: 0.7812 - val_loss: 0.6144 - val_accuracy: 0.6964\n",
            "Epoch 245/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5084 - accuracy: 0.7812 - val_loss: 0.6137 - val_accuracy: 0.6964\n",
            "Epoch 246/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5270 - accuracy: 0.7634 - val_loss: 0.6108 - val_accuracy: 0.6964\n",
            "Epoch 247/1000\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5132 - accuracy: 0.7786 - val_loss: 0.6075 - val_accuracy: 0.6964\n",
            "Epoch 248/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5036 - accuracy: 0.7939 - val_loss: 0.6077 - val_accuracy: 0.6964\n",
            "Epoch 249/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5180 - accuracy: 0.7786 - val_loss: 0.6110 - val_accuracy: 0.6964\n",
            "Epoch 250/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5220 - accuracy: 0.7710 - val_loss: 0.6225 - val_accuracy: 0.6964\n",
            "Epoch 251/1000\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4822 - accuracy: 0.8015 - val_loss: 0.6434 - val_accuracy: 0.6964\n",
            "Epoch 252/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5383 - accuracy: 0.7688 - val_loss: 0.6472 - val_accuracy: 0.6964\n",
            "Epoch 253/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.4725 - accuracy: 0.8092 - val_loss: 0.6382 - val_accuracy: 0.6964\n",
            "Epoch 254/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.4978 - accuracy: 0.7939 - val_loss: 0.6189 - val_accuracy: 0.6964\n",
            "Epoch 255/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5298 - accuracy: 0.7634 - val_loss: 0.6101 - val_accuracy: 0.6964\n",
            "Epoch 256/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5089 - accuracy: 0.7863 - val_loss: 0.6096 - val_accuracy: 0.6964\n",
            "Epoch 257/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5217 - accuracy: 0.7710 - val_loss: 0.6118 - val_accuracy: 0.6964\n",
            "Epoch 258/1000\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.5022 - accuracy: 0.7863 - val_loss: 0.6136 - val_accuracy: 0.6964\n",
            "Epoch 259/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5144 - accuracy: 0.7786 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
            "Epoch 260/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5239 - accuracy: 0.7688 - val_loss: 0.6261 - val_accuracy: 0.6964\n",
            "Epoch 261/1000\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.5339 - accuracy: 0.7634 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 262/1000\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5359 - accuracy: 0.7634 - val_loss: 0.6330 - val_accuracy: 0.6964\n",
            "Epoch 263/1000\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5200 - accuracy: 0.7750 - val_loss: 0.6293 - val_accuracy: 0.6964\n",
            "Epoch 264/1000\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5333 - accuracy: 0.7634 - val_loss: 0.6183 - val_accuracy: 0.6964\n",
            "Epoch 265/1000\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5217 - accuracy: 0.7710 - val_loss: 0.6153 - val_accuracy: 0.6964\n",
            "Epoch 266/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5587 - accuracy: 0.7405 - val_loss: 0.6169 - val_accuracy: 0.6964\n",
            "Epoch 267/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5219 - accuracy: 0.7688 - val_loss: 0.6150 - val_accuracy: 0.6964\n",
            "Epoch 268/1000\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5458 - accuracy: 0.7481 - val_loss: 0.6105 - val_accuracy: 0.6964\n",
            "Epoch 269/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5123 - accuracy: 0.7786 - val_loss: 0.6079 - val_accuracy: 0.6964\n",
            "Epoch 270/1000\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5121 - accuracy: 0.7812 - val_loss: 0.6079 - val_accuracy: 0.6964\n",
            "Epoch 271/1000\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5222 - accuracy: 0.7688 - val_loss: 0.6091 - val_accuracy: 0.6964\n",
            "Epoch 272/1000\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5333 - accuracy: 0.7634 - val_loss: 0.6143 - val_accuracy: 0.6964\n",
            "Epoch 273/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5086 - accuracy: 0.7812 - val_loss: 0.6254 - val_accuracy: 0.6964\n",
            "Epoch 274/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5529 - accuracy: 0.7481 - val_loss: 0.6172 - val_accuracy: 0.6964\n",
            "Epoch 275/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5254 - accuracy: 0.7688 - val_loss: 0.6080 - val_accuracy: 0.6964\n",
            "Epoch 276/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5401 - accuracy: 0.7557 - val_loss: 0.6082 - val_accuracy: 0.6964\n",
            "Epoch 277/1000\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5465 - accuracy: 0.7557 - val_loss: 0.6076 - val_accuracy: 0.6964\n",
            "Epoch 278/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5199 - accuracy: 0.7786 - val_loss: 0.6086 - val_accuracy: 0.6964\n",
            "Epoch 279/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5405 - accuracy: 0.7557 - val_loss: 0.6176 - val_accuracy: 0.6964\n",
            "Epoch 280/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5066 - accuracy: 0.7786 - val_loss: 0.6216 - val_accuracy: 0.6964\n",
            "Epoch 281/1000\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5138 - accuracy: 0.7786 - val_loss: 0.6257 - val_accuracy: 0.6964\n",
            "Epoch 282/1000\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.5112 - accuracy: 0.7786 - val_loss: 0.6289 - val_accuracy: 0.6964\n",
            "Epoch 283/1000\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5255 - accuracy: 0.7688 - val_loss: 0.6228 - val_accuracy: 0.6964\n",
            "Epoch 284/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5073 - accuracy: 0.7786 - val_loss: 0.6161 - val_accuracy: 0.6964\n",
            "Epoch 285/1000\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5449 - accuracy: 0.7481 - val_loss: 0.6088 - val_accuracy: 0.6964\n",
            "Epoch 286/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.7750"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-f2e55d903d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1429\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1432\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001,\n",
        "    # learning_rate=0.00001,\n",
        "    learning_rate=0.00001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "\n",
        "# model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optim)"
      ],
      "metadata": {
        "id": "o-baI0bC2juE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#With 0.00001 lr.\n",
        "n_epochs = 1000\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CY3CLwlU2kga",
        "outputId": "239fef29-75ad-4c4d-e8ba-4172c1e483c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - 8s 588ms/step - loss: 0.8132 - accuracy: 0.4962 - val_loss: 0.6125 - val_accuracy: 0.6964\n",
            "Epoch 2/1000\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.6320 - accuracy: 0.6718 - val_loss: 0.6344 - val_accuracy: 0.6964\n",
            "Epoch 3/1000\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.4760 - accuracy: 0.7786 - val_loss: 0.6474 - val_accuracy: 0.6964\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.4406 - accuracy: 0.7863 - val_loss: 0.6495 - val_accuracy: 0.6964\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.3870 - accuracy: 0.8244 - val_loss: 0.6401 - val_accuracy: 0.6964\n",
            "Epoch 6/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.3392 - accuracy: 0.8244 - val_loss: 0.6363 - val_accuracy: 0.6964\n",
            "Epoch 7/1000\n",
            "5/5 [==============================] - 2s 432ms/step - loss: 0.3277 - accuracy: 0.8625 - val_loss: 0.6260 - val_accuracy: 0.6964\n",
            "Epoch 8/1000\n",
            "5/5 [==============================] - 2s 433ms/step - loss: 0.2532 - accuracy: 0.9250 - val_loss: 0.6206 - val_accuracy: 0.6964\n",
            "Epoch 9/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.2435 - accuracy: 0.9389 - val_loss: 0.6208 - val_accuracy: 0.6964\n",
            "Epoch 10/1000\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.1945 - accuracy: 0.9618 - val_loss: 0.6235 - val_accuracy: 0.6964\n",
            "Epoch 11/1000\n",
            "5/5 [==============================] - 2s 432ms/step - loss: 0.1936 - accuracy: 0.9563 - val_loss: 0.6247 - val_accuracy: 0.6964\n",
            "Epoch 12/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.1586 - accuracy: 0.9771 - val_loss: 0.6288 - val_accuracy: 0.6964\n",
            "Epoch 13/1000\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.1309 - accuracy: 0.9847 - val_loss: 0.6321 - val_accuracy: 0.6964\n",
            "Epoch 14/1000\n",
            "5/5 [==============================] - 2s 432ms/step - loss: 0.1248 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 0.6964\n",
            "Epoch 15/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.1408 - accuracy: 0.9924 - val_loss: 0.6535 - val_accuracy: 0.6964\n",
            "Epoch 16/1000\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.1040 - accuracy: 1.0000 - val_loss: 0.6731 - val_accuracy: 0.6964\n",
            "Epoch 17/1000\n",
            "5/5 [==============================] - 2s 432ms/step - loss: 0.1099 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 0.6964\n",
            "Epoch 18/1000\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0866 - accuracy: 0.9924 - val_loss: 0.7211 - val_accuracy: 0.6964\n",
            "Epoch 19/1000\n",
            "5/5 [==============================] - 2s 431ms/step - loss: 0.0824 - accuracy: 0.9937 - val_loss: 0.7362 - val_accuracy: 0.6964\n",
            "Epoch 20/1000\n",
            "5/5 [==============================] - 2s 357ms/step - loss: 0.0808 - accuracy: 0.9924 - val_loss: 0.7407 - val_accuracy: 0.6964\n",
            "Epoch 21/1000\n",
            "5/5 [==============================] - 2s 356ms/step - loss: 0.0969 - accuracy: 0.9924 - val_loss: 0.7379 - val_accuracy: 0.6964\n",
            "Epoch 22/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.7237 - val_accuracy: 0.6964\n",
            "Epoch 23/1000\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0650 - accuracy: 1.0000 - val_loss: 0.7439 - val_accuracy: 0.6964\n",
            "Epoch 24/1000\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.7701 - val_accuracy: 0.6964\n",
            "Epoch 25/1000\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 0.0553 - accuracy: 0.9924 - val_loss: 0.7843 - val_accuracy: 0.6964\n",
            "Epoch 26/1000\n",
            "5/5 [==============================] - 2s 435ms/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 0.8018 - val_accuracy: 0.6964\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - 2s 432ms/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 0.8214 - val_accuracy: 0.6964\n",
            "Epoch 28/1000\n",
            "5/5 [==============================] - 2s 355ms/step - loss: 0.0786 - accuracy: 0.9771 - val_loss: 0.8157 - val_accuracy: 0.6964\n",
            "Epoch 29/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0689 - accuracy: 0.9924 - val_loss: 0.7925 - val_accuracy: 0.6964\n",
            "Epoch 30/1000\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.7605 - val_accuracy: 0.6964\n",
            "Epoch 31/1000\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.7393 - val_accuracy: 0.6964\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 0.0461 - accuracy: 0.9924 - val_loss: 0.7381 - val_accuracy: 0.6964\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.7636 - val_accuracy: 0.6964\n",
            "Epoch 34/1000\n",
            "5/5 [==============================] - 2s 430ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.7956 - val_accuracy: 0.6964\n",
            "Epoch 35/1000\n",
            "5/5 [==============================] - 2s 431ms/step - loss: 0.0472 - accuracy: 0.9924 - val_loss: 0.8126 - val_accuracy: 0.6964\n",
            "Epoch 36/1000\n",
            "5/5 [==============================] - 2s 352ms/step - loss: 0.0419 - accuracy: 0.9924 - val_loss: 0.8054 - val_accuracy: 0.6964\n",
            "Epoch 37/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0392 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-f2e55d903d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# #With 0.001 lr.\n",
        "# n_epochs = 100\n",
        "# num_train = trdf.shape[0]\n",
        "# num_iterations = int(num_train/bs)\n",
        "\n",
        "# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27675ca7-4cf2-46f0-e286-d0f2e4df2e96",
        "id": "DRT_potd0cYP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 12s 298ms/step - loss: 2.5252 - accuracy: 0.2137 - val_loss: 1.1788 - val_accuracy: 0.3036\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.8841 - accuracy: 0.2137 - val_loss: 0.8746 - val_accuracy: 0.3036\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5555 - accuracy: 0.2061 - val_loss: 0.9770 - val_accuracy: 0.3036\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.8629 - accuracy: 0.2366 - val_loss: 1.1501 - val_accuracy: 0.3036\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.9240 - accuracy: 0.2519 - val_loss: 1.1078 - val_accuracy: 0.3036\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7571 - accuracy: 0.2519 - val_loss: 0.6849 - val_accuracy: 0.3036\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5780 - accuracy: 0.2137 - val_loss: 0.6061 - val_accuracy: 0.3036\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 1.7647 - accuracy: 0.2366 - val_loss: 2.8459 - val_accuracy: 0.3036\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 1.7249 - accuracy: 0.2313 - val_loss: 1.7976 - val_accuracy: 0.3036\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.4020 - accuracy: 0.2137 - val_loss: 1.5527 - val_accuracy: 0.3036\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.6104 - accuracy: 0.1985 - val_loss: 0.6008 - val_accuracy: 0.3036\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5203 - accuracy: 0.2214 - val_loss: 0.6451 - val_accuracy: 0.3036\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6243 - accuracy: 0.2061 - val_loss: 0.7600 - val_accuracy: 0.3036\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6123 - accuracy: 0.2519 - val_loss: 0.6592 - val_accuracy: 0.3036\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6178 - accuracy: 0.2366 - val_loss: 0.8678 - val_accuracy: 0.3036\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6570 - accuracy: 0.2443 - val_loss: 0.6392 - val_accuracy: 0.3036\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.7378 - accuracy: 0.2313 - val_loss: 0.6851 - val_accuracy: 0.3036\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5734 - accuracy: 0.2250 - val_loss: 0.6401 - val_accuracy: 0.3036\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5622 - accuracy: 0.2188 - val_loss: 0.7683 - val_accuracy: 0.3036\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6070 - accuracy: 0.2061 - val_loss: 0.5989 - val_accuracy: 0.3036\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5106 - accuracy: 0.2214 - val_loss: 0.7647 - val_accuracy: 0.3036\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.6578 - accuracy: 0.2313 - val_loss: 0.9309 - val_accuracy: 0.3036\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.7444 - accuracy: 0.2137 - val_loss: 0.8866 - val_accuracy: 0.3036\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5462 - accuracy: 0.2137 - val_loss: 0.6077 - val_accuracy: 0.3036\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5579 - accuracy: 0.2137 - val_loss: 0.5979 - val_accuracy: 0.3036\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.6628 - accuracy: 0.2443 - val_loss: 1.0824 - val_accuracy: 0.3036\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.8410 - accuracy: 0.2443 - val_loss: 1.2277 - val_accuracy: 0.3036\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.1125 - accuracy: 0.1985 - val_loss: 2.5288 - val_accuracy: 0.3036\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 2.9988 - accuracy: 0.2519 - val_loss: 1.3394 - val_accuracy: 0.3036\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 2.4833 - accuracy: 0.2313 - val_loss: 3.4372 - val_accuracy: 0.3036\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 1.6976 - accuracy: 0.2214 - val_loss: 0.6103 - val_accuracy: 0.3036\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.9626 - accuracy: 0.2188 - val_loss: 0.6042 - val_accuracy: 0.3036\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7416 - accuracy: 0.2290 - val_loss: 2.0917 - val_accuracy: 0.3036\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.4749 - accuracy: 0.2214 - val_loss: 1.4386 - val_accuracy: 0.3036\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 1.8756 - accuracy: 0.2137 - val_loss: 2.8180 - val_accuracy: 0.3036\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.8693 - accuracy: 0.2595 - val_loss: 0.7581 - val_accuracy: 0.3036\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 2.6692 - accuracy: 0.2214 - val_loss: 4.2503 - val_accuracy: 0.3036\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 2.3249 - accuracy: 0.2137 - val_loss: 2.5406 - val_accuracy: 0.3036\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 1.4054 - accuracy: 0.2250 - val_loss: 3.2612 - val_accuracy: 0.3036\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.8438 - accuracy: 0.2290 - val_loss: 0.8093 - val_accuracy: 0.3036\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 1.6918 - accuracy: 0.2137 - val_loss: 2.5726 - val_accuracy: 0.3036\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.3355 - accuracy: 0.2443 - val_loss: 0.6171 - val_accuracy: 0.3036\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.8752 - accuracy: 0.2137 - val_loss: 0.8464 - val_accuracy: 0.3036\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6831 - accuracy: 0.2137 - val_loss: 0.6562 - val_accuracy: 0.3036\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6582 - accuracy: 0.2443 - val_loss: 0.7016 - val_accuracy: 0.3036\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.6479 - accuracy: 0.2214 - val_loss: 1.3041 - val_accuracy: 0.3036\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.7763 - accuracy: 0.2214 - val_loss: 0.9464 - val_accuracy: 0.3036\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.7247 - accuracy: 0.2061 - val_loss: 1.5538 - val_accuracy: 0.3036\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.9089 - accuracy: 0.2214 - val_loss: 0.8669 - val_accuracy: 0.3036\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.6223 - accuracy: 0.2250 - val_loss: 0.5929 - val_accuracy: 0.3036\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5873 - accuracy: 0.1985 - val_loss: 1.3367 - val_accuracy: 0.3036\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.9446 - accuracy: 0.2290 - val_loss: 0.9629 - val_accuracy: 0.3036\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.9787 - accuracy: 0.2290 - val_loss: 0.8605 - val_accuracy: 0.3036\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.8550 - accuracy: 0.1985 - val_loss: 1.2659 - val_accuracy: 0.3036\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.8397 - accuracy: 0.2214 - val_loss: 1.1460 - val_accuracy: 0.3036\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.8276 - accuracy: 0.2313 - val_loss: 0.6800 - val_accuracy: 0.3036\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.8478 - accuracy: 0.2290 - val_loss: 0.5947 - val_accuracy: 0.3036\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6099 - accuracy: 0.2137 - val_loss: 0.7485 - val_accuracy: 0.3036\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.6756 - accuracy: 0.2313 - val_loss: 0.8480 - val_accuracy: 0.3036\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7620 - accuracy: 0.2214 - val_loss: 0.9635 - val_accuracy: 0.3036\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 1.2220 - accuracy: 0.2188 - val_loss: 1.0795 - val_accuracy: 0.3036\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.8222 - accuracy: 0.2250 - val_loss: 1.6727 - val_accuracy: 0.3036\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 1.2033 - accuracy: 0.2214 - val_loss: 0.6738 - val_accuracy: 0.3036\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.7002 - accuracy: 0.2290 - val_loss: 1.1449 - val_accuracy: 0.3036\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.9738 - accuracy: 0.2290 - val_loss: 0.6930 - val_accuracy: 0.3036\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 1.2418 - accuracy: 0.2443 - val_loss: 1.4237 - val_accuracy: 0.3036\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.8688 - accuracy: 0.2137 - val_loss: 0.8611 - val_accuracy: 0.3036\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6499 - accuracy: 0.2366 - val_loss: 0.8378 - val_accuracy: 0.3036\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.6575 - accuracy: 0.2214 - val_loss: 1.3530 - val_accuracy: 0.3036\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.3722 - accuracy: 0.2519 - val_loss: 1.4888 - val_accuracy: 0.3036\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.7553 - accuracy: 0.1908 - val_loss: 0.9094 - val_accuracy: 0.3036\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6080 - accuracy: 0.2443 - val_loss: 0.9103 - val_accuracy: 0.3036\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.7119 - accuracy: 0.2519 - val_loss: 0.7323 - val_accuracy: 0.3036\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6990 - accuracy: 0.2214 - val_loss: 0.5927 - val_accuracy: 0.3036\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6678 - accuracy: 0.2214 - val_loss: 0.5927 - val_accuracy: 0.3036\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.7607 - accuracy: 0.2214 - val_loss: 0.7566 - val_accuracy: 0.3036\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.6965 - accuracy: 0.2214 - val_loss: 1.8025 - val_accuracy: 0.3036\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 1.0241 - accuracy: 0.2443 - val_loss: 0.5987 - val_accuracy: 0.3036\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5799 - accuracy: 0.2443 - val_loss: 0.6736 - val_accuracy: 0.3036\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5368 - accuracy: 0.2313 - val_loss: 0.5947 - val_accuracy: 0.3036\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5582 - accuracy: 0.2290 - val_loss: 0.9694 - val_accuracy: 0.3036\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.7515 - accuracy: 0.2061 - val_loss: 0.6340 - val_accuracy: 0.3036\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6618 - accuracy: 0.2443 - val_loss: 0.7202 - val_accuracy: 0.3036\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6470 - accuracy: 0.2290 - val_loss: 0.9875 - val_accuracy: 0.3036\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.6741 - accuracy: 0.2061 - val_loss: 0.6063 - val_accuracy: 0.3036\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5656 - accuracy: 0.2366 - val_loss: 0.5977 - val_accuracy: 0.3036\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5418 - accuracy: 0.2519 - val_loss: 0.6450 - val_accuracy: 0.3036\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5422 - accuracy: 0.2443 - val_loss: 0.5979 - val_accuracy: 0.3036\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.6564 - accuracy: 0.2214 - val_loss: 0.6868 - val_accuracy: 0.3036\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5817 - accuracy: 0.2214 - val_loss: 0.7939 - val_accuracy: 0.3036\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5540 - accuracy: 0.2214 - val_loss: 0.6724 - val_accuracy: 0.3036\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.7064 - accuracy: 0.2519 - val_loss: 2.1639 - val_accuracy: 0.3036\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 1.2786 - accuracy: 0.2290 - val_loss: 1.1236 - val_accuracy: 0.3036\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 1.1066 - accuracy: 0.2366 - val_loss: 1.0895 - val_accuracy: 0.3036\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.7642 - accuracy: 0.2061 - val_loss: 1.6766 - val_accuracy: 0.3036\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.4231 - accuracy: 0.2290 - val_loss: 0.7620 - val_accuracy: 0.3036\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.9252 - accuracy: 0.2250 - val_loss: 1.5716 - val_accuracy: 0.3036\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.9210 - accuracy: 0.2137 - val_loss: 1.3814 - val_accuracy: 0.3036\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 2.1326 - accuracy: 0.2061 - val_loss: 2.9113 - val_accuracy: 0.3036\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.4859 - accuracy: 0.2443 - val_loss: 0.9264 - val_accuracy: 0.3036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "tV1nYjRgYEPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Numpy Array Of Images.\n",
        "import numpy as np\n",
        "\n",
        "train_array = np.load(r'/content/drive/MyDrive/CV-EmpathNet/train_text.npy')\n",
        "train_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcnpT72Bdw_o",
        "outputId": "ae7a475b-3b5a-464a-8b3d-f609e931f830"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Numpy Array Of Images.\n",
        "import numpy as np\n",
        "\n",
        "train_array = np.load(r'/content/drive/MyDrive/CV-EmpathNet/train_image.npy')\n",
        "train_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la1U5BySegjN",
        "outputId": "ecbdd1ef-449b-4c14-8e06-b166d2b3e2e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163, 224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Numpy Array Of Images.\n",
        "import numpy as np\n",
        "\n",
        "train_array = np.load(r'/content/drive/MyDrive/CV-EmpathNet/train_image_3_channels.npy')\n",
        "train_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTOD-PWPexNO",
        "outputId": "4a13c7fd-e5c4-42a7-e853-4e32e9f1f502"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163, 288, 432, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wand\n",
        "!apt-get install libmagickwand-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLiJgJ2zDdc1",
        "outputId": "371b6beb-132c-406d-c121-f922e29bbd4f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wand\n",
            "  Downloading Wand-0.6.7-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 30 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 139 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: wand\n",
            "Successfully installed wand-0.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf spect_aug_noise"
      ],
      "metadata": {
        "id": "-0GUlJOtDu-X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf train_aug_noise\n",
        "# !rm -rf test_aug_noise"
      ],
      "metadata": {
        "id": "RWksD6NiGVMr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create New Augmented Dataset Folders\n",
        "!cp -r train train_aug_noise\n",
        "!cp -r test test_aug_noise"
      ],
      "metadata": {
        "id": "iVo5WwpLDhmc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Increase the number of positive (depressed) class samples using gaussian noise injection in the dataset.\n",
        "\n",
        "from shutil import copy2\n",
        "from wand.image import Image\n",
        "\n",
        "# src = '/content/train/0'\n",
        "# dst = '/content/train_aug_noise/0'\n",
        "# for id in trdf[trdf['PHQ_Binary']==0]['Participant_ID']:\n",
        "#   with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "#     img.noise(\"gaussian\")\n",
        "#     img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n",
        "\n",
        "src = '/content/train/1'\n",
        "dst = '/content/train_aug_noise/1'\n",
        "for id in trdf[trdf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "    img.noise(\"gaussian\")\n",
        "    img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n",
        "\n",
        "src = '/content/test/1'\n",
        "dst = '/content/test_aug_noise/1'\n",
        "for id in tedf[tedf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "    img.noise(\"gaussian\")\n",
        "    img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n",
        "\n",
        "# src = '/content/test/0'\n",
        "# dst = '/content/test_aug_noise/0'\n",
        "# for id in tedf[tedf['PHQ_Binary']==0]['Participant_ID']:\n",
        "#   with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "#     img.noise(\"gaussian\")\n",
        "#     img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n"
      ],
      "metadata": {
        "id": "Vi1JjyT5Hn9N"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6\n",
        "# https://towardsdatascience.com/audio-deep-learning-made-simple-part-3-data-preparation-and-augmentation-24c6e1f6b52"
      ],
      "metadata": {
        "id": "Qhp8hGosfRDp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "_j_SV4FmYEPC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_func = tensorflow.keras.applications.xception.preprocess_input"
      ],
      "metadata": {
        "id": "6QuRwzogYEPC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "train_gen = train_datagen.flow_from_directory('/content/train_aug_noise', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "# train_gen = train_datagen.flow_from_directory('/content/train', (224,224), class_mode = 'categorical', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(train_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbf5d8f-5917-4521-a3fc-f29299fb01fb",
        "id": "eo2jp-UZIwFp"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "test_gen = test_datagen.flow_from_directory('/content/test', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(test_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d67bc0-a2d1-4789-e2b7-a5758f4b58a7",
        "id": "m947NfnhIwFp"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 56 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with Custom CNNs"
      ],
      "metadata": {
        "id": "L36qV8mUUItO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "#create model\n",
        "\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(224,224,3)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(10, activation=softmax))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "NR7hRStYSH3J"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = len(os.listdir('/content/test/1'))\n",
        "n = len(os.listdir('/content/test/0'))\n",
        "print(n/(p+n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aWoMkVqQepc",
        "outputId": "50bdc9b7-d121-477c-e1e0-455f1ced5c51"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6964285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = len(os.listdir('/content/train/1'))\n",
        "n = len(os.listdir('/content/train/0'))\n",
        "print(n/(p+n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm2gu6UnQydX",
        "outputId": "136f0980-c726-4382-b692-af7508b6113a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7730061349693251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  print(layer.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0dc324-532d-47d2-d5a1-2b90a4060aba",
        "id": "Kb3-LiGvYEPD"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath_r = 'weights_best_rmodel.hdf5'\n",
        "filepath_a = 'weights_best_amodel.hdf5'\n",
        "checkpoint1 = ModelCheckpoint(filepath_r,monitor = 'val_recall_15',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint2 = ModelCheckpoint(filepath_a,monitor = 'val_accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "\n",
        "checkpoint3 = ModelCheckpoint(filepath_r,monitor = 'accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint4 = ModelCheckpoint(filepath_a,monitor = 'recall_15',verbose = 1, save_best_only=True, mode='max')"
      ],
      "metadata": {
        "id": "JGFCQXoDUL4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With 0.001 lr.\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "n_epochs = 100\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27675ca7-4cf2-46f0-e286-d0f2e4df2e96",
        "id": "tDZ0eKX1YEPE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 12s 298ms/step - loss: 2.5252 - accuracy: 0.2137 - val_loss: 1.1788 - val_accuracy: 0.3036\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.8841 - accuracy: 0.2137 - val_loss: 0.8746 - val_accuracy: 0.3036\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 105ms/step - loss: 0.5555 - accuracy: 0.2061 - val_loss: 0.9770 - val_accuracy: 0.3036\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.8629 - accuracy: 0.2366 - val_loss: 1.1501 - val_accuracy: 0.3036\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.9240 - accuracy: 0.2519 - val_loss: 1.1078 - val_accuracy: 0.3036\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7571 - accuracy: 0.2519 - val_loss: 0.6849 - val_accuracy: 0.3036\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5780 - accuracy: 0.2137 - val_loss: 0.6061 - val_accuracy: 0.3036\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 1.7647 - accuracy: 0.2366 - val_loss: 2.8459 - val_accuracy: 0.3036\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 1.7249 - accuracy: 0.2313 - val_loss: 1.7976 - val_accuracy: 0.3036\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.4020 - accuracy: 0.2137 - val_loss: 1.5527 - val_accuracy: 0.3036\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.6104 - accuracy: 0.1985 - val_loss: 0.6008 - val_accuracy: 0.3036\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.5203 - accuracy: 0.2214 - val_loss: 0.6451 - val_accuracy: 0.3036\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6243 - accuracy: 0.2061 - val_loss: 0.7600 - val_accuracy: 0.3036\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6123 - accuracy: 0.2519 - val_loss: 0.6592 - val_accuracy: 0.3036\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6178 - accuracy: 0.2366 - val_loss: 0.8678 - val_accuracy: 0.3036\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6570 - accuracy: 0.2443 - val_loss: 0.6392 - val_accuracy: 0.3036\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.7378 - accuracy: 0.2313 - val_loss: 0.6851 - val_accuracy: 0.3036\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5734 - accuracy: 0.2250 - val_loss: 0.6401 - val_accuracy: 0.3036\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.5622 - accuracy: 0.2188 - val_loss: 0.7683 - val_accuracy: 0.3036\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6070 - accuracy: 0.2061 - val_loss: 0.5989 - val_accuracy: 0.3036\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 99ms/step - loss: 0.5106 - accuracy: 0.2214 - val_loss: 0.7647 - val_accuracy: 0.3036\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 0.6578 - accuracy: 0.2313 - val_loss: 0.9309 - val_accuracy: 0.3036\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.7444 - accuracy: 0.2137 - val_loss: 0.8866 - val_accuracy: 0.3036\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5462 - accuracy: 0.2137 - val_loss: 0.6077 - val_accuracy: 0.3036\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5579 - accuracy: 0.2137 - val_loss: 0.5979 - val_accuracy: 0.3036\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.6628 - accuracy: 0.2443 - val_loss: 1.0824 - val_accuracy: 0.3036\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.8410 - accuracy: 0.2443 - val_loss: 1.2277 - val_accuracy: 0.3036\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.1125 - accuracy: 0.1985 - val_loss: 2.5288 - val_accuracy: 0.3036\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 2.9988 - accuracy: 0.2519 - val_loss: 1.3394 - val_accuracy: 0.3036\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 2.4833 - accuracy: 0.2313 - val_loss: 3.4372 - val_accuracy: 0.3036\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 1.6976 - accuracy: 0.2214 - val_loss: 0.6103 - val_accuracy: 0.3036\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.9626 - accuracy: 0.2188 - val_loss: 0.6042 - val_accuracy: 0.3036\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7416 - accuracy: 0.2290 - val_loss: 2.0917 - val_accuracy: 0.3036\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.4749 - accuracy: 0.2214 - val_loss: 1.4386 - val_accuracy: 0.3036\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 1.8756 - accuracy: 0.2137 - val_loss: 2.8180 - val_accuracy: 0.3036\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.8693 - accuracy: 0.2595 - val_loss: 0.7581 - val_accuracy: 0.3036\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 100ms/step - loss: 2.6692 - accuracy: 0.2214 - val_loss: 4.2503 - val_accuracy: 0.3036\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 2.3249 - accuracy: 0.2137 - val_loss: 2.5406 - val_accuracy: 0.3036\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 1.4054 - accuracy: 0.2250 - val_loss: 3.2612 - val_accuracy: 0.3036\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.8438 - accuracy: 0.2290 - val_loss: 0.8093 - val_accuracy: 0.3036\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 1.6918 - accuracy: 0.2137 - val_loss: 2.5726 - val_accuracy: 0.3036\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.3355 - accuracy: 0.2443 - val_loss: 0.6171 - val_accuracy: 0.3036\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.8752 - accuracy: 0.2137 - val_loss: 0.8464 - val_accuracy: 0.3036\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6831 - accuracy: 0.2137 - val_loss: 0.6562 - val_accuracy: 0.3036\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6582 - accuracy: 0.2443 - val_loss: 0.7016 - val_accuracy: 0.3036\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.6479 - accuracy: 0.2214 - val_loss: 1.3041 - val_accuracy: 0.3036\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.7763 - accuracy: 0.2214 - val_loss: 0.9464 - val_accuracy: 0.3036\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 103ms/step - loss: 0.7247 - accuracy: 0.2061 - val_loss: 1.5538 - val_accuracy: 0.3036\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.9089 - accuracy: 0.2214 - val_loss: 0.8669 - val_accuracy: 0.3036\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.6223 - accuracy: 0.2250 - val_loss: 0.5929 - val_accuracy: 0.3036\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.5873 - accuracy: 0.1985 - val_loss: 1.3367 - val_accuracy: 0.3036\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.9446 - accuracy: 0.2290 - val_loss: 0.9629 - val_accuracy: 0.3036\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.9787 - accuracy: 0.2290 - val_loss: 0.8605 - val_accuracy: 0.3036\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.8550 - accuracy: 0.1985 - val_loss: 1.2659 - val_accuracy: 0.3036\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.8397 - accuracy: 0.2214 - val_loss: 1.1460 - val_accuracy: 0.3036\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.8276 - accuracy: 0.2313 - val_loss: 0.6800 - val_accuracy: 0.3036\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.8478 - accuracy: 0.2290 - val_loss: 0.5947 - val_accuracy: 0.3036\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6099 - accuracy: 0.2137 - val_loss: 0.7485 - val_accuracy: 0.3036\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.6756 - accuracy: 0.2313 - val_loss: 0.8480 - val_accuracy: 0.3036\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.7620 - accuracy: 0.2214 - val_loss: 0.9635 - val_accuracy: 0.3036\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 1.2220 - accuracy: 0.2188 - val_loss: 1.0795 - val_accuracy: 0.3036\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 95ms/step - loss: 0.8222 - accuracy: 0.2250 - val_loss: 1.6727 - val_accuracy: 0.3036\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 1.2033 - accuracy: 0.2214 - val_loss: 0.6738 - val_accuracy: 0.3036\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.7002 - accuracy: 0.2290 - val_loss: 1.1449 - val_accuracy: 0.3036\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 104ms/step - loss: 0.9738 - accuracy: 0.2290 - val_loss: 0.6930 - val_accuracy: 0.3036\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 1.2418 - accuracy: 0.2443 - val_loss: 1.4237 - val_accuracy: 0.3036\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.8688 - accuracy: 0.2137 - val_loss: 0.8611 - val_accuracy: 0.3036\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6499 - accuracy: 0.2366 - val_loss: 0.8378 - val_accuracy: 0.3036\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.6575 - accuracy: 0.2214 - val_loss: 1.3530 - val_accuracy: 0.3036\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 1.3722 - accuracy: 0.2519 - val_loss: 1.4888 - val_accuracy: 0.3036\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.7553 - accuracy: 0.1908 - val_loss: 0.9094 - val_accuracy: 0.3036\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.6080 - accuracy: 0.2443 - val_loss: 0.9103 - val_accuracy: 0.3036\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.7119 - accuracy: 0.2519 - val_loss: 0.7323 - val_accuracy: 0.3036\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.6990 - accuracy: 0.2214 - val_loss: 0.5927 - val_accuracy: 0.3036\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6678 - accuracy: 0.2214 - val_loss: 0.5927 - val_accuracy: 0.3036\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 0.7607 - accuracy: 0.2214 - val_loss: 0.7566 - val_accuracy: 0.3036\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.6965 - accuracy: 0.2214 - val_loss: 1.8025 - val_accuracy: 0.3036\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 1.0241 - accuracy: 0.2443 - val_loss: 0.5987 - val_accuracy: 0.3036\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.5799 - accuracy: 0.2443 - val_loss: 0.6736 - val_accuracy: 0.3036\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.5368 - accuracy: 0.2313 - val_loss: 0.5947 - val_accuracy: 0.3036\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5582 - accuracy: 0.2290 - val_loss: 0.9694 - val_accuracy: 0.3036\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.7515 - accuracy: 0.2061 - val_loss: 0.6340 - val_accuracy: 0.3036\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6618 - accuracy: 0.2443 - val_loss: 0.7202 - val_accuracy: 0.3036\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.6470 - accuracy: 0.2290 - val_loss: 0.9875 - val_accuracy: 0.3036\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.6741 - accuracy: 0.2061 - val_loss: 0.6063 - val_accuracy: 0.3036\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.5656 - accuracy: 0.2366 - val_loss: 0.5977 - val_accuracy: 0.3036\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5418 - accuracy: 0.2519 - val_loss: 0.6450 - val_accuracy: 0.3036\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5422 - accuracy: 0.2443 - val_loss: 0.5979 - val_accuracy: 0.3036\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.6564 - accuracy: 0.2214 - val_loss: 0.6868 - val_accuracy: 0.3036\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.5817 - accuracy: 0.2214 - val_loss: 0.7939 - val_accuracy: 0.3036\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.5540 - accuracy: 0.2214 - val_loss: 0.6724 - val_accuracy: 0.3036\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.7064 - accuracy: 0.2519 - val_loss: 2.1639 - val_accuracy: 0.3036\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 1.2786 - accuracy: 0.2290 - val_loss: 1.1236 - val_accuracy: 0.3036\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 1.1066 - accuracy: 0.2366 - val_loss: 1.0895 - val_accuracy: 0.3036\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.7642 - accuracy: 0.2061 - val_loss: 1.6766 - val_accuracy: 0.3036\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.4231 - accuracy: 0.2290 - val_loss: 0.7620 - val_accuracy: 0.3036\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.9252 - accuracy: 0.2250 - val_loss: 1.5716 - val_accuracy: 0.3036\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.9210 - accuracy: 0.2137 - val_loss: 1.3814 - val_accuracy: 0.3036\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 2.1326 - accuracy: 0.2061 - val_loss: 2.9113 - val_accuracy: 0.3036\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 1.4859 - accuracy: 0.2443 - val_loss: 0.9264 - val_accuracy: 0.3036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#With 0.0001 lr\n",
        "\n",
        "optim = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001,\n",
        "    # learning_rate=0.00001,\n",
        "    learning_rate=0.0001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall()], optimizer=optim)\n",
        "n_epochs = 1000\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a81d716-3096-4fa6-f3e3-b5a0ac587158",
        "id": "jt_Pwj0MYEPD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - 2s 193ms/step - loss: 0.6886 - accuracy: 0.6313 - recall_1: 0.2188 - val_loss: 0.6901 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.6694 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7268 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6742 - accuracy: 0.6187 - recall_1: 0.0000e+00 - val_loss: 0.7304 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.6570 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7149 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - 1s 101ms/step - loss: 0.6603 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.7143 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6639 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.7191 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6673 - accuracy: 0.6103 - recall_1: 0.0000e+00 - val_loss: 0.6993 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.6581 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.6921 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6600 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.6967 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.6709 - accuracy: 0.5956 - recall_1: 0.0000e+00 - val_loss: 0.7005 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.6514 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7161 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6426 - accuracy: 0.6500 - recall_1: 0.0000e+00 - val_loss: 0.7241 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6455 - accuracy: 0.6471 - recall_1: 0.0000e+00 - val_loss: 0.7152 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6509 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.7021 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "5/5 [==============================] - 1s 100ms/step - loss: 0.6575 - accuracy: 0.6176 - recall_1: 0.0000e+00 - val_loss: 0.7002 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6378 - accuracy: 0.6562 - recall_1: 0.0000e+00 - val_loss: 0.6965 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6475 - accuracy: 0.6324 - recall_1: 0.0000e+00 - val_loss: 0.7102 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6459 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.7131 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.6506 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.6899 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.6638 - accuracy: 0.5882 - recall_1: 0.0000e+00 - val_loss: 0.6848 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.6427 - accuracy: 0.6471 - recall_1: 0.0000e+00 - val_loss: 0.7041 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6274 - accuracy: 0.6562 - recall_1: 0.0000e+00 - val_loss: 0.7119 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.6514 - accuracy: 0.6125 - recall_1: 0.0000e+00 - val_loss: 0.6880 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6533 - accuracy: 0.5938 - recall_1: 0.0000e+00 - val_loss: 0.6794 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6397 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.7132 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6472 - accuracy: 0.6176 - recall_1: 0.0000e+00 - val_loss: 0.7001 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.6282 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.6882 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.6351 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.6833 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.6314 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7148 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6217 - accuracy: 0.6500 - recall_1: 0.0000e+00 - val_loss: 0.6899 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.6202 - accuracy: 0.6324 - recall_1: 0.0000e+00 - val_loss: 0.6896 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.6199 - accuracy: 0.6324 - recall_1: 0.0000e+00 - val_loss: 0.6779 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.6046 - accuracy: 0.6618 - recall_1: 0.0000e+00 - val_loss: 0.7061 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6407 - accuracy: 0.6029 - recall_1: 0.0000e+00 - val_loss: 0.6830 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6321 - accuracy: 0.6471 - recall_1: 0.1273 - val_loss: 0.6708 - val_accuracy: 0.6575 - val_recall_1: 0.7059\n",
            "Epoch 36/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6282 - accuracy: 0.7941 - recall_1: 0.4902 - val_loss: 0.6900 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6135 - accuracy: 0.6544 - recall_1: 0.0208 - val_loss: 0.7326 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6440 - accuracy: 0.6187 - recall_1: 0.0000e+00 - val_loss: 0.6847 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.6137 - accuracy: 0.6471 - recall_1: 0.0204 - val_loss: 0.6738 - val_accuracy: 0.5479 - val_recall_1: 0.0882\n",
            "Epoch 40/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6183 - accuracy: 0.6838 - recall_1: 0.1765 - val_loss: 0.6721 - val_accuracy: 0.5753 - val_recall_1: 0.1765\n",
            "Epoch 41/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.6184 - accuracy: 0.6187 - recall_1: 0.0317 - val_loss: 0.6764 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6035 - accuracy: 0.6397 - recall_1: 0.0200 - val_loss: 0.6798 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6120 - accuracy: 0.6250 - recall_1: 0.0727 - val_loss: 0.6666 - val_accuracy: 0.6301 - val_recall_1: 0.2941\n",
            "Epoch 44/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.5965 - accuracy: 0.6618 - recall_1: 0.0213 - val_loss: 0.7046 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6048 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.6731 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.5824 - accuracy: 0.6562 - recall_1: 0.0351 - val_loss: 0.6722 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5868 - accuracy: 0.6618 - recall_1: 0.0417 - val_loss: 0.6601 - val_accuracy: 0.6301 - val_recall_1: 0.2941\n",
            "Epoch 48/1000\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 0.5895 - accuracy: 0.6765 - recall_1: 0.1731 - val_loss: 0.6606 - val_accuracy: 0.6027 - val_recall_1: 0.1765\n",
            "Epoch 49/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.5840 - accuracy: 0.6471 - recall_1: 0.0769 - val_loss: 0.6560 - val_accuracy: 0.6027 - val_recall_1: 0.2059\n",
            "Epoch 50/1000\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.6094 - accuracy: 0.7132 - recall_1: 0.6545 - val_loss: 0.6513 - val_accuracy: 0.6438 - val_recall_1: 0.6765\n",
            "Epoch 51/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5855 - accuracy: 0.6912 - recall_1: 0.1522 - val_loss: 0.7469 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6533 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.6823 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 53/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.5970 - accuracy: 0.6562 - recall_1: 0.0179 - val_loss: 0.6704 - val_accuracy: 0.5479 - val_recall_1: 0.0882\n",
            "Epoch 54/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.6026 - accuracy: 0.6250 - recall_1: 0.0164 - val_loss: 0.6677 - val_accuracy: 0.5616 - val_recall_1: 0.1176\n",
            "Epoch 55/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5788 - accuracy: 0.7647 - recall_1: 0.3111 - val_loss: 0.6569 - val_accuracy: 0.6712 - val_recall_1: 0.4412\n",
            "Epoch 56/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5716 - accuracy: 0.6750 - recall_1: 0.1034 - val_loss: 0.6690 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 57/1000\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.5799 - accuracy: 0.6471 - recall_1: 0.0400 - val_loss: 0.6617 - val_accuracy: 0.5753 - val_recall_1: 0.1176\n",
            "Epoch 58/1000\n",
            "5/5 [==============================] - 1s 101ms/step - loss: 0.5621 - accuracy: 0.7279 - recall_1: 0.2444 - val_loss: 0.6523 - val_accuracy: 0.6575 - val_recall_1: 0.3235\n",
            "Epoch 59/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5638 - accuracy: 0.6544 - recall_1: 0.1132 - val_loss: 0.6475 - val_accuracy: 0.6712 - val_recall_1: 0.3529\n",
            "Epoch 60/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5500 - accuracy: 0.7279 - recall_1: 0.2708 - val_loss: 0.6531 - val_accuracy: 0.6027 - val_recall_1: 0.1765\n",
            "Epoch 61/1000\n",
            "5/5 [==============================] - 1s 102ms/step - loss: 0.5521 - accuracy: 0.6985 - recall_1: 0.2778 - val_loss: 0.6392 - val_accuracy: 0.5890 - val_recall_1: 0.5000\n",
            "Epoch 62/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.5576 - accuracy: 0.6912 - recall_1: 0.2308 - val_loss: 0.6684 - val_accuracy: 0.5616 - val_recall_1: 0.0588\n",
            "Epoch 63/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.5477 - accuracy: 0.7647 - recall_1: 0.4000 - val_loss: 0.6346 - val_accuracy: 0.6164 - val_recall_1: 0.4706\n",
            "Epoch 64/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5578 - accuracy: 0.6765 - recall_1: 0.1042 - val_loss: 0.7047 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 65/1000\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.5824 - accuracy: 0.6812 - recall_1: 0.4754 - val_loss: 0.6461 - val_accuracy: 0.6438 - val_recall_1: 0.8235\n",
            "Epoch 66/1000\n",
            "5/5 [==============================] - 1s 102ms/step - loss: 0.5425 - accuracy: 0.7279 - recall_1: 0.3396 - val_loss: 0.6538 - val_accuracy: 0.6164 - val_recall_1: 0.2353\n",
            "Epoch 67/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5319 - accuracy: 0.7353 - recall_1: 0.2800 - val_loss: 0.6399 - val_accuracy: 0.6027 - val_recall_1: 0.4706\n",
            "Epoch 68/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5284 - accuracy: 0.7647 - recall_1: 0.4118 - val_loss: 0.6411 - val_accuracy: 0.6849 - val_recall_1: 0.4118\n",
            "Epoch 69/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5399 - accuracy: 0.7250 - recall_1: 0.3276 - val_loss: 0.6366 - val_accuracy: 0.6301 - val_recall_1: 0.6765\n",
            "Epoch 70/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5309 - accuracy: 0.7500 - recall_1: 0.3922 - val_loss: 0.6286 - val_accuracy: 0.6712 - val_recall_1: 0.4118\n",
            "Epoch 71/1000\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.5434 - accuracy: 0.7875 - recall_1: 0.6825 - val_loss: 0.6374 - val_accuracy: 0.6712 - val_recall_1: 0.3529\n",
            "Epoch 72/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.5161 - accuracy: 0.7721 - recall_1: 0.3800 - val_loss: 0.6281 - val_accuracy: 0.6438 - val_recall_1: 0.6765\n",
            "Epoch 73/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.5257 - accuracy: 0.7812 - recall_1: 0.6167 - val_loss: 0.6520 - val_accuracy: 0.6301 - val_recall_1: 0.2647\n",
            "Epoch 74/1000\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.5247 - accuracy: 0.7000 - recall_1: 0.2951 - val_loss: 0.6366 - val_accuracy: 0.6301 - val_recall_1: 0.8235\n",
            "Epoch 75/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.4923 - accuracy: 0.8529 - recall_1: 0.6863 - val_loss: 0.6494 - val_accuracy: 0.6712 - val_recall_1: 0.3235\n",
            "Epoch 76/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5477 - accuracy: 0.7500 - recall_1: 0.4727 - val_loss: 0.6194 - val_accuracy: 0.6849 - val_recall_1: 0.4412\n",
            "Epoch 77/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5338 - accuracy: 0.7059 - recall_1: 0.2200 - val_loss: 0.6215 - val_accuracy: 0.6712 - val_recall_1: 0.4118\n",
            "Epoch 78/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.4973 - accuracy: 0.8235 - recall_1: 0.6667 - val_loss: 0.6295 - val_accuracy: 0.6575 - val_recall_1: 0.8235\n",
            "Epoch 79/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.4896 - accuracy: 0.7625 - recall_1: 0.4643 - val_loss: 0.6243 - val_accuracy: 0.6986 - val_recall_1: 0.4118\n",
            "Epoch 80/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.4797 - accuracy: 0.7868 - recall_1: 0.4490 - val_loss: 0.7079 - val_accuracy: 0.5068 - val_recall_1: 0.9706\n",
            "Epoch 81/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.5037 - accuracy: 0.7941 - recall_1: 0.7778 - val_loss: 0.6605 - val_accuracy: 0.6301 - val_recall_1: 0.2353\n",
            "Epoch 82/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.4994 - accuracy: 0.7437 - recall_1: 0.2727 - val_loss: 0.6213 - val_accuracy: 0.6438 - val_recall_1: 0.6765\n",
            "Epoch 83/1000\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.4724 - accuracy: 0.8162 - recall_1: 0.6327 - val_loss: 0.6174 - val_accuracy: 0.6027 - val_recall_1: 0.5000\n",
            "Epoch 84/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.4954 - accuracy: 0.7563 - recall_1: 0.3934 - val_loss: 0.6198 - val_accuracy: 0.6301 - val_recall_1: 0.6765\n",
            "Epoch 85/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.4794 - accuracy: 0.8015 - recall_1: 0.4694 - val_loss: 0.6545 - val_accuracy: 0.6575 - val_recall_1: 0.3235\n",
            "Epoch 86/1000\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 0.4623 - accuracy: 0.7574 - recall_1: 0.3043 - val_loss: 0.6739 - val_accuracy: 0.5342 - val_recall_1: 0.9118\n",
            "Epoch 87/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5208 - accuracy: 0.7353 - recall_1: 0.8039 - val_loss: 0.6817 - val_accuracy: 0.6027 - val_recall_1: 0.1765\n",
            "Epoch 88/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.5561 - accuracy: 0.6838 - recall_1: 0.2075 - val_loss: 0.7108 - val_accuracy: 0.5068 - val_recall_1: 0.9412\n",
            "Epoch 89/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.5312 - accuracy: 0.7574 - recall_1: 0.7959 - val_loss: 0.6921 - val_accuracy: 0.5616 - val_recall_1: 0.0882\n",
            "Epoch 90/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5185 - accuracy: 0.6838 - recall_1: 0.1569 - val_loss: 0.6332 - val_accuracy: 0.6027 - val_recall_1: 0.6471\n",
            "Epoch 91/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6607 - accuracy: 0.5588 - recall_1: 0.7018 - val_loss: 0.6872 - val_accuracy: 0.5479 - val_recall_1: 0.9118\n",
            "Epoch 92/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.4893 - accuracy: 0.7574 - recall_1: 0.4468 - val_loss: 0.7158 - val_accuracy: 0.5068 - val_recall_1: 0.0000e+00\n",
            "Epoch 93/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.5608 - accuracy: 0.6812 - recall_1: 0.0893 - val_loss: 0.6640 - val_accuracy: 0.6164 - val_recall_1: 0.5882\n",
            "Epoch 94/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5378 - accuracy: 0.7375 - recall_1: 0.5231 - val_loss: 0.7079 - val_accuracy: 0.5205 - val_recall_1: 0.8824\n",
            "Epoch 95/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5425 - accuracy: 0.7750 - recall_1: 0.7619 - val_loss: 0.6907 - val_accuracy: 0.5479 - val_recall_1: 0.8824\n",
            "Epoch 96/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5166 - accuracy: 0.7500 - recall_1: 0.5870"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-ba18ead8a7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1429\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1432\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation - SpectAug + Gaussian Noise Injection"
      ],
      "metadata": {
        "id": "Odco1PT-cR58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Numpy Array Of Images.\n",
        "import numpy as np\n",
        "\n",
        "train_array = np.load(r'/content/drive/MyDrive/CV-EmpathNet/train_text.npy')\n",
        "train_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "471fd75a-0b93-457a-f331-5e626d97cda6",
        "id": "iBSCifGrcR59"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Numpy Array Of Images.\n",
        "import numpy as np\n",
        "\n",
        "train_array = np.load(r'/content/drive/MyDrive/CV-EmpathNet/train_image.npy')\n",
        "train_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1580a871-181a-4318-d3f2-4b973ccf23cd",
        "id": "1MaKFE2xcR59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163, 224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Numpy Array Of Images.\n",
        "import numpy as np\n",
        "\n",
        "train_array = np.load(r'/content/drive/MyDrive/CV-EmpathNet/train_image_3_channels.npy')\n",
        "train_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c269471-f361-4fa9-8860-f855196f3a6c",
        "id": "naFujf9NcR59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163, 288, 432, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wand\n",
        "!apt-get install libmagickwand-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574fcfa4-bb1e-458e-9bfd-aac41b7be349",
        "id": "dmtNiz3ucR59"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wand\n",
            "  Downloading Wand-0.6.7-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 39.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 30 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 40 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 51 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 61 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 71 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 81 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 92 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 102 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 112 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 122 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 133 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 139 kB 14.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: wand\n",
            "Successfully installed wand-0.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf spect_aug_noise"
      ],
      "metadata": {
        "id": "4IgSVChRcR59"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf train_aug_noise\n",
        "# !rm -rf test_aug_noise"
      ],
      "metadata": {
        "id": "CrN5Oe3FcR59"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r train train_aug_noise\n",
        "!cp -r test test_aug_noise"
      ],
      "metadata": {
        "id": "H8Cx6udccR59"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function Too Perform SpecAugment Strategy, placing random vertical and horizontal masks on the frequency and time domain.\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "from wand.image import Image\n",
        "\n",
        "\n",
        "src = '/content/train/0/302_P_spectrogram.png'\n",
        "dst = 'aug_img.jpg'\n",
        "n_vertical = 1\n",
        "nv_width = 1\n",
        "n_hor = 1\n",
        "nh_width = 1\n",
        "\n",
        "def aug(src, dst, n_vertical = 1, nv_width = 1, n_hor = 1, nh_width=1):\n",
        "  img = cv2.imread(src)\n",
        "\n",
        "  for i in range(n_vertical):\n",
        "    start_i = random.randint(0,224-nv_width)\n",
        "    img[:,start_i:start_i+nv_width,:] = 0\n",
        "\n",
        "  for i in range(n_hor):\n",
        "    start_i = random.randint(0,224-nh_width)\n",
        "    img[start_i:start_i+nh_width,:,:] = 0\n",
        "\n",
        "  cv2.imwrite(dst, img)\n",
        "\n",
        "\n",
        "aug(src,dst)\n",
        "\n"
      ],
      "metadata": {
        "id": "cmfNhxWoeqbk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SpecAgument\n",
        "\n",
        "#Increase the number of positive (depressed) class samples by using gaussian noise injection and SpecAugment like data augmentation techniques on them.\n",
        "from shutil import copy2\n",
        "\n",
        "\n",
        "# src = '/content/train/0'\n",
        "# dst = '/content/train_aug_noise/0'\n",
        "# for id in trdf[trdf['PHQ_Binary']==0]['Participant_ID']:\n",
        "#   with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "#     img.noise(\"gaussian\")\n",
        "#     img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n",
        "\n",
        "src = '/content/train/1'\n",
        "dst = '/content/train_aug_noise/1'\n",
        "for id in trdf[trdf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  aug(os.path.join(src, str(id) +'_P_spectrogram.png'),os.path.join(dst, str(id) +'saug_P_spectrogram.png'))\n",
        "\n",
        "src = '/content/test/1'\n",
        "dst = '/content/test_aug_noise/1'\n",
        "for id in tedf[tedf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  aug(os.path.join(src, str(id) +'_P_spectrogram.png'),os.path.join(dst, str(id) +'saug_P_spectrogram.png'))\n",
        "\n",
        "\n",
        "# src = '/content/test/0'\n",
        "# dst = '/content/test_aug_noise/0'\n",
        "# for id in tedf[tedf['PHQ_Binary']==0]['Participant_ID']:\n",
        "#   with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "#     img.noise(\"gaussian\")\n",
        "#     img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n"
      ],
      "metadata": {
        "id": "SdjrShP-cR5-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding Gaussian Noise\n",
        "\n",
        "from shutil import copy2\n",
        "\n",
        "\n",
        "# src = '/content/train/0'\n",
        "# dst = '/content/train_aug_noise/0'\n",
        "# for id in trdf[trdf['PHQ_Binary']==0]['Participant_ID']:\n",
        "#   with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "#     img.noise(\"gaussian\")\n",
        "#     img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n",
        "\n",
        "src = '/content/train/1'\n",
        "dst = '/content/train_aug_noise/1'\n",
        "for id in trdf[trdf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "    img.noise(\"gaussian\")\n",
        "    img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n",
        "\n",
        "src = '/content/test/1'\n",
        "dst = '/content/test_aug_noise/1'\n",
        "for id in tedf[tedf['PHQ_Binary']==1]['Participant_ID']:\n",
        "  with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "    img.noise(\"gaussian\")\n",
        "    img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n",
        "\n",
        "# src = '/content/test/0'\n",
        "# dst = '/content/test_aug_noise/0'\n",
        "# for id in tedf[tedf['PHQ_Binary']==0]['Participant_ID']:\n",
        "#   with Image(filename = os.path.join(src, str(id) +'_P_spectrogram.png')) as img:\n",
        "#     img.noise(\"gaussian\")\n",
        "#     img.save(filename = os.path.join(dst, str(id) +'aug_P_spectrogram.png'))\n"
      ],
      "metadata": {
        "id": "_Q3xFr3Zf_b8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Imbalance in the train set is resolved after data augmentation.\n",
        "p = len(os.listdir('/content/test/1'))\n",
        "n = len(os.listdir('/content/test/0'))\n",
        "print(p,n,n/(p+n))\n",
        "\n",
        "p = len(os.listdir('/content/train/1'))\n",
        "n = len(os.listdir('/content/train/0'))\n",
        "print(p,n,n/(p+n))\n",
        "\n",
        "p = len(os.listdir('/content/test_aug_noise/1'))\n",
        "n = len(os.listdir('/content/test_aug_noise/0'))\n",
        "print(p,n,n/(p+n))\n",
        "\n",
        "p = len(os.listdir('/content/train_aug_noise/1'))\n",
        "n = len(os.listdir('/content/train_aug_noise/0'))\n",
        "print(p,n,n/(p+n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me6T_LcAfYN0",
        "outputId": "d798225e-85f2-4286-fd39-561f4799df95"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17 39 0.6964285714285714\n",
            "37 126 0.7730061349693251\n",
            "51 39 0.43333333333333335\n",
            "111 126 0.5316455696202531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6\n",
        "# https://towardsdatascience.com/audio-deep-learning-made-simple-part-3-data-preparation-and-augmentation-24c6e1f6b52"
      ],
      "metadata": {
        "id": "FfQ4qNFlcR5_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "p4vWwhZ8cR5_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_func = tensorflow.keras.applications.xception.preprocess_input"
      ],
      "metadata": {
        "id": "J5opbTiKcR5_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "train_gen = train_datagen.flow_from_directory('/content/train_aug_noise', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "# train_gen = train_datagen.flow_from_directory('/content/train', (224,224), class_mode = 'categorical', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(train_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab443f37-4fa9-44f0-a334-b55763f02912",
        "id": "k1rwjMkccR5_"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 237 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch size\n",
        "bs = 32\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preproc_func)\n",
        "test_gen = test_datagen.flow_from_directory('/content/test_aug_noise', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "# test_gen = test_datagen.flow_from_directory('/content/test', (224,224), class_mode = 'binary', batch_size = bs, shuffle=True, seed=42)\n",
        "\n",
        "print('Class Indices')\n",
        "print(test_gen.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf76cb5-fbf0-40fd-97b4-d3fe5e41360a",
        "id": "uKsl-dj9cR5_"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 images belonging to 2 classes.\n",
            "Class Indices\n",
            "{'0': 0, '1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Callback Function To Save Model Based On A Combined Threshold of train and test accuracy and recall.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "class RecallAccuracyCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      print('Epoch:',epoch, 'Logs:',logs)\n",
        "\n",
        "      if logs['accuracy']>0.80 and logs['recall']>0.76 and logs['val_accuracy'] > 0.6 and logs['val_recall']>0.8:\n",
        "        print('Val Accuracy More Than Set Thresholds', logs)\n",
        "        print('Saving Model','-'*30)\n",
        "        self.model.save(f'{epoch}_model{str(logs)}.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "mc85IhqIIx0T"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results Obtained, Models Saved\n",
        "# 104_model{'loss': 0.4074825346469879, 'accuracy': 0.8374999761581421, 'recall': 0.9130434989929199,  'val_accuracy': 0.6111111044883728, 'val_recall': 0.843137264251709}.h5 'val_loss': 0.6829150915145874,\n",
        "# 86_model{'loss': 0.4598311483860016, 'accuracy': 0.8374999761581421, 'recall': 0.7702702879905701,  'val_accuracy': 0.6333333253860474, 'val_recall': 0.8039215803146362}.h5 'val_loss': 0.6286814212799072,\n",
        "# /content/98_model{'loss': 0.39345240592956543, 'accuracy': 0.8581560254096985, 'recall': 0.8529411554336548, , 'val_accuracy': 0.6111111044883728, 'val_recall': 0.843137264251709}.h5"
      ],
      "metadata": {
        "id": "BV5DAvNaXYtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recall Metric For Monitoring\n",
        "r = tf.keras.metrics.Recall()"
      ],
      "metadata": {
        "id": "CgjgI2y6RfA7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Custom CNN with Data Augmentation"
      ],
      "metadata": {
        "id": "lha9KO71VsfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom CNN 2\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(224,224,3)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.25))\n",
        "# model.add(Dense(10, activation=softmax))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Optional Checkpoints to save models based on different metrics.\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath_r = 'saug_weights_best_rmodel.hdf5'\n",
        "filepath_a = 'saug_weights_best_amodel.hdf5'\n",
        "filepath_vr = 'saug_weights_best_vrmodel.hdf5'\n",
        "filepath_va = 'saug_weights_best_vamodel.hdf5'\n",
        "\n",
        "# checkpoint = ModelCheckpoint(filepath,monitor = 'val_accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint1 = ModelCheckpoint(filepath_vr,monitor = 'val_recall',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint2 = ModelCheckpoint(filepath_va,monitor = 'val_accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint3 = ModelCheckpoint(filepath_a,monitor = 'accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint4 = ModelCheckpoint(filepath_r,monitor = 'recall',verbose = 1, save_best_only=True, mode='max')\n",
        "\n",
        "optim = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001,\n",
        "    # learning_rate=0.00001,\n",
        "    learning_rate=0.0001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy', r], optimizer=optim)\n",
        "#With 0.0001 lr.\n",
        "n_epochs = 1000\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2,checkpoint3,checkpoint4])\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks = [RecallAccuracyCallback()])\n",
        "# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nqJXbuWgwLK_",
        "outputId": "16a03d8f-91f9-4219-e6a6-426eb4f5b5a3"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.7000 - accuracy: 0.5188 - recall: 0.6048Epoch: 0 Logs: {'loss': 0.6999680399894714, 'accuracy': 0.518750011920929, 'recall': 0.6048387289047241, 'val_loss': 0.6854690313339233, 'val_accuracy': 0.5666666626930237, 'val_recall': 1.0}\n",
            "5/5 [==============================] - 2s 253ms/step - loss: 0.7000 - accuracy: 0.5188 - recall: 0.6048 - val_loss: 0.6855 - val_accuracy: 0.5667 - val_recall: 1.0000\n",
            "Epoch 2/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6991 - accuracy: 0.4844 - recall: 0.4844Epoch: 1 Logs: {'loss': 0.6981925964355469, 'accuracy': 0.4964539110660553, 'recall': 0.4492753744125366, 'val_loss': 0.6955417394638062, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6982 - accuracy: 0.4965 - recall: 0.4493 - val_loss: 0.6955 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.5319 - recall: 0.0000e+00Epoch: 2 Logs: {'loss': 0.6928126215934753, 'accuracy': 0.5319148898124695, 'recall': 0.0, 'val_loss': 0.6946485042572021, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6928 - accuracy: 0.5319 - recall: 0.0000e+00 - val_loss: 0.6946 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5437 - recall: 0.0000e+00Epoch: 3 Logs: {'loss': 0.692050576210022, 'accuracy': 0.543749988079071, 'recall': 0.0, 'val_loss': 0.6964287757873535, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.6921 - accuracy: 0.5437 - recall: 0.0000e+00 - val_loss: 0.6964 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5250 - recall: 0.0000e+00Epoch: 4 Logs: {'loss': 0.6917611360549927, 'accuracy': 0.5249999761581421, 'recall': 0.0, 'val_loss': 0.6991260051727295, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.6918 - accuracy: 0.5250 - recall: 0.0000e+00 - val_loss: 0.6991 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.5813 - recall: 0.0000e+00Epoch: 5 Logs: {'loss': 0.683931827545166, 'accuracy': 0.581250011920929, 'recall': 0.0, 'val_loss': 0.7109754681587219, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.6839 - accuracy: 0.5813 - recall: 0.0000e+00 - val_loss: 0.7110 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6960 - accuracy: 0.5156 - recall: 0.0000e+00Epoch: 6 Logs: {'loss': 0.6880140900611877, 'accuracy': 0.5460993051528931, 'recall': 0.0, 'val_loss': 0.7251560688018799, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.6880 - accuracy: 0.5461 - recall: 0.0000e+00 - val_loss: 0.7252 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.7011 - accuracy: 0.5312 - recall: 0.0000e+00Epoch: 7 Logs: {'loss': 0.7011136412620544, 'accuracy': 0.53125, 'recall': 0.0, 'val_loss': 0.737035870552063, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.7011 - accuracy: 0.5312 - recall: 0.0000e+00 - val_loss: 0.7370 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.5674 - recall: 0.0000e+00Epoch: 8 Logs: {'loss': 0.6809993982315063, 'accuracy': 0.567375898361206, 'recall': 0.0, 'val_loss': 0.7115599513053894, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6810 - accuracy: 0.5674 - recall: 0.0000e+00 - val_loss: 0.7116 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.5035 - recall: 0.0282    Epoch: 9 Logs: {'loss': 0.6950400471687317, 'accuracy': 0.5035461187362671, 'recall': 0.028169013559818268, 'val_loss': 0.6927481889724731, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 152ms/step - loss: 0.6950 - accuracy: 0.5035 - recall: 0.0282 - val_loss: 0.6927 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 11/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4681 - recall: 0.3485Epoch: 10 Logs: {'loss': 0.6932401061058044, 'accuracy': 0.4680851101875305, 'recall': 0.3484848439693451, 'val_loss': 0.6931644678115845, 'val_accuracy': 0.4444444477558136, 'val_recall': 0.019607843831181526}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.6932 - accuracy: 0.4681 - recall: 0.3485 - val_loss: 0.6932 - val_accuracy: 0.4444 - val_recall: 0.0196\n",
            "Epoch 12/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6937 - accuracy: 0.5469 - recall: 0.8143Epoch: 11 Logs: {'loss': 0.6932798624038696, 'accuracy': 0.5531914830207825, 'recall': 0.8333333134651184, 'val_loss': 0.6899805068969727, 'val_accuracy': 0.5666666626930237, 'val_recall': 1.0}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6933 - accuracy: 0.5532 - recall: 0.8333 - val_loss: 0.6900 - val_accuracy: 0.5667 - val_recall: 1.0000\n",
            "Epoch 13/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6955 - accuracy: 0.4539 - recall: 1.0000Epoch: 12 Logs: {'loss': 0.6955429911613464, 'accuracy': 0.45390069484710693, 'recall': 1.0, 'val_loss': 0.6918021440505981, 'val_accuracy': 0.5666666626930237, 'val_recall': 1.0}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.6955 - accuracy: 0.4539 - recall: 1.0000 - val_loss: 0.6918 - val_accuracy: 0.5667 - val_recall: 1.0000\n",
            "Epoch 14/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5312 - recall: 0.2698Epoch: 13 Logs: {'loss': 0.6926788687705994, 'accuracy': 0.53125, 'recall': 0.2698412835597992, 'val_loss': 0.6956236362457275, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6927 - accuracy: 0.5312 - recall: 0.2698 - val_loss: 0.6956 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5188 - recall: 0.0000e+00Epoch: 14 Logs: {'loss': 0.6917973160743713, 'accuracy': 0.518750011920929, 'recall': 0.0, 'val_loss': 0.6978147029876709, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.6918 - accuracy: 0.5188 - recall: 0.0000e+00 - val_loss: 0.6978 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.5248 - recall: 0.0000e+00Epoch: 15 Logs: {'loss': 0.6911119222640991, 'accuracy': 0.5248227119445801, 'recall': 0.0, 'val_loss': 0.7009381055831909, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6911 - accuracy: 0.5248 - recall: 0.0000e+00 - val_loss: 0.7009 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.5390 - recall: 0.0000e+00Epoch: 16 Logs: {'loss': 0.6909918785095215, 'accuracy': 0.5390070676803589, 'recall': 0.0, 'val_loss': 0.709787905216217, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6910 - accuracy: 0.5390 - recall: 0.0000e+00 - val_loss: 0.7098 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.5248 - recall: 0.0000e+00Epoch: 17 Logs: {'loss': 0.6919289231300354, 'accuracy': 0.5248227119445801, 'recall': 0.0, 'val_loss': 0.7041321396827698, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6919 - accuracy: 0.5248 - recall: 0.0000e+00 - val_loss: 0.7041 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.5674 - recall: 0.0000e+00Epoch: 18 Logs: {'loss': 0.6848307847976685, 'accuracy': 0.567375898361206, 'recall': 0.0, 'val_loss': 0.7048024535179138, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6848 - accuracy: 0.5674 - recall: 0.0000e+00 - val_loss: 0.7048 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5177 - recall: 0.0000e+00Epoch: 19 Logs: {'loss': 0.6917811632156372, 'accuracy': 0.5177304744720459, 'recall': 0.0, 'val_loss': 0.7095727920532227, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.6918 - accuracy: 0.5177 - recall: 0.0000e+00 - val_loss: 0.7096 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.5674 - recall: 0.0000e+00Epoch: 20 Logs: {'loss': 0.6813730001449585, 'accuracy': 0.567375898361206, 'recall': 0.0, 'val_loss': 0.7134796380996704, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6814 - accuracy: 0.5674 - recall: 0.0000e+00 - val_loss: 0.7135 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.5063 - recall: 0.0000e+00Epoch: 21 Logs: {'loss': 0.6949693560600281, 'accuracy': 0.5062500238418579, 'recall': 0.0, 'val_loss': 0.7063737511634827, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.6950 - accuracy: 0.5063 - recall: 0.0000e+00 - val_loss: 0.7064 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.5674 - recall: 0.0000e+00Epoch: 22 Logs: {'loss': 0.6820574998855591, 'accuracy': 0.567375898361206, 'recall': 0.0, 'val_loss': 0.7045676112174988, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.6821 - accuracy: 0.5674 - recall: 0.0000e+00 - val_loss: 0.7046 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6865 - accuracy: 0.5469 - recall: 0.0000e+00Epoch: 23 Logs: {'loss': 0.6864216923713684, 'accuracy': 0.5460993051528931, 'recall': 0.0, 'val_loss': 0.707182765007019, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.6864 - accuracy: 0.5461 - recall: 0.0000e+00 - val_loss: 0.7072 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.5688 - recall: 0.0000e+00Epoch: 24 Logs: {'loss': 0.6798310279846191, 'accuracy': 0.5687500238418579, 'recall': 0.0, 'val_loss': 0.7072306275367737, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.6798 - accuracy: 0.5688 - recall: 0.0000e+00 - val_loss: 0.7072 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.5437 - recall: 0.0000e+00Epoch: 25 Logs: {'loss': 0.6849583387374878, 'accuracy': 0.543749988079071, 'recall': 0.0, 'val_loss': 0.7121076583862305, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.6850 - accuracy: 0.5437 - recall: 0.0000e+00 - val_loss: 0.7121 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6781 - accuracy: 0.5674 - recall: 0.0000e+00Epoch: 26 Logs: {'loss': 0.6781114339828491, 'accuracy': 0.567375898361206, 'recall': 0.0, 'val_loss': 0.7120979428291321, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6781 - accuracy: 0.5674 - recall: 0.0000e+00 - val_loss: 0.7121 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6916 - accuracy: 0.5156 - recall: 0.0000e+00Epoch: 27 Logs: {'loss': 0.6898149251937866, 'accuracy': 0.5248227119445801, 'recall': 0.0, 'val_loss': 0.7004849910736084, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.6898 - accuracy: 0.5248 - recall: 0.0000e+00 - val_loss: 0.7005 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6818 - accuracy: 0.5547 - recall: 0.0000e+00Epoch: 28 Logs: {'loss': 0.6834977865219116, 'accuracy': 0.5460993051528931, 'recall': 0.0, 'val_loss': 0.7052940726280212, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.6835 - accuracy: 0.5461 - recall: 0.0000e+00 - val_loss: 0.7053 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6948 - accuracy: 0.4453 - recall: 0.0000e+00Epoch: 29 Logs: {'loss': 0.6945001482963562, 'accuracy': 0.45390069484710693, 'recall': 0.01315789483487606, 'val_loss': 0.6913168430328369, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.8823529481887817}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.6945 - accuracy: 0.4539 - recall: 0.0132 - val_loss: 0.6913 - val_accuracy: 0.5778 - val_recall: 0.8824\n",
            "Epoch 31/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.6667 - recall: 0.4667Epoch: 30 Logs: {'loss': 0.6887995004653931, 'accuracy': 0.6666666865348816, 'recall': 0.46666666865348816, 'val_loss': 0.6980618238449097, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6888 - accuracy: 0.6667 - recall: 0.4667 - val_loss: 0.6981 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.4894 - recall: 0.0000e+00Epoch: 31 Logs: {'loss': 0.6918103694915771, 'accuracy': 0.4893617033958435, 'recall': 0.0, 'val_loss': 0.7097626328468323, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6918 - accuracy: 0.4894 - recall: 0.0000e+00 - val_loss: 0.7098 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6812 - accuracy: 0.5319 - recall: 0.0000e+00Epoch: 32 Logs: {'loss': 0.681244432926178, 'accuracy': 0.5319148898124695, 'recall': 0.0, 'val_loss': 0.702423095703125, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6812 - accuracy: 0.5319 - recall: 0.0000e+00 - val_loss: 0.7024 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.5603 - recall: 0.0000e+00Epoch: 33 Logs: {'loss': 0.6763671636581421, 'accuracy': 0.5602836608886719, 'recall': 0.0, 'val_loss': 0.702729344367981, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 152ms/step - loss: 0.6764 - accuracy: 0.5603 - recall: 0.0000e+00 - val_loss: 0.7027 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5106 - recall: 0.0000e+00Epoch: 34 Logs: {'loss': 0.6817615032196045, 'accuracy': 0.5106382966041565, 'recall': 0.0, 'val_loss': 0.6958449482917786, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 155ms/step - loss: 0.6818 - accuracy: 0.5106 - recall: 0.0000e+00 - val_loss: 0.6958 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 36/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.5035 - recall: 0.0000e+00Epoch: 35 Logs: {'loss': 0.6838235259056091, 'accuracy': 0.5035461187362671, 'recall': 0.0, 'val_loss': 0.6962185502052307, 'val_accuracy': 0.41111111640930176, 'val_recall': 0.019607843831181526}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6838 - accuracy: 0.5035 - recall: 0.0000e+00 - val_loss: 0.6962 - val_accuracy: 0.4111 - val_recall: 0.0196\n",
            "Epoch 37/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6772 - accuracy: 0.5745 - recall: 0.1406Epoch: 36 Logs: {'loss': 0.6771546006202698, 'accuracy': 0.5744680762290955, 'recall': 0.140625, 'val_loss': 0.6928762793540955, 'val_accuracy': 0.47777777910232544, 'val_recall': 0.1764705926179886}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6772 - accuracy: 0.5745 - recall: 0.1406 - val_loss: 0.6929 - val_accuracy: 0.4778 - val_recall: 0.1765\n",
            "Epoch 38/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.6099 - recall: 0.0517Epoch: 37 Logs: {'loss': 0.6659693717956543, 'accuracy': 0.609929084777832, 'recall': 0.0517241396009922, 'val_loss': 0.7002102136611938, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.6660 - accuracy: 0.6099 - recall: 0.0517 - val_loss: 0.7002 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.5250 - recall: 0.0000e+00Epoch: 38 Logs: {'loss': 0.6740757822990417, 'accuracy': 0.5249999761581421, 'recall': 0.0, 'val_loss': 0.7030478119850159, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.6741 - accuracy: 0.5250 - recall: 0.0000e+00 - val_loss: 0.7030 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 40/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6721 - accuracy: 0.5437 - recall: 0.0000e+00Epoch: 39 Logs: {'loss': 0.6721264123916626, 'accuracy': 0.543749988079071, 'recall': 0.0, 'val_loss': 0.6973876953125, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6721 - accuracy: 0.5437 - recall: 0.0000e+00 - val_loss: 0.6974 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 41/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6479 - accuracy: 0.6383 - recall: 0.0000e+00Epoch: 40 Logs: {'loss': 0.6478859782218933, 'accuracy': 0.6382978558540344, 'recall': 0.0, 'val_loss': 0.7244565486907959, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6479 - accuracy: 0.6383 - recall: 0.0000e+00 - val_loss: 0.7245 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5125 - recall: 0.0000e+00Epoch: 41 Logs: {'loss': 0.6926552653312683, 'accuracy': 0.512499988079071, 'recall': 0.0, 'val_loss': 0.7067614793777466, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6927 - accuracy: 0.5125 - recall: 0.0000e+00 - val_loss: 0.7068 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6805 - accuracy: 0.5156 - recall: 0.0896    Epoch: 42 Logs: {'loss': 0.6810370087623596, 'accuracy': 0.5177304744720459, 'recall': 0.10000000149011612, 'val_loss': 0.6871588826179504, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.7843137383460999}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.6810 - accuracy: 0.5177 - recall: 0.1000 - val_loss: 0.6872 - val_accuracy: 0.5556 - val_recall: 0.7843\n",
            "Epoch 44/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.6313 - recall: 0.3421Epoch: 43 Logs: {'loss': 0.6717318296432495, 'accuracy': 0.6312500238418579, 'recall': 0.34210526943206787, 'val_loss': 0.6937340497970581, 'val_accuracy': 0.47777777910232544, 'val_recall': 0.13725490868091583}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.6717 - accuracy: 0.6313 - recall: 0.3421 - val_loss: 0.6937 - val_accuracy: 0.4778 - val_recall: 0.1373\n",
            "Epoch 45/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6605 - accuracy: 0.5625 - recall: 0.0417    Epoch: 44 Logs: {'loss': 0.6604517698287964, 'accuracy': 0.5625, 'recall': 0.0416666679084301, 'val_loss': 0.6909936666488647, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.23529411852359772}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6605 - accuracy: 0.5625 - recall: 0.0417 - val_loss: 0.6910 - val_accuracy: 0.5333 - val_recall: 0.2353\n",
            "Epoch 46/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6696 - accuracy: 0.5859 - recall: 0.2063Epoch: 45 Logs: {'loss': 0.6689060926437378, 'accuracy': 0.5744680762290955, 'recall': 0.18571428954601288, 'val_loss': 0.6775435209274292, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.8039215803146362}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.6689 - accuracy: 0.5745 - recall: 0.1857 - val_loss: 0.6775 - val_accuracy: 0.6333 - val_recall: 0.8039\n",
            "Epoch 47/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6578 - accuracy: 0.6500 - recall: 0.2464Epoch: 46 Logs: {'loss': 0.6577862501144409, 'accuracy': 0.6499999761581421, 'recall': 0.24637681245803833, 'val_loss': 0.695780873298645, 'val_accuracy': 0.4555555582046509, 'val_recall': 0.03921568766236305}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.6578 - accuracy: 0.6500 - recall: 0.2464 - val_loss: 0.6958 - val_accuracy: 0.4556 - val_recall: 0.0392\n",
            "Epoch 48/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.6525 - recall: 0.4795Epoch: 47 Logs: {'loss': 0.6626943945884705, 'accuracy': 0.652482271194458, 'recall': 0.4794520437717438, 'val_loss': 0.6740213632583618, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.8039215803146362}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6627 - accuracy: 0.6525 - recall: 0.4795 - val_loss: 0.6740 - val_accuracy: 0.6333 - val_recall: 0.8039\n",
            "Epoch 49/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6491 - accuracy: 0.6562 - recall: 0.3377Epoch: 48 Logs: {'loss': 0.6490998864173889, 'accuracy': 0.65625, 'recall': 0.33766233921051025, 'val_loss': 0.6907374262809753, 'val_accuracy': 0.5111111402511597, 'val_recall': 0.1568627506494522}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.6491 - accuracy: 0.6562 - recall: 0.3377 - val_loss: 0.6907 - val_accuracy: 0.5111 - val_recall: 0.1569\n",
            "Epoch 50/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6359 - accuracy: 0.6750 - recall: 0.3750Epoch: 49 Logs: {'loss': 0.6359072923660278, 'accuracy': 0.675000011920929, 'recall': 0.375, 'val_loss': 0.6646612286567688, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.8627451062202454}\n",
            "5/5 [==============================] - 1s 148ms/step - loss: 0.6359 - accuracy: 0.6750 - recall: 0.3750 - val_loss: 0.6647 - val_accuracy: 0.5889 - val_recall: 0.8627\n",
            "Epoch 51/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.7188 - recall: 0.4000Epoch: 50 Logs: {'loss': 0.6181092262268066, 'accuracy': 0.71875, 'recall': 0.4000000059604645, 'val_loss': 0.7225116491317749, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.6181 - accuracy: 0.7188 - recall: 0.4000 - val_loss: 0.7225 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6402 - accuracy: 0.5816 - recall: 0.3333Epoch: 51 Logs: {'loss': 0.6401850581169128, 'accuracy': 0.5815602540969849, 'recall': 0.3333333432674408, 'val_loss': 0.6682805418968201, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.6470588445663452}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.6402 - accuracy: 0.5816 - recall: 0.3333 - val_loss: 0.6683 - val_accuracy: 0.6333 - val_recall: 0.6471\n",
            "Epoch 53/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.6170 - recall: 0.2381Epoch: 52 Logs: {'loss': 0.6430473327636719, 'accuracy': 0.6170212626457214, 'recall': 0.2380952388048172, 'val_loss': 0.6627448201179504, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.843137264251709}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.6430 - accuracy: 0.6170 - recall: 0.2381 - val_loss: 0.6627 - val_accuracy: 0.6000 - val_recall: 0.8431\n",
            "Epoch 54/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6234 - accuracy: 0.7312 - recall: 0.7733Epoch: 53 Logs: {'loss': 0.623425304889679, 'accuracy': 0.731249988079071, 'recall': 0.7733333110809326, 'val_loss': 0.6912397742271423, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.3137255012989044}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.6234 - accuracy: 0.7312 - recall: 0.7733 - val_loss: 0.6912 - val_accuracy: 0.5556 - val_recall: 0.3137\n",
            "Epoch 55/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6374 - accuracy: 0.6170 - recall: 0.2571Epoch: 54 Logs: {'loss': 0.6373920440673828, 'accuracy': 0.6170212626457214, 'recall': 0.2571428716182709, 'val_loss': 0.6666919589042664, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.8235294222831726}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6374 - accuracy: 0.6170 - recall: 0.2571 - val_loss: 0.6667 - val_accuracy: 0.5667 - val_recall: 0.8235\n",
            "Epoch 56/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.6438 - recall: 0.8101Epoch: 55 Logs: {'loss': 0.6255320906639099, 'accuracy': 0.643750011920929, 'recall': 0.8101266026496887, 'val_loss': 0.66392982006073, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.9215686321258545}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.6255 - accuracy: 0.6438 - recall: 0.8101 - val_loss: 0.6639 - val_accuracy: 0.5889 - val_recall: 0.9216\n",
            "Epoch 57/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5854 - accuracy: 0.7163 - recall: 0.5738Epoch: 56 Logs: {'loss': 0.5854402184486389, 'accuracy': 0.716312050819397, 'recall': 0.5737704634666443, 'val_loss': 0.6943075656890869, 'val_accuracy': 0.5444444417953491, 'val_recall': 0.29411765933036804}\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.5854 - accuracy: 0.7163 - recall: 0.5738 - val_loss: 0.6943 - val_accuracy: 0.5444 - val_recall: 0.2941\n",
            "Epoch 58/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6388 - accuracy: 0.6099 - recall: 0.1940Epoch: 57 Logs: {'loss': 0.6387900710105896, 'accuracy': 0.609929084777832, 'recall': 0.19402985274791718, 'val_loss': 0.6663796901702881, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.9803921580314636}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.6388 - accuracy: 0.6099 - recall: 0.1940 - val_loss: 0.6664 - val_accuracy: 0.6000 - val_recall: 0.9804\n",
            "Epoch 59/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.6875 - recall: 0.6571Epoch: 58 Logs: {'loss': 0.5919505953788757, 'accuracy': 0.6875, 'recall': 0.6571428775787354, 'val_loss': 0.6871510744094849, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5920 - accuracy: 0.6875 - recall: 0.6571 - val_loss: 0.6872 - val_accuracy: 0.5556 - val_recall: 0.4118\n",
            "Epoch 60/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5801 - accuracy: 0.6625 - recall: 0.2388Epoch: 59 Logs: {'loss': 0.5800951719284058, 'accuracy': 0.6625000238418579, 'recall': 0.23880596458911896, 'val_loss': 0.6666535139083862, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.7450980544090271}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.5801 - accuracy: 0.6625 - recall: 0.2388 - val_loss: 0.6667 - val_accuracy: 0.6111 - val_recall: 0.7451\n",
            "Epoch 61/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5679 - accuracy: 0.7969 - recall: 0.8182Epoch: 60 Logs: {'loss': 0.5802509188652039, 'accuracy': 0.7659574747085571, 'recall': 0.8235294222831726, 'val_loss': 0.6581311821937561, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.843137264251709}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5803 - accuracy: 0.7660 - recall: 0.8235 - val_loss: 0.6581 - val_accuracy: 0.5667 - val_recall: 0.8431\n",
            "Epoch 62/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.7188 - recall: 0.4429Epoch: 61 Logs: {'loss': 0.5629730224609375, 'accuracy': 0.71875, 'recall': 0.44285714626312256, 'val_loss': 0.6898588538169861, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.37254902720451355}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5630 - accuracy: 0.7188 - recall: 0.4429 - val_loss: 0.6899 - val_accuracy: 0.5667 - val_recall: 0.3725\n",
            "Epoch 63/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.6562 - recall: 0.4533Epoch: 62 Logs: {'loss': 0.5955615639686584, 'accuracy': 0.65625, 'recall': 0.4533333480358124, 'val_loss': 0.6555373668670654, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.8823529481887817}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5956 - accuracy: 0.6562 - recall: 0.4533 - val_loss: 0.6555 - val_accuracy: 0.5778 - val_recall: 0.8824\n",
            "Epoch 64/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.8000 - recall: 0.7808Epoch: 63 Logs: {'loss': 0.5586953163146973, 'accuracy': 0.800000011920929, 'recall': 0.7808219194412231, 'val_loss': 0.6710001826286316, 'val_accuracy': 0.5444444417953491, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5587 - accuracy: 0.8000 - recall: 0.7808 - val_loss: 0.6710 - val_accuracy: 0.5444 - val_recall: 0.4118\n",
            "Epoch 65/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5943 - accuracy: 0.7092 - recall: 0.7534Epoch: 64 Logs: {'loss': 0.5942779183387756, 'accuracy': 0.7092198729515076, 'recall': 0.7534246444702148, 'val_loss': 0.6565566062927246, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.6470588445663452}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5943 - accuracy: 0.7092 - recall: 0.7534 - val_loss: 0.6566 - val_accuracy: 0.6111 - val_recall: 0.6471\n",
            "Epoch 66/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.6938 - recall: 0.4675Epoch: 65 Logs: {'loss': 0.5613171458244324, 'accuracy': 0.6937500238418579, 'recall': 0.4675324559211731, 'val_loss': 0.6475147604942322, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.7843137383460999}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5613 - accuracy: 0.6938 - recall: 0.4675 - val_loss: 0.6475 - val_accuracy: 0.6111 - val_recall: 0.7843\n",
            "Epoch 67/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5323 - accuracy: 0.8014 - recall: 0.7463Epoch: 66 Logs: {'loss': 0.5323197245597839, 'accuracy': 0.8014184236526489, 'recall': 0.746268630027771, 'val_loss': 0.6447607278823853, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.843137264251709}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.5323 - accuracy: 0.8014 - recall: 0.7463 - val_loss: 0.6448 - val_accuracy: 0.5889 - val_recall: 0.8431\n",
            "Epoch 68/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.7312 - recall: 0.5972Epoch: 67 Logs: {'loss': 0.5558019876480103, 'accuracy': 0.731249988079071, 'recall': 0.5972222089767456, 'val_loss': 0.7300118207931519, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.21568627655506134}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5558 - accuracy: 0.7312 - recall: 0.5972 - val_loss: 0.7300 - val_accuracy: 0.5333 - val_recall: 0.2157\n",
            "Epoch 69/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5855 - accuracy: 0.6738 - recall: 0.6667Epoch: 68 Logs: {'loss': 0.5855304002761841, 'accuracy': 0.673758864402771, 'recall': 0.6666666865348816, 'val_loss': 0.6666022539138794, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5855 - accuracy: 0.6738 - recall: 0.6667 - val_loss: 0.6666 - val_accuracy: 0.5889 - val_recall: 0.4902\n",
            "Epoch 70/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.6170 - recall: 0.1905Epoch: 69 Logs: {'loss': 0.6367262005805969, 'accuracy': 0.6170212626457214, 'recall': 0.190476194024086, 'val_loss': 0.6997813582420349, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.6367 - accuracy: 0.6170 - recall: 0.1905 - val_loss: 0.6998 - val_accuracy: 0.5556 - val_recall: 0.5882\n",
            "Epoch 71/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5921 - accuracy: 0.7109 - recall: 0.6290Epoch: 70 Logs: {'loss': 0.6016291975975037, 'accuracy': 0.695035457611084, 'recall': 0.6567164063453674, 'val_loss': 0.7049148678779602, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.9803921580314636}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.6016 - accuracy: 0.6950 - recall: 0.6567 - val_loss: 0.7049 - val_accuracy: 0.5889 - val_recall: 0.9804\n",
            "Epoch 72/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.6879 - recall: 0.8873Epoch: 71 Logs: {'loss': 0.6073509454727173, 'accuracy': 0.6879432797431946, 'recall': 0.8873239159584045, 'val_loss': 0.6690856218338013, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.8039215803146362}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6074 - accuracy: 0.6879 - recall: 0.8873 - val_loss: 0.6691 - val_accuracy: 0.5778 - val_recall: 0.8039\n",
            "Epoch 73/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5540 - accuracy: 0.7500 - recall: 0.6441Epoch: 72 Logs: {'loss': 0.5493794679641724, 'accuracy': 0.7659574747085571, 'recall': 0.6769230961799622, 'val_loss': 0.6386591792106628, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.8235294222831726}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5494 - accuracy: 0.7660 - recall: 0.6769 - val_loss: 0.6387 - val_accuracy: 0.6000 - val_recall: 0.8235\n",
            "Epoch 74/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4997 - accuracy: 0.8085 - recall: 0.7538Epoch: 73 Logs: {'loss': 0.4996795654296875, 'accuracy': 0.8085106611251831, 'recall': 0.7538461685180664, 'val_loss': 0.6554860472679138, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.4997 - accuracy: 0.8085 - recall: 0.7538 - val_loss: 0.6555 - val_accuracy: 0.6333 - val_recall: 0.4314\n",
            "Epoch 75/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5410 - accuracy: 0.6938 - recall: 0.7564Epoch: 74 Logs: {'loss': 0.541046142578125, 'accuracy': 0.6937500238418579, 'recall': 0.7564102411270142, 'val_loss': 0.7540572881698608, 'val_accuracy': 0.5222222208976746, 'val_recall': 0.1764705926179886}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.5410 - accuracy: 0.6938 - recall: 0.7564 - val_loss: 0.7541 - val_accuracy: 0.5222 - val_recall: 0.1765\n",
            "Epoch 76/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.8014 - recall: 0.7231Epoch: 75 Logs: {'loss': 0.5029604434967041, 'accuracy': 0.8014184236526489, 'recall': 0.7230769395828247, 'val_loss': 0.6392484903335571, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.6274510025978088}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.5030 - accuracy: 0.8014 - recall: 0.7231 - val_loss: 0.6392 - val_accuracy: 0.6333 - val_recall: 0.6275\n",
            "Epoch 77/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4779 - accuracy: 0.8511 - recall: 0.7857Epoch: 76 Logs: {'loss': 0.4779220521450043, 'accuracy': 0.8510638475418091, 'recall': 0.7857142686843872, 'val_loss': 0.6470837593078613, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.4779 - accuracy: 0.8511 - recall: 0.7857 - val_loss: 0.6471 - val_accuracy: 0.6222 - val_recall: 0.5098\n",
            "Epoch 78/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4897 - accuracy: 0.8014 - recall: 0.8209Epoch: 77 Logs: {'loss': 0.48971545696258545, 'accuracy': 0.8014184236526489, 'recall': 0.8208954930305481, 'val_loss': 0.7476368546485901, 'val_accuracy': 0.5444444417953491, 'val_recall': 0.23529411852359772}\n",
            "5/5 [==============================] - 1s 152ms/step - loss: 0.4897 - accuracy: 0.8014 - recall: 0.8209 - val_loss: 0.7476 - val_accuracy: 0.5444 - val_recall: 0.2353\n",
            "Epoch 79/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.7312 - recall: 0.7215Epoch: 78 Logs: {'loss': 0.5408514738082886, 'accuracy': 0.731249988079071, 'recall': 0.7215189933776855, 'val_loss': 0.6432884335517883, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.9019607901573181}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.5409 - accuracy: 0.7312 - recall: 0.7215 - val_loss: 0.6433 - val_accuracy: 0.5778 - val_recall: 0.9020\n",
            "Epoch 80/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.7092 - recall: 0.4714Epoch: 79 Logs: {'loss': 0.5351221561431885, 'accuracy': 0.7092198729515076, 'recall': 0.4714285731315613, 'val_loss': 0.6869792342185974, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.9411764740943909}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.5351 - accuracy: 0.7092 - recall: 0.4714 - val_loss: 0.6870 - val_accuracy: 0.5778 - val_recall: 0.9412\n",
            "Epoch 81/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.7188 - recall: 0.7436Epoch: 80 Logs: {'loss': 0.5326938629150391, 'accuracy': 0.71875, 'recall': 0.7435897588729858, 'val_loss': 0.7431923747062683, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.29411765933036804}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.5327 - accuracy: 0.7188 - recall: 0.7436 - val_loss: 0.7432 - val_accuracy: 0.5556 - val_recall: 0.2941\n",
            "Epoch 82/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.7500 - recall: 0.6806Epoch: 81 Logs: {'loss': 0.535082221031189, 'accuracy': 0.75, 'recall': 0.6805555820465088, 'val_loss': 0.6578559875488281, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.8627451062202454}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5351 - accuracy: 0.7500 - recall: 0.6806 - val_loss: 0.6579 - val_accuracy: 0.5889 - val_recall: 0.8627\n",
            "Epoch 83/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4897 - accuracy: 0.7734 - recall: 0.6852Epoch: 82 Logs: {'loss': 0.49369314312934875, 'accuracy': 0.7659574747085571, 'recall': 0.6557376980781555, 'val_loss': 0.6598963737487793, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.6470588445663452}\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.4937 - accuracy: 0.7660 - recall: 0.6557 - val_loss: 0.6599 - val_accuracy: 0.6222 - val_recall: 0.6471\n",
            "Epoch 84/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.8313 - recall: 0.7568Epoch: 83 Logs: {'loss': 0.45262303948402405, 'accuracy': 0.831250011920929, 'recall': 0.7567567825317383, 'val_loss': 0.6492865085601807, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.9019607901573181}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.4526 - accuracy: 0.8313 - recall: 0.7568 - val_loss: 0.6493 - val_accuracy: 0.6000 - val_recall: 0.9020\n",
            "Epoch 85/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4445 - accuracy: 0.8723 - recall: 0.8710Epoch: 84 Logs: {'loss': 0.4445461928844452, 'accuracy': 0.8723404407501221, 'recall': 0.8709677457809448, 'val_loss': 0.6643164157867432, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.4445 - accuracy: 0.8723 - recall: 0.8710 - val_loss: 0.6643 - val_accuracy: 0.6222 - val_recall: 0.4902\n",
            "Epoch 86/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.8156 - recall: 0.7536Epoch: 85 Logs: {'loss': 0.44122055172920227, 'accuracy': 0.8156028389930725, 'recall': 0.7536231875419617, 'val_loss': 0.6511062383651733, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.9019607901573181}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4412 - accuracy: 0.8156 - recall: 0.7536 - val_loss: 0.6511 - val_accuracy: 0.5778 - val_recall: 0.9020\n",
            "Epoch 87/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4598 - accuracy: 0.8375 - recall: 0.7703Epoch: 86 Logs: {'loss': 0.4598311483860016, 'accuracy': 0.8374999761581421, 'recall': 0.7702702879905701, 'val_loss': 0.6286814212799072, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.8039215803146362}\n",
            "Val Accuracy More Than Set Thresholds {'loss': 0.4598311483860016, 'accuracy': 0.8374999761581421, 'recall': 0.7702702879905701, 'val_loss': 0.6286814212799072, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.8039215803146362}\n",
            "Saving Model ------------------------------\n",
            "5/5 [==============================] - 3s 664ms/step - loss: 0.4598 - accuracy: 0.8375 - recall: 0.7703 - val_loss: 0.6287 - val_accuracy: 0.6333 - val_recall: 0.8039\n",
            "Epoch 88/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3952 - accuracy: 0.8984 - recall: 0.8500Epoch: 87 Logs: {'loss': 0.4038110375404358, 'accuracy': 0.8936170339584351, 'recall': 0.859375, 'val_loss': 0.6899085640907288, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.37254902720451355}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.4038 - accuracy: 0.8936 - recall: 0.8594 - val_loss: 0.6899 - val_accuracy: 0.6000 - val_recall: 0.3725\n",
            "Epoch 89/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.8085 - recall: 0.6769Epoch: 88 Logs: {'loss': 0.4354705810546875, 'accuracy': 0.8085106611251831, 'recall': 0.6769230961799622, 'val_loss': 0.6543024778366089, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.9019607901573181}\n",
            "5/5 [==============================] - 1s 158ms/step - loss: 0.4355 - accuracy: 0.8085 - recall: 0.6769 - val_loss: 0.6543 - val_accuracy: 0.5778 - val_recall: 0.9020\n",
            "Epoch 90/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8188 - recall: 0.7361Epoch: 89 Logs: {'loss': 0.4212983548641205, 'accuracy': 0.8187500238418579, 'recall': 0.7361111044883728, 'val_loss': 0.6639770269393921, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.4213 - accuracy: 0.8188 - recall: 0.7361 - val_loss: 0.6640 - val_accuracy: 0.6222 - val_recall: 0.4902\n",
            "Epoch 91/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.7943 - recall: 0.8514Epoch: 90 Logs: {'loss': 0.4571545124053955, 'accuracy': 0.7943262457847595, 'recall': 0.8513513803482056, 'val_loss': 0.7763664126396179, 'val_accuracy': 0.5444444417953491, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.4572 - accuracy: 0.7943 - recall: 0.8514 - val_loss: 0.7764 - val_accuracy: 0.5444 - val_recall: 0.2549\n",
            "Epoch 92/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.7447 - recall: 0.5238Epoch: 91 Logs: {'loss': 0.4774734675884247, 'accuracy': 0.7446808218955994, 'recall': 0.523809552192688, 'val_loss': 0.7079929709434509, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.9411764740943909}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.4775 - accuracy: 0.7447 - recall: 0.5238 - val_loss: 0.7080 - val_accuracy: 0.5889 - val_recall: 0.9412\n",
            "Epoch 93/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.7375 - recall: 0.6765Epoch: 92 Logs: {'loss': 0.5137943029403687, 'accuracy': 0.737500011920929, 'recall': 0.6764705777168274, 'val_loss': 0.7382710576057434, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.3529411852359772}\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.5138 - accuracy: 0.7375 - recall: 0.6765 - val_loss: 0.7383 - val_accuracy: 0.5889 - val_recall: 0.3529\n",
            "Epoch 94/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.8062 - recall: 0.8267Epoch: 93 Logs: {'loss': 0.4480525851249695, 'accuracy': 0.8062499761581421, 'recall': 0.8266666531562805, 'val_loss': 0.6901377439498901, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.9019607901573181}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.4481 - accuracy: 0.8062 - recall: 0.8267 - val_loss: 0.6901 - val_accuracy: 0.5667 - val_recall: 0.9020\n",
            "Epoch 95/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4438 - accuracy: 0.7447 - recall: 0.6667Epoch: 94 Logs: {'loss': 0.4437624216079712, 'accuracy': 0.7446808218955994, 'recall': 0.6666666865348816, 'val_loss': 0.6806367635726929, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.4438 - accuracy: 0.7447 - recall: 0.6667 - val_loss: 0.6806 - val_accuracy: 0.6333 - val_recall: 0.6078\n",
            "Epoch 96/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4148 - accuracy: 0.8125 - recall: 0.8571Epoch: 95 Logs: {'loss': 0.41479092836380005, 'accuracy': 0.8125, 'recall': 0.8571428656578064, 'val_loss': 0.6570318937301636, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.8627451062202454}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.4148 - accuracy: 0.8125 - recall: 0.8571 - val_loss: 0.6570 - val_accuracy: 0.5889 - val_recall: 0.8627\n",
            "Epoch 97/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.8000 - recall: 0.6711Epoch: 96 Logs: {'loss': 0.4379096031188965, 'accuracy': 0.800000011920929, 'recall': 0.6710526347160339, 'val_loss': 0.6419141292572021, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.7843137383460999}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.4379 - accuracy: 0.8000 - recall: 0.6711 - val_loss: 0.6419 - val_accuracy: 0.6000 - val_recall: 0.7843\n",
            "Epoch 98/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.8687 - recall: 0.9014Epoch: 97 Logs: {'loss': 0.3802955150604248, 'accuracy': 0.8687499761581421, 'recall': 0.9014084339141846, 'val_loss': 0.7037022709846497, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.3803 - accuracy: 0.8687 - recall: 0.9014 - val_loss: 0.7037 - val_accuracy: 0.5889 - val_recall: 0.4118\n",
            "Epoch 99/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.8582 - recall: 0.8529Epoch: 98 Logs: {'loss': 0.39345240592956543, 'accuracy': 0.8581560254096985, 'recall': 0.8529411554336548, 'val_loss': 0.6412600874900818, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.843137264251709}\n",
            "Val Accuracy More Than Set Thresholds {'loss': 0.39345240592956543, 'accuracy': 0.8581560254096985, 'recall': 0.8529411554336548, 'val_loss': 0.6412600874900818, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.843137264251709}\n",
            "Saving Model ------------------------------\n",
            "5/5 [==============================] - 2s 559ms/step - loss: 0.3935 - accuracy: 0.8582 - recall: 0.8529 - val_loss: 0.6413 - val_accuracy: 0.6111 - val_recall: 0.8431\n",
            "Epoch 100/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3822 - accuracy: 0.8438 - recall: 0.8000Epoch: 99 Logs: {'loss': 0.39748796820640564, 'accuracy': 0.8226950168609619, 'recall': 0.7384615540504456, 'val_loss': 0.6514006853103638, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.3975 - accuracy: 0.8227 - recall: 0.7385 - val_loss: 0.6514 - val_accuracy: 0.6333 - val_recall: 0.5686\n",
            "Epoch 101/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3578 - accuracy: 0.8652 - recall: 0.8030Epoch: 100 Logs: {'loss': 0.3577878773212433, 'accuracy': 0.8652482032775879, 'recall': 0.8030303120613098, 'val_loss': 0.6462721824645996, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.7254902124404907}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.3578 - accuracy: 0.8652 - recall: 0.8030 - val_loss: 0.6463 - val_accuracy: 0.6222 - val_recall: 0.7255\n",
            "Epoch 102/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.8794 - recall: 0.8657Epoch: 101 Logs: {'loss': 0.33139562606811523, 'accuracy': 0.8794326186180115, 'recall': 0.8656716346740723, 'val_loss': 0.7452553510665894, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.37254902720451355}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.3314 - accuracy: 0.8794 - recall: 0.8657 - val_loss: 0.7453 - val_accuracy: 0.5889 - val_recall: 0.3725\n",
            "Epoch 103/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.7801 - recall: 0.7727Epoch: 102 Logs: {'loss': 0.4289115071296692, 'accuracy': 0.7801418304443359, 'recall': 0.7727272510528564, 'val_loss': 0.6534069776535034, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.7843137383460999}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.4289 - accuracy: 0.7801 - recall: 0.7727 - val_loss: 0.6534 - val_accuracy: 0.6000 - val_recall: 0.7843\n",
            "Epoch 104/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.7943 - recall: 0.6032Epoch: 103 Logs: {'loss': 0.41003456711769104, 'accuracy': 0.7943262457847595, 'recall': 0.60317462682724, 'val_loss': 0.682593047618866, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.7254902124404907}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.4100 - accuracy: 0.7943 - recall: 0.6032 - val_loss: 0.6826 - val_accuracy: 0.5889 - val_recall: 0.7255\n",
            "Epoch 105/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.4075 - accuracy: 0.8375 - recall: 0.9130Epoch: 104 Logs: {'loss': 0.4074825346469879, 'accuracy': 0.8374999761581421, 'recall': 0.9130434989929199, 'val_loss': 0.6829150915145874, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.843137264251709}\n",
            "Val Accuracy More Than Set Thresholds {'loss': 0.4074825346469879, 'accuracy': 0.8374999761581421, 'recall': 0.9130434989929199, 'val_loss': 0.6829150915145874, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.843137264251709}\n",
            "Saving Model ------------------------------\n",
            "5/5 [==============================] - 2s 564ms/step - loss: 0.4075 - accuracy: 0.8375 - recall: 0.9130 - val_loss: 0.6829 - val_accuracy: 0.6111 - val_recall: 0.8431\n",
            "Epoch 106/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8794 - recall: 0.8462Epoch: 105 Logs: {'loss': 0.34780070185661316, 'accuracy': 0.8794326186180115, 'recall': 0.8461538553237915, 'val_loss': 0.6550817489624023, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.7450980544090271}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.3478 - accuracy: 0.8794 - recall: 0.8462 - val_loss: 0.6551 - val_accuracy: 0.6333 - val_recall: 0.7451\n",
            "Epoch 107/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3620 - accuracy: 0.8594 - recall: 0.9062Epoch: 106 Logs: {'loss': 0.3543522357940674, 'accuracy': 0.8652482032775879, 'recall': 0.8999999761581421, 'val_loss': 0.7220479249954224, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.3544 - accuracy: 0.8652 - recall: 0.9000 - val_loss: 0.7220 - val_accuracy: 0.6222 - val_recall: 0.4118\n",
            "Epoch 108/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8250 - recall: 0.7792Epoch: 107 Logs: {'loss': 0.3730802536010742, 'accuracy': 0.824999988079071, 'recall': 0.7792207598686218, 'val_loss': 0.6565417647361755, 'val_accuracy': 0.6555555462837219, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 149ms/step - loss: 0.3731 - accuracy: 0.8250 - recall: 0.7792 - val_loss: 0.6565 - val_accuracy: 0.6556 - val_recall: 0.6078\n",
            "Epoch 109/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3412 - accuracy: 0.8672 - recall: 0.7937Epoch: 108 Logs: {'loss': 0.32850685715675354, 'accuracy': 0.8723404407501221, 'recall': 0.8028169274330139, 'val_loss': 0.7300475835800171, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.9215686321258545}\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.3285 - accuracy: 0.8723 - recall: 0.8028 - val_loss: 0.7300 - val_accuracy: 0.5778 - val_recall: 0.9216\n",
            "Epoch 110/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.8188 - recall: 0.7750Epoch: 109 Logs: {'loss': 0.3710472285747528, 'accuracy': 0.8187500238418579, 'recall': 0.7749999761581421, 'val_loss': 0.6966257691383362, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.3710 - accuracy: 0.8188 - recall: 0.7750 - val_loss: 0.6966 - val_accuracy: 0.6222 - val_recall: 0.4510\n",
            "Epoch 111/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.8813 - recall: 0.9259Epoch: 110 Logs: {'loss': 0.3145982027053833, 'accuracy': 0.8812500238418579, 'recall': 0.9259259104728699, 'val_loss': 0.7038968205451965, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.3146 - accuracy: 0.8813 - recall: 0.9259 - val_loss: 0.7039 - val_accuracy: 0.6111 - val_recall: 0.4510\n",
            "Epoch 112/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.8511 - recall: 0.7231Epoch: 111 Logs: {'loss': 0.32802262902259827, 'accuracy': 0.8510638475418091, 'recall': 0.7230769395828247, 'val_loss': 0.6761224269866943, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.7647058963775635}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.3280 - accuracy: 0.8511 - recall: 0.7231 - val_loss: 0.6761 - val_accuracy: 0.5889 - val_recall: 0.7647\n",
            "Epoch 113/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3160 - accuracy: 0.8984 - recall: 0.9104Epoch: 112 Logs: {'loss': 0.3240633010864258, 'accuracy': 0.8936170339584351, 'recall': 0.9154929518699646, 'val_loss': 0.6734181642532349, 'val_accuracy': 0.6555555462837219, 'val_recall': 0.686274528503418}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.3241 - accuracy: 0.8936 - recall: 0.9155 - val_loss: 0.6734 - val_accuracy: 0.6556 - val_recall: 0.6863\n",
            "Epoch 114/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.8582 - recall: 0.8507Epoch: 113 Logs: {'loss': 0.3237607479095459, 'accuracy': 0.8581560254096985, 'recall': 0.8507462739944458, 'val_loss': 0.713981568813324, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.3238 - accuracy: 0.8582 - recall: 0.8507 - val_loss: 0.7140 - val_accuracy: 0.6333 - val_recall: 0.4706\n",
            "Epoch 115/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8625 - recall: 0.8158Epoch: 114 Logs: {'loss': 0.3339602053165436, 'accuracy': 0.862500011920929, 'recall': 0.8157894611358643, 'val_loss': 0.713293731212616, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.3340 - accuracy: 0.8625 - recall: 0.8158 - val_loss: 0.7133 - val_accuracy: 0.6333 - val_recall: 0.4706\n",
            "Epoch 116/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.8936 - recall: 0.8500Epoch: 115 Logs: {'loss': 0.3026760518550873, 'accuracy': 0.8936170339584351, 'recall': 0.8500000238418579, 'val_loss': 0.678618848323822, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.3027 - accuracy: 0.8936 - recall: 0.8500 - val_loss: 0.6786 - val_accuracy: 0.6333 - val_recall: 0.5686\n",
            "Epoch 117/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3070 - accuracy: 0.8906 - recall: 0.7451Epoch: 116 Logs: {'loss': 0.294110506772995, 'accuracy': 0.9007092118263245, 'recall': 0.7719298005104065, 'val_loss': 0.67545086145401, 'val_accuracy': 0.6555555462837219, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.2941 - accuracy: 0.9007 - recall: 0.7719 - val_loss: 0.6755 - val_accuracy: 0.6556 - val_recall: 0.6078\n",
            "Epoch 118/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.8794 - recall: 0.8871Epoch: 117 Logs: {'loss': 0.29164284467697144, 'accuracy': 0.8794326186180115, 'recall': 0.8870967626571655, 'val_loss': 0.6725505590438843, 'val_accuracy': 0.644444465637207, 'val_recall': 0.7254902124404907}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.2916 - accuracy: 0.8794 - recall: 0.8871 - val_loss: 0.6726 - val_accuracy: 0.6444 - val_recall: 0.7255\n",
            "Epoch 119/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9062 - recall: 0.8718Epoch: 118 Logs: {'loss': 0.2719793915748596, 'accuracy': 0.90625, 'recall': 0.8717948794364929, 'val_loss': 0.7548432946205139, 'val_accuracy': 0.644444465637207, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.2720 - accuracy: 0.9062 - recall: 0.8718 - val_loss: 0.7548 - val_accuracy: 0.6444 - val_recall: 0.4314\n",
            "Epoch 120/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.8865 - recall: 0.8871Epoch: 119 Logs: {'loss': 0.29963552951812744, 'accuracy': 0.8865247964859009, 'recall': 0.8870967626571655, 'val_loss': 0.6923344731330872, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.2996 - accuracy: 0.8865 - recall: 0.8871 - val_loss: 0.6923 - val_accuracy: 0.6222 - val_recall: 0.5294\n",
            "Epoch 121/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.8562 - recall: 0.7895Epoch: 120 Logs: {'loss': 0.3142019212245941, 'accuracy': 0.856249988079071, 'recall': 0.7894737124443054, 'val_loss': 0.6727254390716553, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.7058823704719543}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.3142 - accuracy: 0.8562 - recall: 0.7895 - val_loss: 0.6727 - val_accuracy: 0.6333 - val_recall: 0.7059\n",
            "Epoch 122/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.9125 - recall: 0.8649Epoch: 121 Logs: {'loss': 0.27671074867248535, 'accuracy': 0.9125000238418579, 'recall': 0.8648648858070374, 'val_loss': 0.6786318421363831, 'val_accuracy': 0.6666666865348816, 'val_recall': 0.7058823704719543}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.2767 - accuracy: 0.9125 - recall: 0.8649 - val_loss: 0.6786 - val_accuracy: 0.6667 - val_recall: 0.7059\n",
            "Epoch 123/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.8687 - recall: 0.8169Epoch: 122 Logs: {'loss': 0.2999890446662903, 'accuracy': 0.8687499761581421, 'recall': 0.8169013857841492, 'val_loss': 0.6900984644889832, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.3000 - accuracy: 0.8687 - recall: 0.8169 - val_loss: 0.6901 - val_accuracy: 0.6333 - val_recall: 0.5882\n",
            "Epoch 124/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.8875 - recall: 0.8378Epoch: 123 Logs: {'loss': 0.2560809254646301, 'accuracy': 0.887499988079071, 'recall': 0.837837815284729, 'val_loss': 0.6794710159301758, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.7058823704719543}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.2561 - accuracy: 0.8875 - recall: 0.8378 - val_loss: 0.6795 - val_accuracy: 0.6778 - val_recall: 0.7059\n",
            "Epoch 125/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9250 - recall: 0.8767Epoch: 124 Logs: {'loss': 0.24145035445690155, 'accuracy': 0.925000011920929, 'recall': 0.8767123222351074, 'val_loss': 0.7561132907867432, 'val_accuracy': 0.6555555462837219, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.2415 - accuracy: 0.9250 - recall: 0.8767 - val_loss: 0.7561 - val_accuracy: 0.6556 - val_recall: 0.4510\n",
            "Epoch 126/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.9007 - recall: 0.8630Epoch: 125 Logs: {'loss': 0.25547564029693604, 'accuracy': 0.9007092118263245, 'recall': 0.8630136847496033, 'val_loss': 0.7005760073661804, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.2555 - accuracy: 0.9007 - recall: 0.8630 - val_loss: 0.7006 - val_accuracy: 0.6333 - val_recall: 0.5882\n",
            "Epoch 127/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.9149 - recall: 0.8475Epoch: 126 Logs: {'loss': 0.229180708527565, 'accuracy': 0.914893627166748, 'recall': 0.8474576473236084, 'val_loss': 0.7023088932037354, 'val_accuracy': 0.6555555462837219, 'val_recall': 0.6470588445663452}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.2292 - accuracy: 0.9149 - recall: 0.8475 - val_loss: 0.7023 - val_accuracy: 0.6556 - val_recall: 0.6471\n",
            "Epoch 128/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2558 - accuracy: 0.9219 - recall: 0.9500Epoch: 127 Logs: {'loss': 0.2629275619983673, 'accuracy': 0.9078013896942139, 'recall': 0.9104477763175964, 'val_loss': 0.8870707154273987, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.3921568691730499}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.2629 - accuracy: 0.9078 - recall: 0.9104 - val_loss: 0.8871 - val_accuracy: 0.6222 - val_recall: 0.3922\n",
            "Epoch 129/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9187 - recall: 0.8625Epoch: 128 Logs: {'loss': 0.2532351016998291, 'accuracy': 0.918749988079071, 'recall': 0.862500011920929, 'val_loss': 0.7276077270507812, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.8039215803146362}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.2532 - accuracy: 0.9187 - recall: 0.8625 - val_loss: 0.7276 - val_accuracy: 0.5667 - val_recall: 0.8039\n",
            "Epoch 130/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9062 - recall: 0.9189Epoch: 129 Logs: {'loss': 0.24942147731781006, 'accuracy': 0.90625, 'recall': 0.9189189076423645, 'val_loss': 0.8209378719329834, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.2494 - accuracy: 0.9062 - recall: 0.9189 - val_loss: 0.8209 - val_accuracy: 0.6222 - val_recall: 0.4118\n",
            "Epoch 131/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.8794 - recall: 0.9000Epoch: 130 Logs: {'loss': 0.28986063599586487, 'accuracy': 0.8794326186180115, 'recall': 0.8999999761581421, 'val_loss': 0.7343690395355225, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.7843137383460999}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.2899 - accuracy: 0.8794 - recall: 0.9000 - val_loss: 0.7344 - val_accuracy: 0.5667 - val_recall: 0.7843\n",
            "Epoch 132/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.9149 - recall: 0.8750Epoch: 131 Logs: {'loss': 0.23066692054271698, 'accuracy': 0.914893627166748, 'recall': 0.875, 'val_loss': 0.7630280256271362, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.2307 - accuracy: 0.9149 - recall: 0.8750 - val_loss: 0.7630 - val_accuracy: 0.6111 - val_recall: 0.4902\n",
            "Epoch 133/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9062 - recall: 0.9200Epoch: 132 Logs: {'loss': 0.2421470433473587, 'accuracy': 0.90625, 'recall': 0.9200000166893005, 'val_loss': 0.7159251570701599, 'val_accuracy': 0.6555555462837219, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 149ms/step - loss: 0.2421 - accuracy: 0.9062 - recall: 0.9200 - val_loss: 0.7159 - val_accuracy: 0.6556 - val_recall: 0.6667\n",
            "Epoch 134/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 0.8582 - recall: 0.7465Epoch: 133 Logs: {'loss': 0.30858948826789856, 'accuracy': 0.8581560254096985, 'recall': 0.7464788556098938, 'val_loss': 0.7113106846809387, 'val_accuracy': 0.6666666865348816, 'val_recall': 0.7058823704719543}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.3086 - accuracy: 0.8582 - recall: 0.7465 - val_loss: 0.7113 - val_accuracy: 0.6667 - val_recall: 0.7059\n",
            "Epoch 135/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8875 - recall: 0.8857Epoch: 134 Logs: {'loss': 0.28831416368484497, 'accuracy': 0.887499988079071, 'recall': 0.8857142925262451, 'val_loss': 0.8857250809669495, 'val_accuracy': 0.5111111402511597, 'val_recall': 0.27450981736183167}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.2883 - accuracy: 0.8875 - recall: 0.8857 - val_loss: 0.8857 - val_accuracy: 0.5111 - val_recall: 0.2745\n",
            "Epoch 136/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8865 - recall: 0.8060Epoch: 135 Logs: {'loss': 0.29633936285972595, 'accuracy': 0.8865247964859009, 'recall': 0.8059701323509216, 'val_loss': 0.7325839400291443, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.7647058963775635}\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.2963 - accuracy: 0.8865 - recall: 0.8060 - val_loss: 0.7326 - val_accuracy: 0.5889 - val_recall: 0.7647\n",
            "Epoch 137/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.8813 - recall: 0.9125Epoch: 136 Logs: {'loss': 0.2881198823451996, 'accuracy': 0.8812500238418579, 'recall': 0.9125000238418579, 'val_loss': 0.7504619359970093, 'val_accuracy': 0.644444465637207, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 148ms/step - loss: 0.2881 - accuracy: 0.8813 - recall: 0.9125 - val_loss: 0.7505 - val_accuracy: 0.6444 - val_recall: 0.6667\n",
            "Epoch 138/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9250 - recall: 0.9067Epoch: 137 Logs: {'loss': 0.21945087611675262, 'accuracy': 0.925000011920929, 'recall': 0.9066666960716248, 'val_loss': 0.7295905351638794, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.2195 - accuracy: 0.9250 - recall: 0.9067 - val_loss: 0.7296 - val_accuracy: 0.6222 - val_recall: 0.6078\n",
            "Epoch 139/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9187 - recall: 0.9067Epoch: 138 Logs: {'loss': 0.2221335917711258, 'accuracy': 0.918749988079071, 'recall': 0.9066666960716248, 'val_loss': 0.7674624919891357, 'val_accuracy': 0.644444465637207, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.2221 - accuracy: 0.9187 - recall: 0.9067 - val_loss: 0.7675 - val_accuracy: 0.6444 - val_recall: 0.5098\n",
            "Epoch 140/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9000 - recall: 0.8158Epoch: 139 Logs: {'loss': 0.222163125872612, 'accuracy': 0.8999999761581421, 'recall': 0.8157894611358643, 'val_loss': 0.7456916570663452, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.7450980544090271}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.2222 - accuracy: 0.9000 - recall: 0.8158 - val_loss: 0.7457 - val_accuracy: 0.6111 - val_recall: 0.7451\n",
            "Epoch 141/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.8936 - recall: 0.8485Epoch: 140 Logs: {'loss': 0.2573692798614502, 'accuracy': 0.8936170339584351, 'recall': 0.8484848737716675, 'val_loss': 0.7377210855484009, 'val_accuracy': 0.644444465637207, 'val_recall': 0.686274528503418}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.2574 - accuracy: 0.8936 - recall: 0.8485 - val_loss: 0.7377 - val_accuracy: 0.6444 - val_recall: 0.6863\n",
            "Epoch 142/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.9007 - recall: 0.8226Epoch: 141 Logs: {'loss': 0.25882893800735474, 'accuracy': 0.9007092118263245, 'recall': 0.8225806355476379, 'val_loss': 0.7921702265739441, 'val_accuracy': 0.644444465637207, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.2588 - accuracy: 0.9007 - recall: 0.8226 - val_loss: 0.7922 - val_accuracy: 0.6444 - val_recall: 0.4706\n",
            "Epoch 143/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9500 - recall: 0.9067Epoch: 142 Logs: {'loss': 0.19476976990699768, 'accuracy': 0.949999988079071, 'recall': 0.9066666960716248, 'val_loss': 0.7929737567901611, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1948 - accuracy: 0.9500 - recall: 0.9067 - val_loss: 0.7930 - val_accuracy: 0.6333 - val_recall: 0.4706\n",
            "Epoch 144/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2380 - accuracy: 0.8865 - recall: 0.8983Epoch: 143 Logs: {'loss': 0.23803465068340302, 'accuracy': 0.8865247964859009, 'recall': 0.8983050584793091, 'val_loss': 0.8079400062561035, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.2380 - accuracy: 0.8865 - recall: 0.8983 - val_loss: 0.8079 - val_accuracy: 0.6111 - val_recall: 0.4706\n",
            "Epoch 145/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.8938 - recall: 0.7714Epoch: 144 Logs: {'loss': 0.25542622804641724, 'accuracy': 0.893750011920929, 'recall': 0.7714285850524902, 'val_loss': 0.7744327187538147, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.2554 - accuracy: 0.8938 - recall: 0.7714 - val_loss: 0.7744 - val_accuracy: 0.6222 - val_recall: 0.6078\n",
            "Epoch 146/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.8562 - recall: 0.9091Epoch: 145 Logs: {'loss': 0.26440349221229553, 'accuracy': 0.856249988079071, 'recall': 0.9090909361839294, 'val_loss': 0.829633355140686, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.2644 - accuracy: 0.8562 - recall: 0.9091 - val_loss: 0.8296 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 147/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9187 - recall: 0.8961Epoch: 146 Logs: {'loss': 0.23394954204559326, 'accuracy': 0.918749988079071, 'recall': 0.8961039185523987, 'val_loss': 0.7609329223632812, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.2339 - accuracy: 0.9187 - recall: 0.8961 - val_loss: 0.7609 - val_accuracy: 0.6333 - val_recall: 0.6667\n",
            "Epoch 148/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9250 - recall: 0.9452Epoch: 147 Logs: {'loss': 0.19353517889976501, 'accuracy': 0.925000011920929, 'recall': 0.9452054500579834, 'val_loss': 0.8178847432136536, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1935 - accuracy: 0.9250 - recall: 0.9452 - val_loss: 0.8179 - val_accuracy: 0.6333 - val_recall: 0.4902\n",
            "Epoch 149/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9312 - recall: 0.8750Epoch: 148 Logs: {'loss': 0.18829037249088287, 'accuracy': 0.9312499761581421, 'recall': 0.875, 'val_loss': 0.8110879063606262, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.1883 - accuracy: 0.9312 - recall: 0.8750 - val_loss: 0.8111 - val_accuracy: 0.6222 - val_recall: 0.4902\n",
            "Epoch 150/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9625 - recall: 0.9481Epoch: 149 Logs: {'loss': 0.1420269012451172, 'accuracy': 0.9624999761581421, 'recall': 0.948051929473877, 'val_loss': 0.8777577877044678, 'val_accuracy': 0.644444465637207, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.1420 - accuracy: 0.9625 - recall: 0.9481 - val_loss: 0.8778 - val_accuracy: 0.6444 - val_recall: 0.4510\n",
            "Epoch 151/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9149 - recall: 0.8657Epoch: 150 Logs: {'loss': 0.22659066319465637, 'accuracy': 0.914893627166748, 'recall': 0.8656716346740723, 'val_loss': 0.8139371275901794, 'val_accuracy': 0.644444465637207, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.2266 - accuracy: 0.9149 - recall: 0.8657 - val_loss: 0.8139 - val_accuracy: 0.6444 - val_recall: 0.5098\n",
            "Epoch 152/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9291 - recall: 0.8676Epoch: 151 Logs: {'loss': 0.185679629445076, 'accuracy': 0.9290780425071716, 'recall': 0.8676470518112183, 'val_loss': 0.7964674830436707, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1857 - accuracy: 0.9291 - recall: 0.8676 - val_loss: 0.7965 - val_accuracy: 0.5889 - val_recall: 0.5882\n",
            "Epoch 153/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9187 - recall: 0.9221Epoch: 152 Logs: {'loss': 0.21103346347808838, 'accuracy': 0.918749988079071, 'recall': 0.9220778942108154, 'val_loss': 0.8002777099609375, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.2110 - accuracy: 0.9187 - recall: 0.9221 - val_loss: 0.8003 - val_accuracy: 0.6000 - val_recall: 0.6078\n",
            "Epoch 154/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9362 - recall: 0.8971Epoch: 153 Logs: {'loss': 0.19740504026412964, 'accuracy': 0.936170220375061, 'recall': 0.8970588445663452, 'val_loss': 0.8150604963302612, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1974 - accuracy: 0.9362 - recall: 0.8971 - val_loss: 0.8151 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 155/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9220 - recall: 0.8806Epoch: 154 Logs: {'loss': 0.22081388533115387, 'accuracy': 0.9219858050346375, 'recall': 0.8805969953536987, 'val_loss': 0.9260575771331787, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.2208 - accuracy: 0.9220 - recall: 0.8806 - val_loss: 0.9261 - val_accuracy: 0.6000 - val_recall: 0.4118\n",
            "Epoch 156/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9433 - recall: 0.9355Epoch: 155 Logs: {'loss': 0.16540361940860748, 'accuracy': 0.9432623982429504, 'recall': 0.9354838728904724, 'val_loss': 0.8010062575340271, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 152ms/step - loss: 0.1654 - accuracy: 0.9433 - recall: 0.9355 - val_loss: 0.8010 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 157/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9375 - recall: 0.9067Epoch: 156 Logs: {'loss': 0.18392810225486755, 'accuracy': 0.9375, 'recall': 0.9066666960716248, 'val_loss': 0.8384122848510742, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.1839 - accuracy: 0.9375 - recall: 0.9067 - val_loss: 0.8384 - val_accuracy: 0.6333 - val_recall: 0.5294\n",
            "Epoch 158/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1678 - accuracy: 0.9219 - recall: 0.8644Epoch: 157 Logs: {'loss': 0.18278458714485168, 'accuracy': 0.9078013896942139, 'recall': 0.8484848737716675, 'val_loss': 0.8026956915855408, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.6274510025978088}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1828 - accuracy: 0.9078 - recall: 0.8485 - val_loss: 0.8027 - val_accuracy: 0.6111 - val_recall: 0.6275\n",
            "Epoch 159/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9500 - recall: 0.9467Epoch: 158 Logs: {'loss': 0.18078958988189697, 'accuracy': 0.949999988079071, 'recall': 0.9466666579246521, 'val_loss': 0.851832389831543, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1808 - accuracy: 0.9500 - recall: 0.9467 - val_loss: 0.8518 - val_accuracy: 0.6111 - val_recall: 0.5882\n",
            "Epoch 160/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9312 - recall: 0.8846Epoch: 159 Logs: {'loss': 0.186931774020195, 'accuracy': 0.9312499761581421, 'recall': 0.8846153616905212, 'val_loss': 0.8265555500984192, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.6470588445663452}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.1869 - accuracy: 0.9312 - recall: 0.8846 - val_loss: 0.8266 - val_accuracy: 0.6222 - val_recall: 0.6471\n",
            "Epoch 161/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9250 - recall: 0.9130Epoch: 160 Logs: {'loss': 0.21139323711395264, 'accuracy': 0.925000011920929, 'recall': 0.9130434989929199, 'val_loss': 0.8601768612861633, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.2114 - accuracy: 0.9250 - recall: 0.9130 - val_loss: 0.8602 - val_accuracy: 0.6111 - val_recall: 0.5294\n",
            "Epoch 162/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9291 - recall: 0.9062Epoch: 161 Logs: {'loss': 0.19632402062416077, 'accuracy': 0.9290780425071716, 'recall': 0.90625, 'val_loss': 0.8804166913032532, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1963 - accuracy: 0.9291 - recall: 0.9062 - val_loss: 0.8804 - val_accuracy: 0.6333 - val_recall: 0.4902\n",
            "Epoch 163/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.9078 - recall: 0.8571Epoch: 162 Logs: {'loss': 0.1918521374464035, 'accuracy': 0.9078013896942139, 'recall': 0.8571428656578064, 'val_loss': 0.8354097008705139, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1919 - accuracy: 0.9078 - recall: 0.8571 - val_loss: 0.8354 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 164/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1762 - accuracy: 0.9375 - recall: 0.8966Epoch: 163 Logs: {'loss': 0.1806378811597824, 'accuracy': 0.936170220375061, 'recall': 0.9032257795333862, 'val_loss': 0.8794686198234558, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.1806 - accuracy: 0.9362 - recall: 0.9032 - val_loss: 0.8795 - val_accuracy: 0.6333 - val_recall: 0.5098\n",
            "Epoch 165/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9312 - recall: 0.8974Epoch: 164 Logs: {'loss': 0.19033358991146088, 'accuracy': 0.9312499761581421, 'recall': 0.8974359035491943, 'val_loss': 0.8243675827980042, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.1903 - accuracy: 0.9312 - recall: 0.8974 - val_loss: 0.8244 - val_accuracy: 0.6111 - val_recall: 0.5882\n",
            "Epoch 166/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9375 - recall: 0.9041Epoch: 165 Logs: {'loss': 0.18082809448242188, 'accuracy': 0.9375, 'recall': 0.9041095972061157, 'val_loss': 0.8806150555610657, 'val_accuracy': 0.644444465637207, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1808 - accuracy: 0.9375 - recall: 0.9041 - val_loss: 0.8806 - val_accuracy: 0.6444 - val_recall: 0.5098\n",
            "Epoch 167/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9433 - recall: 0.8871Epoch: 166 Logs: {'loss': 0.1596476137638092, 'accuracy': 0.9432623982429504, 'recall': 0.8870967626571655, 'val_loss': 0.8482969403266907, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.1596 - accuracy: 0.9433 - recall: 0.8871 - val_loss: 0.8483 - val_accuracy: 0.6111 - val_recall: 0.5686\n",
            "Epoch 168/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9078 - recall: 0.8507Epoch: 167 Logs: {'loss': 0.19412405788898468, 'accuracy': 0.9078013896942139, 'recall': 0.8507462739944458, 'val_loss': 0.8402116298675537, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.6274510025978088}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1941 - accuracy: 0.9078 - recall: 0.8507 - val_loss: 0.8402 - val_accuracy: 0.6111 - val_recall: 0.6275\n",
            "Epoch 169/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9312 - recall: 0.9620Epoch: 168 Logs: {'loss': 0.1833341419696808, 'accuracy': 0.9312499761581421, 'recall': 0.9620253443717957, 'val_loss': 0.8703746199607849, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.1833 - accuracy: 0.9312 - recall: 0.9620 - val_loss: 0.8704 - val_accuracy: 0.6000 - val_recall: 0.5882\n",
            "Epoch 170/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9220 - recall: 0.9000Epoch: 169 Logs: {'loss': 0.18105575442314148, 'accuracy': 0.9219858050346375, 'recall': 0.8999999761581421, 'val_loss': 0.8824516534805298, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1811 - accuracy: 0.9220 - recall: 0.9000 - val_loss: 0.8825 - val_accuracy: 0.6000 - val_recall: 0.5490\n",
            "Epoch 171/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1603 - accuracy: 0.9219 - recall: 0.9143Epoch: 170 Logs: {'loss': 0.16867491602897644, 'accuracy': 0.9219858050346375, 'recall': 0.9054054021835327, 'val_loss': 0.9043385982513428, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.1687 - accuracy: 0.9220 - recall: 0.9054 - val_loss: 0.9043 - val_accuracy: 0.6222 - val_recall: 0.4902\n",
            "Epoch 172/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9312 - recall: 0.8750Epoch: 171 Logs: {'loss': 0.16596290469169617, 'accuracy': 0.9312499761581421, 'recall': 0.875, 'val_loss': 0.8666120767593384, 'val_accuracy': 0.644444465637207, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1660 - accuracy: 0.9312 - recall: 0.8750 - val_loss: 0.8666 - val_accuracy: 0.6444 - val_recall: 0.6078\n",
            "Epoch 173/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9500 - recall: 0.9367Epoch: 172 Logs: {'loss': 0.15148964524269104, 'accuracy': 0.949999988079071, 'recall': 0.9367088675498962, 'val_loss': 0.921015739440918, 'val_accuracy': 0.644444465637207, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1515 - accuracy: 0.9500 - recall: 0.9367 - val_loss: 0.9210 - val_accuracy: 0.6444 - val_recall: 0.5294\n",
            "Epoch 174/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1998 - accuracy: 0.9219 - recall: 0.9048Epoch: 173 Logs: {'loss': 0.19366902112960815, 'accuracy': 0.9290780425071716, 'recall': 0.9104477763175964, 'val_loss': 0.8762223720550537, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.1937 - accuracy: 0.9291 - recall: 0.9104 - val_loss: 0.8762 - val_accuracy: 0.5889 - val_recall: 0.5882\n",
            "Epoch 175/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1466 - accuracy: 0.9609 - recall: 0.9492Epoch: 174 Logs: {'loss': 0.1396571695804596, 'accuracy': 0.9645389914512634, 'recall': 0.9538461565971375, 'val_loss': 0.933386504650116, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1397 - accuracy: 0.9645 - recall: 0.9538 - val_loss: 0.9334 - val_accuracy: 0.6333 - val_recall: 0.5098\n",
            "Epoch 176/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9433 - recall: 0.9219Epoch: 175 Logs: {'loss': 0.16241885721683502, 'accuracy': 0.9432623982429504, 'recall': 0.921875, 'val_loss': 0.9134345054626465, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1624 - accuracy: 0.9433 - recall: 0.9219 - val_loss: 0.9134 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 177/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9375 - recall: 0.8800Epoch: 176 Logs: {'loss': 0.14528557658195496, 'accuracy': 0.9375, 'recall': 0.8799999952316284, 'val_loss': 0.8879454135894775, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1453 - accuracy: 0.9375 - recall: 0.8800 - val_loss: 0.8879 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 178/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9250 - recall: 0.8831Epoch: 177 Logs: {'loss': 0.18363726139068604, 'accuracy': 0.925000011920929, 'recall': 0.8831169009208679, 'val_loss': 0.9263638257980347, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.1836 - accuracy: 0.9250 - recall: 0.8831 - val_loss: 0.9264 - val_accuracy: 0.6333 - val_recall: 0.5490\n",
            "Epoch 179/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9563 - recall: 0.9474Epoch: 178 Logs: {'loss': 0.1542121022939682, 'accuracy': 0.956250011920929, 'recall': 0.9473684430122375, 'val_loss': 0.873888373374939, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.6470588445663452}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1542 - accuracy: 0.9563 - recall: 0.9474 - val_loss: 0.8739 - val_accuracy: 0.6000 - val_recall: 0.6471\n",
            "Epoch 180/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9574 - recall: 0.9474Epoch: 179 Logs: {'loss': 0.16714546084403992, 'accuracy': 0.957446813583374, 'recall': 0.9473684430122375, 'val_loss': 1.001118779182434, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1671 - accuracy: 0.9574 - recall: 0.9474 - val_loss: 1.0011 - val_accuracy: 0.5889 - val_recall: 0.4510\n",
            "Epoch 181/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9250 - recall: 0.8961Epoch: 180 Logs: {'loss': 0.21251413226127625, 'accuracy': 0.925000011920929, 'recall': 0.8961039185523987, 'val_loss': 0.9079768061637878, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.2125 - accuracy: 0.9250 - recall: 0.8961 - val_loss: 0.9080 - val_accuracy: 0.5889 - val_recall: 0.5686\n",
            "Epoch 182/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9125 - recall: 0.8333Epoch: 181 Logs: {'loss': 0.22837838530540466, 'accuracy': 0.9125000238418579, 'recall': 0.8333333134651184, 'val_loss': 1.0132085084915161, 'val_accuracy': 0.47777777910232544, 'val_recall': 0.3921568691730499}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.2284 - accuracy: 0.9125 - recall: 0.8333 - val_loss: 1.0132 - val_accuracy: 0.4778 - val_recall: 0.3922\n",
            "Epoch 183/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1888 - accuracy: 0.9433 - recall: 0.9538Epoch: 182 Logs: {'loss': 0.188791885972023, 'accuracy': 0.9432623982429504, 'recall': 0.9538461565971375, 'val_loss': 0.8807740807533264, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.686274528503418}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1888 - accuracy: 0.9433 - recall: 0.9538 - val_loss: 0.8808 - val_accuracy: 0.5778 - val_recall: 0.6863\n",
            "Epoch 184/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9375 - recall: 0.9211Epoch: 183 Logs: {'loss': 0.1615140438079834, 'accuracy': 0.9375, 'recall': 0.9210526347160339, 'val_loss': 1.04887056350708, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1615 - accuracy: 0.9375 - recall: 0.9211 - val_loss: 1.0489 - val_accuracy: 0.5889 - val_recall: 0.4118\n",
            "Epoch 185/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9312 - recall: 0.9146Epoch: 184 Logs: {'loss': 0.17482292652130127, 'accuracy': 0.9312499761581421, 'recall': 0.9146341681480408, 'val_loss': 0.9227062463760376, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1748 - accuracy: 0.9312 - recall: 0.9146 - val_loss: 0.9227 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 186/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9574 - recall: 0.9167Epoch: 185 Logs: {'loss': 0.14640827476978302, 'accuracy': 0.957446813583374, 'recall': 0.9166666865348816, 'val_loss': 0.9921379685401917, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1464 - accuracy: 0.9574 - recall: 0.9167 - val_loss: 0.9921 - val_accuracy: 0.6000 - val_recall: 0.4902\n",
            "Epoch 187/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9716 - recall: 0.9844Epoch: 186 Logs: {'loss': 0.13079366087913513, 'accuracy': 0.9716312289237976, 'recall': 0.984375, 'val_loss': 0.9158060550689697, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1308 - accuracy: 0.9716 - recall: 0.9844 - val_loss: 0.9158 - val_accuracy: 0.6000 - val_recall: 0.5882\n",
            "Epoch 188/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1616 - accuracy: 0.9297 - recall: 0.8966Epoch: 187 Logs: {'loss': 0.16007736325263977, 'accuracy': 0.9290780425071716, 'recall': 0.8870967626571655, 'val_loss': 1.0668368339538574, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.1601 - accuracy: 0.9291 - recall: 0.8871 - val_loss: 1.0668 - val_accuracy: 0.6111 - val_recall: 0.4510\n",
            "Epoch 189/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9433 - recall: 0.9048Epoch: 188 Logs: {'loss': 0.14880938827991486, 'accuracy': 0.9432623982429504, 'recall': 0.9047619104385376, 'val_loss': 0.9499692916870117, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.1488 - accuracy: 0.9433 - recall: 0.9048 - val_loss: 0.9500 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 190/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9716 - recall: 0.9577Epoch: 189 Logs: {'loss': 0.1369432806968689, 'accuracy': 0.9716312289237976, 'recall': 0.9577465057373047, 'val_loss': 0.966155469417572, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1369 - accuracy: 0.9716 - recall: 0.9577 - val_loss: 0.9662 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 191/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9716 - recall: 0.9531Epoch: 190 Logs: {'loss': 0.12017470598220825, 'accuracy': 0.9716312289237976, 'recall': 0.953125, 'val_loss': 1.0080149173736572, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.1202 - accuracy: 0.9716 - recall: 0.9531 - val_loss: 1.0080 - val_accuracy: 0.6333 - val_recall: 0.5294\n",
            "Epoch 192/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.9438 - recall: 0.9275Epoch: 191 Logs: {'loss': 0.13737010955810547, 'accuracy': 0.9437500238418579, 'recall': 0.9275362491607666, 'val_loss': 0.9779289960861206, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1374 - accuracy: 0.9438 - recall: 0.9275 - val_loss: 0.9779 - val_accuracy: 0.6000 - val_recall: 0.5490\n",
            "Epoch 193/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9625 - recall: 0.9444Epoch: 192 Logs: {'loss': 0.13060344755649567, 'accuracy': 0.9624999761581421, 'recall': 0.9444444179534912, 'val_loss': 0.9928518533706665, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.1306 - accuracy: 0.9625 - recall: 0.9444 - val_loss: 0.9929 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 194/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9716 - recall: 0.9508Epoch: 193 Logs: {'loss': 0.11569719761610031, 'accuracy': 0.9716312289237976, 'recall': 0.9508196711540222, 'val_loss': 0.9941987991333008, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 155ms/step - loss: 0.1157 - accuracy: 0.9716 - recall: 0.9508 - val_loss: 0.9942 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 195/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1231 - accuracy: 0.9453 - recall: 0.9649Epoch: 194 Logs: {'loss': 0.11696221679449081, 'accuracy': 0.9503546357154846, 'recall': 0.9696969985961914, 'val_loss': 0.9862099289894104, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.1170 - accuracy: 0.9504 - recall: 0.9697 - val_loss: 0.9862 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 196/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1254 - accuracy: 0.9531 - recall: 0.9508Epoch: 195 Logs: {'loss': 0.11905234307050705, 'accuracy': 0.957446813583374, 'recall': 0.95652174949646, 'val_loss': 1.002437710762024, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.1191 - accuracy: 0.9574 - recall: 0.9565 - val_loss: 1.0024 - val_accuracy: 0.6333 - val_recall: 0.5490\n",
            "Epoch 197/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9645 - recall: 0.9394Epoch: 196 Logs: {'loss': 0.11455473303794861, 'accuracy': 0.9645389914512634, 'recall': 0.939393937587738, 'val_loss': 1.0381121635437012, 'val_accuracy': 0.644444465637207, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1146 - accuracy: 0.9645 - recall: 0.9394 - val_loss: 1.0381 - val_accuracy: 0.6444 - val_recall: 0.5294\n",
            "Epoch 198/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9362 - recall: 0.9412Epoch: 197 Logs: {'loss': 0.14170701801776886, 'accuracy': 0.936170220375061, 'recall': 0.9411764740943909, 'val_loss': 0.9640175104141235, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.1417 - accuracy: 0.9362 - recall: 0.9412 - val_loss: 0.9640 - val_accuracy: 0.6000 - val_recall: 0.5882\n",
            "Epoch 199/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9716 - recall: 0.9683Epoch: 198 Logs: {'loss': 0.10106852650642395, 'accuracy': 0.9716312289237976, 'recall': 0.9682539701461792, 'val_loss': 1.047942042350769, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1011 - accuracy: 0.9716 - recall: 0.9683 - val_loss: 1.0479 - val_accuracy: 0.5889 - val_recall: 0.5098\n",
            "Epoch 200/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9500 - recall: 0.9467Epoch: 199 Logs: {'loss': 0.12478574365377426, 'accuracy': 0.949999988079071, 'recall': 0.9466666579246521, 'val_loss': 0.9906228184700012, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.1248 - accuracy: 0.9500 - recall: 0.9467 - val_loss: 0.9906 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 201/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9574 - recall: 0.9429Epoch: 200 Logs: {'loss': 0.1349678784608841, 'accuracy': 0.957446813583374, 'recall': 0.9428571462631226, 'val_loss': 1.007844090461731, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1350 - accuracy: 0.9574 - recall: 0.9429 - val_loss: 1.0078 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 202/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9504 - recall: 0.9254Epoch: 201 Logs: {'loss': 0.1379815638065338, 'accuracy': 0.9503546357154846, 'recall': 0.9253731369972229, 'val_loss': 1.0378851890563965, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.1380 - accuracy: 0.9504 - recall: 0.9254 - val_loss: 1.0379 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 203/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9645 - recall: 0.9672Epoch: 202 Logs: {'loss': 0.11474019289016724, 'accuracy': 0.9645389914512634, 'recall': 0.9672130942344666, 'val_loss': 1.0401872396469116, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1147 - accuracy: 0.9645 - recall: 0.9672 - val_loss: 1.0402 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 204/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9688 - recall: 0.9459Epoch: 203 Logs: {'loss': 0.11195269972085953, 'accuracy': 0.96875, 'recall': 0.9459459185600281, 'val_loss': 1.0269160270690918, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1120 - accuracy: 0.9688 - recall: 0.9459 - val_loss: 1.0269 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 205/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9574 - recall: 0.9531Epoch: 204 Logs: {'loss': 0.10727578401565552, 'accuracy': 0.957446813583374, 'recall': 0.953125, 'val_loss': 1.0442287921905518, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.1073 - accuracy: 0.9574 - recall: 0.9531 - val_loss: 1.0442 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 206/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1243 - accuracy: 0.9609 - recall: 0.9365Epoch: 205 Logs: {'loss': 0.11631602793931961, 'accuracy': 0.9645389914512634, 'recall': 0.9420289993286133, 'val_loss': 1.0446081161499023, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1163 - accuracy: 0.9645 - recall: 0.9420 - val_loss: 1.0446 - val_accuracy: 0.6000 - val_recall: 0.5490\n",
            "Epoch 207/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9645 - recall: 0.9524Epoch: 206 Logs: {'loss': 0.12950237095355988, 'accuracy': 0.9645389914512634, 'recall': 0.9523809552192688, 'val_loss': 1.0045133829116821, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1295 - accuracy: 0.9645 - recall: 0.9524 - val_loss: 1.0045 - val_accuracy: 0.6000 - val_recall: 0.6078\n",
            "Epoch 208/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9574 - recall: 0.9545Epoch: 207 Logs: {'loss': 0.12113595753908157, 'accuracy': 0.957446813583374, 'recall': 0.9545454382896423, 'val_loss': 1.052844762802124, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1211 - accuracy: 0.9574 - recall: 0.9545 - val_loss: 1.0528 - val_accuracy: 0.6000 - val_recall: 0.5490\n",
            "Epoch 209/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9688 - recall: 0.9429Epoch: 208 Logs: {'loss': 0.10188794136047363, 'accuracy': 0.96875, 'recall': 0.9428571462631226, 'val_loss': 1.0472450256347656, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 148ms/step - loss: 0.1019 - accuracy: 0.9688 - recall: 0.9429 - val_loss: 1.0472 - val_accuracy: 0.6222 - val_recall: 0.5686\n",
            "Epoch 210/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9625 - recall: 0.9452Epoch: 209 Logs: {'loss': 0.11832194030284882, 'accuracy': 0.9624999761581421, 'recall': 0.9452054500579834, 'val_loss': 1.0819300413131714, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1183 - accuracy: 0.9625 - recall: 0.9452 - val_loss: 1.0819 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 211/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1390 - accuracy: 0.9531 - recall: 0.9298Epoch: 210 Logs: {'loss': 0.1293741911649704, 'accuracy': 0.957446813583374, 'recall': 0.9375, 'val_loss': 1.0883336067199707, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.1294 - accuracy: 0.9574 - recall: 0.9375 - val_loss: 1.0883 - val_accuracy: 0.6222 - val_recall: 0.5294\n",
            "Epoch 212/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1373 - accuracy: 0.9453 - recall: 0.8889Epoch: 211 Logs: {'loss': 0.13886871933937073, 'accuracy': 0.9503546357154846, 'recall': 0.89552241563797, 'val_loss': 1.0188632011413574, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.1389 - accuracy: 0.9504 - recall: 0.8955 - val_loss: 1.0189 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 213/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9716 - recall: 0.9538Epoch: 212 Logs: {'loss': 0.12050408124923706, 'accuracy': 0.9716312289237976, 'recall': 0.9538461565971375, 'val_loss': 1.1111760139465332, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.1205 - accuracy: 0.9716 - recall: 0.9538 - val_loss: 1.1112 - val_accuracy: 0.6111 - val_recall: 0.5294\n",
            "Epoch 214/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9574 - recall: 0.9706Epoch: 213 Logs: {'loss': 0.12009847909212112, 'accuracy': 0.957446813583374, 'recall': 0.970588207244873, 'val_loss': 1.0350677967071533, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1201 - accuracy: 0.9574 - recall: 0.9706 - val_loss: 1.0351 - val_accuracy: 0.6000 - val_recall: 0.5882\n",
            "Epoch 215/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9716 - recall: 0.9394Epoch: 214 Logs: {'loss': 0.10038900375366211, 'accuracy': 0.9716312289237976, 'recall': 0.939393937587738, 'val_loss': 1.1087712049484253, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1004 - accuracy: 0.9716 - recall: 0.9394 - val_loss: 1.1088 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 216/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1057 - accuracy: 0.9531 - recall: 0.9344Epoch: 215 Logs: {'loss': 0.09835405647754669, 'accuracy': 0.957446813583374, 'recall': 0.9411764740943909, 'val_loss': 1.0640472173690796, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.0984 - accuracy: 0.9574 - recall: 0.9412 - val_loss: 1.0640 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 217/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9625 - recall: 0.9459Epoch: 216 Logs: {'loss': 0.09896846860647202, 'accuracy': 0.9624999761581421, 'recall': 0.9459459185600281, 'val_loss': 1.0744645595550537, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0990 - accuracy: 0.9625 - recall: 0.9459 - val_loss: 1.0745 - val_accuracy: 0.6111 - val_recall: 0.5686\n",
            "Epoch 218/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1160 - accuracy: 0.9453 - recall: 0.9385Epoch: 217 Logs: {'loss': 0.10746480524539948, 'accuracy': 0.9503546357154846, 'recall': 0.9402984976768494, 'val_loss': 1.1171374320983887, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.1075 - accuracy: 0.9504 - recall: 0.9403 - val_loss: 1.1171 - val_accuracy: 0.6333 - val_recall: 0.5490\n",
            "Epoch 219/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9787 - recall: 0.9531Epoch: 218 Logs: {'loss': 0.0890190377831459, 'accuracy': 0.978723406791687, 'recall': 0.953125, 'val_loss': 1.1160757541656494, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0890 - accuracy: 0.9787 - recall: 0.9531 - val_loss: 1.1161 - val_accuracy: 0.6222 - val_recall: 0.5490\n",
            "Epoch 220/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9625 - recall: 0.9487Epoch: 219 Logs: {'loss': 0.10648264735937119, 'accuracy': 0.9624999761581421, 'recall': 0.9487179517745972, 'val_loss': 1.0772119760513306, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.1065 - accuracy: 0.9625 - recall: 0.9487 - val_loss: 1.0772 - val_accuracy: 0.6222 - val_recall: 0.5686\n",
            "Epoch 221/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9625 - recall: 0.9630Epoch: 220 Logs: {'loss': 0.1038191094994545, 'accuracy': 0.9624999761581421, 'recall': 0.9629629850387573, 'val_loss': 1.0821540355682373, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 149ms/step - loss: 0.1038 - accuracy: 0.9625 - recall: 0.9630 - val_loss: 1.0822 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 222/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9563 - recall: 0.9333Epoch: 221 Logs: {'loss': 0.1149311289191246, 'accuracy': 0.956250011920929, 'recall': 0.9333333373069763, 'val_loss': 1.1132724285125732, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.1149 - accuracy: 0.9563 - recall: 0.9333 - val_loss: 1.1133 - val_accuracy: 0.5889 - val_recall: 0.5294\n",
            "Epoch 223/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9688 - recall: 0.9737Epoch: 222 Logs: {'loss': 0.0986875668168068, 'accuracy': 0.96875, 'recall': 0.9736841917037964, 'val_loss': 1.0804558992385864, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0987 - accuracy: 0.9688 - recall: 0.9737 - val_loss: 1.0805 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 224/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9716 - recall: 0.9524Epoch: 223 Logs: {'loss': 0.09338512271642685, 'accuracy': 0.9716312289237976, 'recall': 0.9523809552192688, 'val_loss': 1.111453652381897, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.0934 - accuracy: 0.9716 - recall: 0.9524 - val_loss: 1.1115 - val_accuracy: 0.6111 - val_recall: 0.5686\n",
            "Epoch 225/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9563 - recall: 0.9315Epoch: 224 Logs: {'loss': 0.11460225284099579, 'accuracy': 0.956250011920929, 'recall': 0.931506872177124, 'val_loss': 1.1457587480545044, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1146 - accuracy: 0.9563 - recall: 0.9315 - val_loss: 1.1458 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 226/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9645 - recall: 0.9583Epoch: 225 Logs: {'loss': 0.09543921798467636, 'accuracy': 0.9645389914512634, 'recall': 0.9583333134651184, 'val_loss': 1.1191946268081665, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0954 - accuracy: 0.9645 - recall: 0.9583 - val_loss: 1.1192 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 227/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1043 - accuracy: 0.9688 - recall: 0.9661Epoch: 226 Logs: {'loss': 0.09824305027723312, 'accuracy': 0.9716312289237976, 'recall': 0.9696969985961914, 'val_loss': 1.1524312496185303, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.0982 - accuracy: 0.9716 - recall: 0.9697 - val_loss: 1.1524 - val_accuracy: 0.6111 - val_recall: 0.5294\n",
            "Epoch 228/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9688 - recall: 0.9487Epoch: 227 Logs: {'loss': 0.09284158796072006, 'accuracy': 0.96875, 'recall': 0.9487179517745972, 'val_loss': 1.129960298538208, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0928 - accuracy: 0.9688 - recall: 0.9487 - val_loss: 1.1300 - val_accuracy: 0.5889 - val_recall: 0.5294\n",
            "Epoch 229/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9625 - recall: 0.9600Epoch: 228 Logs: {'loss': 0.102854885160923, 'accuracy': 0.9624999761581421, 'recall': 0.9599999785423279, 'val_loss': 1.121180772781372, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.1029 - accuracy: 0.9625 - recall: 0.9600 - val_loss: 1.1212 - val_accuracy: 0.6000 - val_recall: 0.5686\n",
            "Epoch 230/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9574 - recall: 0.9538Epoch: 229 Logs: {'loss': 0.10645759850740433, 'accuracy': 0.957446813583374, 'recall': 0.9538461565971375, 'val_loss': 1.154051423072815, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.1065 - accuracy: 0.9574 - recall: 0.9538 - val_loss: 1.1541 - val_accuracy: 0.6000 - val_recall: 0.5098\n",
            "Epoch 231/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9504 - recall: 0.9531Epoch: 230 Logs: {'loss': 0.1255933791399002, 'accuracy': 0.9503546357154846, 'recall': 0.953125, 'val_loss': 1.5093681812286377, 'val_accuracy': 0.4555555582046509, 'val_recall': 0.1764705926179886}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1256 - accuracy: 0.9504 - recall: 0.9531 - val_loss: 1.5094 - val_accuracy: 0.4556 - val_recall: 0.1765\n",
            "Epoch 232/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9433 - recall: 0.8923Epoch: 231 Logs: {'loss': 0.15705381333827972, 'accuracy': 0.9432623982429504, 'recall': 0.892307698726654, 'val_loss': 1.1043531894683838, 'val_accuracy': 0.5111111402511597, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.1571 - accuracy: 0.9433 - recall: 0.8923 - val_loss: 1.1044 - val_accuracy: 0.5111 - val_recall: 0.6667\n",
            "Epoch 233/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1387 - accuracy: 0.9453 - recall: 1.0000Epoch: 232 Logs: {'loss': 0.1354989856481552, 'accuracy': 0.9432623982429504, 'recall': 0.9861111044883728, 'val_loss': 1.1423918008804321, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.1355 - accuracy: 0.9433 - recall: 0.9861 - val_loss: 1.1424 - val_accuracy: 0.6000 - val_recall: 0.5490\n",
            "Epoch 234/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9750 - recall: 0.9740Epoch: 233 Logs: {'loss': 0.07748313248157501, 'accuracy': 0.9750000238418579, 'recall': 0.9740259647369385, 'val_loss': 1.2106685638427734, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0775 - accuracy: 0.9750 - recall: 0.9740 - val_loss: 1.2107 - val_accuracy: 0.6000 - val_recall: 0.5098\n",
            "Epoch 235/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9750 - recall: 0.9615Epoch: 234 Logs: {'loss': 0.07468738406896591, 'accuracy': 0.9750000238418579, 'recall': 0.9615384340286255, 'val_loss': 1.1653268337249756, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0747 - accuracy: 0.9750 - recall: 0.9615 - val_loss: 1.1653 - val_accuracy: 0.6111 - val_recall: 0.5882\n",
            "Epoch 236/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9433 - recall: 0.9180Epoch: 235 Logs: {'loss': 0.1318604201078415, 'accuracy': 0.9432623982429504, 'recall': 0.9180327653884888, 'val_loss': 1.521909475326538, 'val_accuracy': 0.5222222208976746, 'val_recall': 0.23529411852359772}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.1319 - accuracy: 0.9433 - recall: 0.9180 - val_loss: 1.5219 - val_accuracy: 0.5222 - val_recall: 0.2353\n",
            "Epoch 237/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9574 - recall: 0.9259Epoch: 236 Logs: {'loss': 0.11150223761796951, 'accuracy': 0.957446813583374, 'recall': 0.9259259104728699, 'val_loss': 1.1167129278182983, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.1115 - accuracy: 0.9574 - recall: 0.9259 - val_loss: 1.1167 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 238/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0947 - accuracy: 0.9688 - recall: 0.9667Epoch: 237 Logs: {'loss': 0.09203445166349411, 'accuracy': 0.9716312289237976, 'recall': 0.96875, 'val_loss': 1.1572660207748413, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.0920 - accuracy: 0.9716 - recall: 0.9688 - val_loss: 1.1573 - val_accuracy: 0.5889 - val_recall: 0.5098\n",
            "Epoch 239/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9645 - recall: 0.9344Epoch: 238 Logs: {'loss': 0.08795569092035294, 'accuracy': 0.9645389914512634, 'recall': 0.9344262480735779, 'val_loss': 1.1786161661148071, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0880 - accuracy: 0.9645 - recall: 0.9344 - val_loss: 1.1786 - val_accuracy: 0.6000 - val_recall: 0.5098\n",
            "Epoch 240/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9750 - recall: 0.9571Epoch: 239 Logs: {'loss': 0.08947507292032242, 'accuracy': 0.9750000238418579, 'recall': 0.9571428298950195, 'val_loss': 1.2830582857131958, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0895 - accuracy: 0.9750 - recall: 0.9571 - val_loss: 1.2831 - val_accuracy: 0.5889 - val_recall: 0.4902\n",
            "Epoch 241/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9750 - recall: 0.9467Epoch: 240 Logs: {'loss': 0.08042776584625244, 'accuracy': 0.9750000238418579, 'recall': 0.9466666579246521, 'val_loss': 1.2152811288833618, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0804 - accuracy: 0.9750 - recall: 0.9467 - val_loss: 1.2153 - val_accuracy: 0.6111 - val_recall: 0.5294\n",
            "Epoch 242/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9812 - recall: 0.9875Epoch: 241 Logs: {'loss': 0.07771657407283783, 'accuracy': 0.981249988079071, 'recall': 0.987500011920929, 'val_loss': 1.2709455490112305, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0777 - accuracy: 0.9812 - recall: 0.9875 - val_loss: 1.2709 - val_accuracy: 0.5333 - val_recall: 0.4314\n",
            "Epoch 243/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9787 - recall: 0.9697Epoch: 242 Logs: {'loss': 0.07662330567836761, 'accuracy': 0.978723406791687, 'recall': 0.9696969985961914, 'val_loss': 1.1689900159835815, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0766 - accuracy: 0.9787 - recall: 0.9697 - val_loss: 1.1690 - val_accuracy: 0.5778 - val_recall: 0.5686\n",
            "Epoch 244/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9787 - recall: 1.0000Epoch: 243 Logs: {'loss': 0.0865270122885704, 'accuracy': 0.978723406791687, 'recall': 1.0, 'val_loss': 1.190663456916809, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0865 - accuracy: 0.9787 - recall: 1.0000 - val_loss: 1.1907 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 245/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0817 - accuracy: 0.9688 - recall: 0.9623Epoch: 244 Logs: {'loss': 0.09434688836336136, 'accuracy': 0.957446813583374, 'recall': 0.931034505367279, 'val_loss': 1.3375015258789062, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.0943 - accuracy: 0.9574 - recall: 0.9310 - val_loss: 1.3375 - val_accuracy: 0.6000 - val_recall: 0.4902\n",
            "Epoch 246/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9688 - recall: 0.9714Epoch: 245 Logs: {'loss': 0.09112431108951569, 'accuracy': 0.96875, 'recall': 0.9714285731315613, 'val_loss': 1.2373418807983398, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 148ms/step - loss: 0.0911 - accuracy: 0.9688 - recall: 0.9714 - val_loss: 1.2373 - val_accuracy: 0.6111 - val_recall: 0.5294\n",
            "Epoch 247/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9787 - recall: 0.9577Epoch: 246 Logs: {'loss': 0.08769635856151581, 'accuracy': 0.978723406791687, 'recall': 0.9577465057373047, 'val_loss': 1.3036315441131592, 'val_accuracy': 0.4888888895511627, 'val_recall': 0.37254902720451355}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.0877 - accuracy: 0.9787 - recall: 0.9577 - val_loss: 1.3036 - val_accuracy: 0.4889 - val_recall: 0.3725\n",
            "Epoch 248/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9688 - recall: 0.9571Epoch: 247 Logs: {'loss': 0.09448102116584778, 'accuracy': 0.96875, 'recall': 0.9571428298950195, 'val_loss': 1.1985177993774414, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0945 - accuracy: 0.9688 - recall: 0.9571 - val_loss: 1.1985 - val_accuracy: 0.5778 - val_recall: 0.5686\n",
            "Epoch 249/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9716 - recall: 0.9865Epoch: 248 Logs: {'loss': 0.07816669344902039, 'accuracy': 0.9716312289237976, 'recall': 0.9864864945411682, 'val_loss': 1.2651015520095825, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0782 - accuracy: 0.9716 - recall: 0.9865 - val_loss: 1.2651 - val_accuracy: 0.6000 - val_recall: 0.5490\n",
            "Epoch 250/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9787 - recall: 0.9559Epoch: 249 Logs: {'loss': 0.06756177544593811, 'accuracy': 0.978723406791687, 'recall': 0.9558823704719543, 'val_loss': 1.241881251335144, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5686274766921997}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0676 - accuracy: 0.9787 - recall: 0.9559 - val_loss: 1.2419 - val_accuracy: 0.5889 - val_recall: 0.5686\n",
            "Epoch 251/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9688 - recall: 0.9867Epoch: 250 Logs: {'loss': 0.08616816997528076, 'accuracy': 0.96875, 'recall': 0.9866666793823242, 'val_loss': 1.228878140449524, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0862 - accuracy: 0.9688 - recall: 0.9867 - val_loss: 1.2289 - val_accuracy: 0.5667 - val_recall: 0.5882\n",
            "Epoch 252/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9812 - recall: 0.9867Epoch: 251 Logs: {'loss': 0.0790562778711319, 'accuracy': 0.981249988079071, 'recall': 0.9866666793823242, 'val_loss': 1.2877260446548462, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0791 - accuracy: 0.9812 - recall: 0.9867 - val_loss: 1.2877 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 253/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0761 - accuracy: 0.9766 - recall: 0.9500Epoch: 252 Logs: {'loss': 0.0762292817234993, 'accuracy': 0.978723406791687, 'recall': 0.9552238583564758, 'val_loss': 1.2709400653839111, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.0762 - accuracy: 0.9787 - recall: 0.9552 - val_loss: 1.2709 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 254/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9858 - recall: 1.0000Epoch: 253 Logs: {'loss': 0.06543377041816711, 'accuracy': 0.9858155846595764, 'recall': 1.0, 'val_loss': 1.2793372869491577, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.0654 - accuracy: 0.9858 - recall: 1.0000 - val_loss: 1.2793 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 255/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9937 - recall: 0.9870Epoch: 254 Logs: {'loss': 0.05843344330787659, 'accuracy': 0.9937499761581421, 'recall': 0.9870129823684692, 'val_loss': 1.3127763271331787, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0584 - accuracy: 0.9937 - recall: 0.9870 - val_loss: 1.3128 - val_accuracy: 0.5889 - val_recall: 0.4902\n",
            "Epoch 256/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9787 - recall: 0.9552Epoch: 255 Logs: {'loss': 0.0730433389544487, 'accuracy': 0.978723406791687, 'recall': 0.9552238583564758, 'val_loss': 1.2330282926559448, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0730 - accuracy: 0.9787 - recall: 0.9552 - val_loss: 1.2330 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 257/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9787 - recall: 1.0000Epoch: 256 Logs: {'loss': 0.06326553970575333, 'accuracy': 0.978723406791687, 'recall': 1.0, 'val_loss': 1.3364245891571045, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.0633 - accuracy: 0.9787 - recall: 1.0000 - val_loss: 1.3364 - val_accuracy: 0.6000 - val_recall: 0.5098\n",
            "Epoch 258/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9716 - recall: 0.9385Epoch: 257 Logs: {'loss': 0.07740002125501633, 'accuracy': 0.9716312289237976, 'recall': 0.9384615421295166, 'val_loss': 1.3456565141677856, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0774 - accuracy: 0.9716 - recall: 0.9385 - val_loss: 1.3457 - val_accuracy: 0.5889 - val_recall: 0.4902\n",
            "Epoch 259/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9875 - recall: 0.9722Epoch: 258 Logs: {'loss': 0.06399602442979813, 'accuracy': 0.987500011920929, 'recall': 0.9722222089767456, 'val_loss': 1.3053816556930542, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0640 - accuracy: 0.9875 - recall: 0.9722 - val_loss: 1.3054 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 260/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0640 - accuracy: 0.9844 - recall: 0.9836Epoch: 259 Logs: {'loss': 0.06444013863801956, 'accuracy': 0.978723406791687, 'recall': 0.9846153855323792, 'val_loss': 1.3345612287521362, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.0644 - accuracy: 0.9787 - recall: 0.9846 - val_loss: 1.3346 - val_accuracy: 0.5889 - val_recall: 0.4902\n",
            "Epoch 261/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9787 - recall: 0.9545Epoch: 260 Logs: {'loss': 0.07807034999132156, 'accuracy': 0.978723406791687, 'recall': 0.9545454382896423, 'val_loss': 1.4479151964187622, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.37254902720451355}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.0781 - accuracy: 0.9787 - recall: 0.9545 - val_loss: 1.4479 - val_accuracy: 0.5333 - val_recall: 0.3725\n",
            "Epoch 262/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9787 - recall: 0.9683Epoch: 261 Logs: {'loss': 0.08632899075746536, 'accuracy': 0.978723406791687, 'recall': 0.9682539701461792, 'val_loss': 1.2361476421356201, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.0863 - accuracy: 0.9787 - recall: 0.9683 - val_loss: 1.2361 - val_accuracy: 0.5556 - val_recall: 0.5882\n",
            "Epoch 263/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9875 - recall: 0.9872Epoch: 262 Logs: {'loss': 0.07076713442802429, 'accuracy': 0.987500011920929, 'recall': 0.9871794581413269, 'val_loss': 1.3618422746658325, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0708 - accuracy: 0.9875 - recall: 0.9872 - val_loss: 1.3618 - val_accuracy: 0.5889 - val_recall: 0.5098\n",
            "Epoch 264/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9875 - recall: 0.9740Epoch: 263 Logs: {'loss': 0.061145465821027756, 'accuracy': 0.987500011920929, 'recall': 0.9740259647369385, 'val_loss': 1.3225053548812866, 'val_accuracy': 0.6111111044883728, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 148ms/step - loss: 0.0611 - accuracy: 0.9875 - recall: 0.9740 - val_loss: 1.3225 - val_accuracy: 0.6111 - val_recall: 0.5490\n",
            "Epoch 265/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9787 - recall: 0.9857Epoch: 264 Logs: {'loss': 0.07293831557035446, 'accuracy': 0.978723406791687, 'recall': 0.9857142567634583, 'val_loss': 1.3162925243377686, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.0729 - accuracy: 0.9787 - recall: 0.9857 - val_loss: 1.3163 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 266/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9929 - recall: 0.9857Epoch: 265 Logs: {'loss': 0.06005757302045822, 'accuracy': 0.9929078221321106, 'recall': 0.9857142567634583, 'val_loss': 1.3614795207977295, 'val_accuracy': 0.5222222208976746, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.0601 - accuracy: 0.9929 - recall: 0.9857 - val_loss: 1.3615 - val_accuracy: 0.5222 - val_recall: 0.4510\n",
            "Epoch 267/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9937 - recall: 0.9863Epoch: 266 Logs: {'loss': 0.07378281652927399, 'accuracy': 0.9937499761581421, 'recall': 0.9863013625144958, 'val_loss': 1.3267295360565186, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0738 - accuracy: 0.9937 - recall: 0.9863 - val_loss: 1.3267 - val_accuracy: 0.6000 - val_recall: 0.5490\n",
            "Epoch 268/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9716 - recall: 0.9853Epoch: 267 Logs: {'loss': 0.05677246302366257, 'accuracy': 0.9716312289237976, 'recall': 0.9852941036224365, 'val_loss': 1.408732295036316, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0568 - accuracy: 0.9716 - recall: 0.9853 - val_loss: 1.4087 - val_accuracy: 0.6000 - val_recall: 0.5098\n",
            "Epoch 269/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9645 - recall: 0.9275Epoch: 268 Logs: {'loss': 0.08785808831453323, 'accuracy': 0.9645389914512634, 'recall': 0.9275362491607666, 'val_loss': 1.4930728673934937, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0879 - accuracy: 0.9645 - recall: 0.9275 - val_loss: 1.4931 - val_accuracy: 0.5556 - val_recall: 0.4118\n",
            "Epoch 270/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9716 - recall: 0.9701Epoch: 269 Logs: {'loss': 0.07136852294206619, 'accuracy': 0.9716312289237976, 'recall': 0.9701492786407471, 'val_loss': 1.2778016328811646, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.0714 - accuracy: 0.9716 - recall: 0.9701 - val_loss: 1.2778 - val_accuracy: 0.5667 - val_recall: 0.5882\n",
            "Epoch 271/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0736 - accuracy: 0.9844 - recall: 1.0000Epoch: 270 Logs: {'loss': 0.07387769967317581, 'accuracy': 0.9858155846595764, 'recall': 1.0, 'val_loss': 1.3399181365966797, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.0739 - accuracy: 0.9858 - recall: 1.0000 - val_loss: 1.3399 - val_accuracy: 0.5889 - val_recall: 0.5294\n",
            "Epoch 272/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0796 - accuracy: 0.9766 - recall: 0.9492Epoch: 271 Logs: {'loss': 0.07651948183774948, 'accuracy': 0.978723406791687, 'recall': 0.9538461565971375, 'val_loss': 1.3612669706344604, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.0765 - accuracy: 0.9787 - recall: 0.9538 - val_loss: 1.3613 - val_accuracy: 0.5889 - val_recall: 0.5294\n",
            "Epoch 273/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9929 - recall: 0.9841Epoch: 272 Logs: {'loss': 0.05158716440200806, 'accuracy': 0.9929078221321106, 'recall': 0.9841269850730896, 'val_loss': 1.3365532159805298, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0516 - accuracy: 0.9929 - recall: 0.9841 - val_loss: 1.3366 - val_accuracy: 0.5556 - val_recall: 0.5490\n",
            "Epoch 274/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9716 - recall: 0.9828Epoch: 273 Logs: {'loss': 0.07715339213609695, 'accuracy': 0.9716312289237976, 'recall': 0.982758641242981, 'val_loss': 1.461626648902893, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.0772 - accuracy: 0.9716 - recall: 0.9828 - val_loss: 1.4616 - val_accuracy: 0.5333 - val_recall: 0.4314\n",
            "Epoch 275/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9937 - recall: 0.9875Epoch: 274 Logs: {'loss': 0.06812559813261032, 'accuracy': 0.9937499761581421, 'recall': 0.987500011920929, 'val_loss': 1.3428399562835693, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0681 - accuracy: 0.9937 - recall: 0.9875 - val_loss: 1.3428 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 276/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9929 - recall: 0.9857Epoch: 275 Logs: {'loss': 0.0595291405916214, 'accuracy': 0.9929078221321106, 'recall': 0.9857142567634583, 'val_loss': 1.4159622192382812, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.0595 - accuracy: 0.9929 - recall: 0.9857 - val_loss: 1.4160 - val_accuracy: 0.5889 - val_recall: 0.5098\n",
            "Epoch 277/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 1.0000 - recall: 1.0000Epoch: 276 Logs: {'loss': 0.049885183572769165, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.416028380393982, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0499 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.4160 - val_accuracy: 0.6000 - val_recall: 0.5098\n",
            "Epoch 278/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9858 - recall: 0.9692Epoch: 277 Logs: {'loss': 0.06562504917383194, 'accuracy': 0.9858155846595764, 'recall': 0.9692307710647583, 'val_loss': 1.3973946571350098, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.0656 - accuracy: 0.9858 - recall: 0.9692 - val_loss: 1.3974 - val_accuracy: 0.5778 - val_recall: 0.5098\n",
            "Epoch 279/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9787 - recall: 0.9677Epoch: 278 Logs: {'loss': 0.05654766410589218, 'accuracy': 0.978723406791687, 'recall': 0.9677419066429138, 'val_loss': 1.429197907447815, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0565 - accuracy: 0.9787 - recall: 0.9677 - val_loss: 1.4292 - val_accuracy: 0.5889 - val_recall: 0.5098\n",
            "Epoch 280/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9787 - recall: 0.9538Epoch: 279 Logs: {'loss': 0.0751861035823822, 'accuracy': 0.978723406791687, 'recall': 0.9538461565971375, 'val_loss': 1.4204305410385132, 'val_accuracy': 0.5444444417953491, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 150ms/step - loss: 0.0752 - accuracy: 0.9787 - recall: 0.9538 - val_loss: 1.4204 - val_accuracy: 0.5444 - val_recall: 0.4706\n",
            "Epoch 281/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9858 - recall: 0.9868Epoch: 280 Logs: {'loss': 0.056519508361816406, 'accuracy': 0.9858155846595764, 'recall': 0.9868420958518982, 'val_loss': 1.3739094734191895, 'val_accuracy': 0.5444444417953491, 'val_recall': 0.7058823704719543}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0565 - accuracy: 0.9858 - recall: 0.9868 - val_loss: 1.3739 - val_accuracy: 0.5444 - val_recall: 0.7059\n",
            "Epoch 282/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9812 - recall: 0.9863Epoch: 281 Logs: {'loss': 0.07944335788488388, 'accuracy': 0.981249988079071, 'recall': 0.9863013625144958, 'val_loss': 1.516650676727295, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0794 - accuracy: 0.9812 - recall: 0.9863 - val_loss: 1.5167 - val_accuracy: 0.5667 - val_recall: 0.4706\n",
            "Epoch 283/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9787 - recall: 0.9508Epoch: 282 Logs: {'loss': 0.07769040763378143, 'accuracy': 0.978723406791687, 'recall': 0.9508196711540222, 'val_loss': 1.4132566452026367, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.5882353186607361}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.0777 - accuracy: 0.9787 - recall: 0.9508 - val_loss: 1.4133 - val_accuracy: 0.6000 - val_recall: 0.5882\n",
            "Epoch 284/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9858 - recall: 0.9844Epoch: 283 Logs: {'loss': 0.05669746175408363, 'accuracy': 0.9858155846595764, 'recall': 0.984375, 'val_loss': 1.4603381156921387, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0567 - accuracy: 0.9858 - recall: 0.9844 - val_loss: 1.4603 - val_accuracy: 0.5333 - val_recall: 0.4314\n",
            "Epoch 285/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9929 - recall: 1.0000Epoch: 284 Logs: {'loss': 0.057682111859321594, 'accuracy': 0.9929078221321106, 'recall': 1.0, 'val_loss': 1.3878464698791504, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0577 - accuracy: 0.9929 - recall: 1.0000 - val_loss: 1.3878 - val_accuracy: 0.5333 - val_recall: 0.4706\n",
            "Epoch 286/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9750 - recall: 0.9744Epoch: 285 Logs: {'loss': 0.06180676817893982, 'accuracy': 0.9750000238418579, 'recall': 0.9743589758872986, 'val_loss': 1.4192872047424316, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0618 - accuracy: 0.9750 - recall: 0.9744 - val_loss: 1.4193 - val_accuracy: 0.5889 - val_recall: 0.5490\n",
            "Epoch 287/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9929 - recall: 0.9848Epoch: 286 Logs: {'loss': 0.06705471128225327, 'accuracy': 0.9929078221321106, 'recall': 0.9848484992980957, 'val_loss': 1.5757659673690796, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 158ms/step - loss: 0.0671 - accuracy: 0.9929 - recall: 0.9848 - val_loss: 1.5758 - val_accuracy: 0.5889 - val_recall: 0.4902\n",
            "Epoch 288/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9937 - recall: 0.9867Epoch: 287 Logs: {'loss': 0.06102627515792847, 'accuracy': 0.9937499761581421, 'recall': 0.9866666793823242, 'val_loss': 1.413087248802185, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0610 - accuracy: 0.9937 - recall: 0.9867 - val_loss: 1.4131 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 289/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000 - recall: 1.0000Epoch: 288 Logs: {'loss': 0.03472517058253288, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.4766730070114136, 'val_accuracy': 0.5444444417953491, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.0347 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.4767 - val_accuracy: 0.5444 - val_recall: 0.4510\n",
            "Epoch 290/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9929 - recall: 0.9848Epoch: 289 Logs: {'loss': 0.049107443541288376, 'accuracy': 0.9929078221321106, 'recall': 0.9848484992980957, 'val_loss': 1.4434400796890259, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0491 - accuracy: 0.9929 - recall: 0.9848 - val_loss: 1.4434 - val_accuracy: 0.5778 - val_recall: 0.5098\n",
            "Epoch 291/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9929 - recall: 0.9851Epoch: 290 Logs: {'loss': 0.06490679830312729, 'accuracy': 0.9929078221321106, 'recall': 0.9850746393203735, 'val_loss': 1.4381754398345947, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0649 - accuracy: 0.9929 - recall: 0.9851 - val_loss: 1.4382 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 292/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9937 - recall: 0.9861Epoch: 291 Logs: {'loss': 0.044591017067432404, 'accuracy': 0.9937499761581421, 'recall': 0.9861111044883728, 'val_loss': 1.497915267944336, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0446 - accuracy: 0.9937 - recall: 0.9861 - val_loss: 1.4979 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 293/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9937 - recall: 0.9873Epoch: 292 Logs: {'loss': 0.04041016846895218, 'accuracy': 0.9937499761581421, 'recall': 0.9873417615890503, 'val_loss': 1.4611674547195435, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0404 - accuracy: 0.9937 - recall: 0.9873 - val_loss: 1.4612 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 294/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9937 - recall: 1.0000Epoch: 293 Logs: {'loss': 0.048218242824077606, 'accuracy': 0.9937499761581421, 'recall': 1.0, 'val_loss': 1.4822648763656616, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0482 - accuracy: 0.9937 - recall: 1.0000 - val_loss: 1.4823 - val_accuracy: 0.5667 - val_recall: 0.5098\n",
            "Epoch 295/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9929 - recall: 0.9831Epoch: 294 Logs: {'loss': 0.04529864713549614, 'accuracy': 0.9929078221321106, 'recall': 0.9830508232116699, 'val_loss': 1.5594619512557983, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0453 - accuracy: 0.9929 - recall: 0.9831 - val_loss: 1.5595 - val_accuracy: 0.5667 - val_recall: 0.4510\n",
            "Epoch 296/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9929 - recall: 0.9836Epoch: 295 Logs: {'loss': 0.04541052505373955, 'accuracy': 0.9929078221321106, 'recall': 0.9836065769195557, 'val_loss': 1.4616481065750122, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0454 - accuracy: 0.9929 - recall: 0.9836 - val_loss: 1.4616 - val_accuracy: 0.5889 - val_recall: 0.5490\n",
            "Epoch 297/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9875 - recall: 0.9875Epoch: 296 Logs: {'loss': 0.05223943665623665, 'accuracy': 0.987500011920929, 'recall': 0.987500011920929, 'val_loss': 1.4349604845046997, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0522 - accuracy: 0.9875 - recall: 0.9875 - val_loss: 1.4350 - val_accuracy: 0.5556 - val_recall: 0.5294\n",
            "Epoch 298/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9858 - recall: 0.9692Epoch: 297 Logs: {'loss': 0.08744700998067856, 'accuracy': 0.9858155846595764, 'recall': 0.9692307710647583, 'val_loss': 1.6362050771713257, 'val_accuracy': 0.5111111402511597, 'val_recall': 0.3529411852359772}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.0874 - accuracy: 0.9858 - recall: 0.9692 - val_loss: 1.6362 - val_accuracy: 0.5111 - val_recall: 0.3529\n",
            "Epoch 299/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9858 - recall: 0.9831Epoch: 298 Logs: {'loss': 0.04551767185330391, 'accuracy': 0.9858155846595764, 'recall': 0.9830508232116699, 'val_loss': 1.483360767364502, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.6078431606292725}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0455 - accuracy: 0.9858 - recall: 0.9831 - val_loss: 1.4834 - val_accuracy: 0.5556 - val_recall: 0.6078\n",
            "Epoch 300/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9937 - recall: 1.0000Epoch: 299 Logs: {'loss': 0.04432879015803337, 'accuracy': 0.9937499761581421, 'recall': 1.0, 'val_loss': 1.6357364654541016, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0443 - accuracy: 0.9937 - recall: 1.0000 - val_loss: 1.6357 - val_accuracy: 0.5667 - val_recall: 0.4902\n",
            "Epoch 301/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9787 - recall: 0.9508Epoch: 300 Logs: {'loss': 0.0569133497774601, 'accuracy': 0.978723406791687, 'recall': 0.9508196711540222, 'val_loss': 1.5066207647323608, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0569 - accuracy: 0.9787 - recall: 0.9508 - val_loss: 1.5066 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 302/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 1.0000 - recall: 1.0000Epoch: 301 Logs: {'loss': 0.0454217791557312, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5099021196365356, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0454 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5099 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 303/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9929 - recall: 0.9831Epoch: 302 Logs: {'loss': 0.04646315425634384, 'accuracy': 0.9929078221321106, 'recall': 0.9830508232116699, 'val_loss': 1.5386316776275635, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0465 - accuracy: 0.9929 - recall: 0.9831 - val_loss: 1.5386 - val_accuracy: 0.5778 - val_recall: 0.5098\n",
            "Epoch 304/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9929 - recall: 0.9851Epoch: 303 Logs: {'loss': 0.03407619893550873, 'accuracy': 0.9929078221321106, 'recall': 0.9850746393203735, 'val_loss': 1.539384126663208, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.0341 - accuracy: 0.9929 - recall: 0.9851 - val_loss: 1.5394 - val_accuracy: 0.5778 - val_recall: 0.4902\n",
            "Epoch 305/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000 - recall: 1.0000Epoch: 304 Logs: {'loss': 0.03218744322657585, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5055426359176636, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0322 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5055 - val_accuracy: 0.5778 - val_recall: 0.5098\n",
            "Epoch 306/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9937 - recall: 0.9851Epoch: 305 Logs: {'loss': 0.04113209992647171, 'accuracy': 0.9937499761581421, 'recall': 0.9850746393203735, 'val_loss': 1.522653579711914, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0411 - accuracy: 0.9937 - recall: 0.9851 - val_loss: 1.5227 - val_accuracy: 0.5889 - val_recall: 0.5294\n",
            "Epoch 307/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 1.0000 - recall: 1.0000Epoch: 306 Logs: {'loss': 0.025069624185562134, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5535656213760376, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0251 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5536 - val_accuracy: 0.6000 - val_recall: 0.5294\n",
            "Epoch 308/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9929 - recall: 0.9863Epoch: 307 Logs: {'loss': 0.036167629063129425, 'accuracy': 0.9929078221321106, 'recall': 0.9863013625144958, 'val_loss': 1.5538654327392578, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 152ms/step - loss: 0.0362 - accuracy: 0.9929 - recall: 0.9863 - val_loss: 1.5539 - val_accuracy: 0.5889 - val_recall: 0.5098\n",
            "Epoch 309/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9929 - recall: 0.9831Epoch: 308 Logs: {'loss': 0.0430033802986145, 'accuracy': 0.9929078221321106, 'recall': 0.9830508232116699, 'val_loss': 1.528909683227539, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0430 - accuracy: 0.9929 - recall: 0.9831 - val_loss: 1.5289 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 310/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000 - recall: 1.0000Epoch: 309 Logs: {'loss': 0.03301156312227249, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5484198331832886, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0330 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5484 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 311/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9937 - recall: 0.9868Epoch: 310 Logs: {'loss': 0.03473532199859619, 'accuracy': 0.9937499761581421, 'recall': 0.9868420958518982, 'val_loss': 1.5712361335754395, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0347 - accuracy: 0.9937 - recall: 0.9868 - val_loss: 1.5712 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 312/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9937 - recall: 0.9863Epoch: 311 Logs: {'loss': 0.03153097257018089, 'accuracy': 0.9937499761581421, 'recall': 0.9863013625144958, 'val_loss': 1.5603352785110474, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0315 - accuracy: 0.9937 - recall: 0.9863 - val_loss: 1.5603 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 313/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000 - recall: 1.0000Epoch: 312 Logs: {'loss': 0.03273562341928482, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6005109548568726, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0327 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6005 - val_accuracy: 0.5778 - val_recall: 0.5294\n",
            "Epoch 314/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 1.0000 - recall: 1.0000Epoch: 313 Logs: {'loss': 0.032359324395656586, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5496572256088257, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0324 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5497 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 315/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000 - recall: 1.0000Epoch: 314 Logs: {'loss': 0.03190094977617264, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.56925630569458, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.0319 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5693 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 316/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9929 - recall: 0.9831Epoch: 315 Logs: {'loss': 0.029118439182639122, 'accuracy': 0.9929078221321106, 'recall': 0.9830508232116699, 'val_loss': 1.5640075206756592, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0291 - accuracy: 0.9929 - recall: 0.9831 - val_loss: 1.5640 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 317/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000 - recall: 1.0000Epoch: 316 Logs: {'loss': 0.02275525964796543, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5986690521240234, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0228 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5987 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 318/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9875 - recall: 0.9744Epoch: 317 Logs: {'loss': 0.03828877583146095, 'accuracy': 0.987500011920929, 'recall': 0.9743589758872986, 'val_loss': 1.5986127853393555, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0383 - accuracy: 0.9875 - recall: 0.9744 - val_loss: 1.5986 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 319/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000 - recall: 1.0000Epoch: 318 Logs: {'loss': 0.037161700427532196, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5847991704940796, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 148ms/step - loss: 0.0372 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5848 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 320/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000 - recall: 1.0000Epoch: 319 Logs: {'loss': 0.034174103289842606, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6360920667648315, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0342 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6361 - val_accuracy: 0.5667 - val_recall: 0.5098\n",
            "Epoch 321/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000 - recall: 1.0000Epoch: 320 Logs: {'loss': 0.027529437094926834, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6097540855407715, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0275 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6098 - val_accuracy: 0.5556 - val_recall: 0.5098\n",
            "Epoch 322/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000 - recall: 1.0000Epoch: 321 Logs: {'loss': 0.03197533264756203, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.5865979194641113, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.0320 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.5866 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 323/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0203 - accuracy: 1.0000 - recall: 1.0000Epoch: 322 Logs: {'loss': 0.025236280634999275, 'accuracy': 0.9929078221321106, 'recall': 0.9850746393203735, 'val_loss': 1.650156021118164, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.0252 - accuracy: 0.9929 - recall: 0.9851 - val_loss: 1.6502 - val_accuracy: 0.5889 - val_recall: 0.5098\n",
            "Epoch 324/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000 - recall: 1.0000Epoch: 323 Logs: {'loss': 0.030682126060128212, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6133623123168945, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0307 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6134 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 325/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000 - recall: 1.0000Epoch: 324 Logs: {'loss': 0.030420714989304543, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6259634494781494, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0304 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6260 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 326/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 1.0000 - recall: 1.0000Epoch: 325 Logs: {'loss': 0.02137751691043377, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6186953783035278, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0214 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6187 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 327/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000 - recall: 1.0000Epoch: 326 Logs: {'loss': 0.026665452867746353, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.634522557258606, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0267 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6345 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 328/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000 - recall: 1.0000Epoch: 327 Logs: {'loss': 0.02769845351576805, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6389299631118774, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.0277 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6389 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 329/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 1.0000 - recall: 1.0000Epoch: 328 Logs: {'loss': 0.021540982648730278, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6639580726623535, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 153ms/step - loss: 0.0215 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6640 - val_accuracy: 0.5889 - val_recall: 0.5490\n",
            "Epoch 330/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0273 - accuracy: 1.0000 - recall: 1.0000Epoch: 329 Logs: {'loss': 0.025926250964403152, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.653341293334961, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.0259 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6533 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 331/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000 - recall: 1.0000Epoch: 330 Logs: {'loss': 0.026366639882326126, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.663569688796997, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0264 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6636 - val_accuracy: 0.5556 - val_recall: 0.5098\n",
            "Epoch 332/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 1.0000 - recall: 1.0000Epoch: 331 Logs: {'loss': 0.025908594951033592, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6275709867477417, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0259 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6276 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 333/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000 - recall: 1.0000Epoch: 332 Logs: {'loss': 0.027906885370612144, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6335406303405762, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0279 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6335 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 334/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000 - recall: 1.0000Epoch: 333 Logs: {'loss': 0.023724617436528206, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.7040717601776123, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0237 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.7041 - val_accuracy: 0.5889 - val_recall: 0.5490\n",
            "Epoch 335/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000 - recall: 1.0000Epoch: 334 Logs: {'loss': 0.03334944322705269, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6774039268493652, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.0333 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6774 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 336/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000 - recall: 1.0000Epoch: 335 Logs: {'loss': 0.024948418140411377, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6885268688201904, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.0249 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6885 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 337/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000 - recall: 1.0000Epoch: 336 Logs: {'loss': 0.02227490395307541, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6768569946289062, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0223 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6769 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 338/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0255 - accuracy: 1.0000 - recall: 1.0000Epoch: 337 Logs: {'loss': 0.025491157546639442, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.7627604007720947, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.47058823704719543}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.0255 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.7628 - val_accuracy: 0.5667 - val_recall: 0.4706\n",
            "Epoch 339/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 1.0000 - recall: 1.0000Epoch: 338 Logs: {'loss': 0.022883664816617966, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.694478988647461, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0229 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6945 - val_accuracy: 0.5556 - val_recall: 0.5098\n",
            "Epoch 340/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 1.0000 - recall: 1.0000Epoch: 339 Logs: {'loss': 0.021294480189681053, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6862066984176636, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0213 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6862 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 341/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 1.0000 - recall: 1.0000Epoch: 340 Logs: {'loss': 0.02399468794465065, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.7109249830245972, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0240 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.7109 - val_accuracy: 0.5778 - val_recall: 0.5294\n",
            "Epoch 342/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000 - recall: 1.0000Epoch: 341 Logs: {'loss': 0.024235805496573448, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.7163225412368774, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0242 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.7163 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 343/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 1.0000 - recall: 1.0000Epoch: 342 Logs: {'loss': 0.02460438199341297, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.7114214897155762, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 155ms/step - loss: 0.0246 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.7114 - val_accuracy: 0.5778 - val_recall: 0.5490\n",
            "Epoch 344/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000 - recall: 1.0000Epoch: 343 Logs: {'loss': 0.026925522834062576, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.7103677988052368, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 151ms/step - loss: 0.0269 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.7104 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 345/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000 - recall: 1.0000Epoch: 344 Logs: {'loss': 0.02833842672407627, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.7155758142471313, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 152ms/step - loss: 0.0283 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.7156 - val_accuracy: 0.5667 - val_recall: 0.5294\n",
            "Epoch 346/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 1.0000 - recall: 1.0000Epoch: 345 Logs: {'loss': 0.025188695639371872, 'accuracy': 1.0, 'recall': 1.0, 'val_loss': 1.6978471279144287, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 147ms/step - loss: 0.0252 - accuracy: 1.0000 - recall: 1.0000 - val_loss: 1.6978 - val_accuracy: 0.5667 - val_recall: 0.5490\n",
            "Epoch 347/1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-073fcf7b43aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2,checkpoint3,checkpoint4])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRecallAccuracyCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Moving the weights from colab to gdrive\n",
        "# !mv /content/model_e104_a83_r91_va61_vr84.h5 /content/drive/MyDrive/CV-EmpathNet/\n",
        "# !mv /content/model_e /content/drive/MyDrive/CV-EmpathNet/\n",
        "# !mv /content/model85856184.h5 /content/drive/MyDrive/CV-EmpathNet/\n",
        "\n",
        "#Best Results = \n",
        "# 104_model{'loss': 0.4074825346469879, 'accuracy': 0.8374999761581421, 'recall': 0.9130434989929199,  'val_accuracy': 0.6111111044883728, 'val_recall': 0.843137264251709}.h5 'val_loss': 0.6829150915145874,"
      ],
      "metadata": {
        "id": "tYwlL-iPYLCz"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xception with Data Augmentation"
      ],
      "metadata": {
        "id": "GaOrj2yZWJZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = tf.keras.metrics.Recall()"
      ],
      "metadata": {
        "id": "0P2P1cYWvkDu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Callback Function To Save Xception Model based on  metric thresholds\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "class XceptionRecallAccuracyCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      print('Epoch:',epoch, 'Logs:',logs)\n",
        "\n",
        "      # if logs['accuracy']>0.80 and logs['recall']>0.76 and logs['val_accuracy'] > 0.6 and logs['val_recall']>0.8:\n",
        "      if logs['accuracy']>0.80 and logs['recall']>0.7 and logs['val_accuracy'] > 0.8 and logs['val_recall']>0.6:\n",
        "        print('Val Accuracy More Than Set Thresholds', logs)\n",
        "        print('Saving Model','-'*30)\n",
        "        self.model.save(f'/content/drive/MyDrive/CV-EmpathNet/XceptionNetWeights/{epoch}_model{str(logs)}.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "izSjtn4IsUgQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "\n",
        "\n",
        "\n",
        "pretrained_model = Xception(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "pretrained_model.trainable = False\n",
        "model = Sequential()\n",
        "model.add(pretrained_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "#Optional Checkpoints to save model  based on metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath_r = 'saug_weights_best_rmodel.hdf5'\n",
        "filepath_a = 'saug_weights_best_amodel.hdf5'\n",
        "filepath_vr = 'saug_weights_best_vrmodel.hdf5'\n",
        "filepath_va = 'saug_weights_best_vamodel.hdf5'\n",
        "\n",
        "checkpoint1 = ModelCheckpoint(filepath_vr,monitor = 'val_recall_25',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint2 = ModelCheckpoint(filepath_va,monitor = 'val_accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint3 = ModelCheckpoint(filepath_a,monitor = 'accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "checkpoint4 = ModelCheckpoint(filepath_r,monitor = 'recall_25',verbose = 1, save_best_only=True, mode='max')\n",
        "\n",
        "#With 0.0001 lr.\n",
        "optim = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001,\n",
        "    # learning_rate=0.00001,\n",
        "    learning_rate=0.0001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy', r], optimizer=optim)\n",
        "n_epochs = 1000\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2,checkpoint3,checkpoint4])\n",
        "# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2])\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks = [XceptionRecallAccuracyCallback()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8HK0vi5GsAEG",
        "outputId": "cd725391-6bcd-49a1-eda3-100354d1ad6e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.6000 - recall: 0.4758Epoch: 0 Logs: {'loss': 0.6872004270553589, 'accuracy': 0.6000000238418579, 'recall': 0.47580644488334656, 'val_loss': 0.7046597599983215, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 4s 352ms/step - loss: 0.6872 - accuracy: 0.6000 - recall: 0.4758 - val_loss: 0.7047 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6844 - accuracy: 0.5437 - recall: 0.0000e+00Epoch: 1 Logs: {'loss': 0.6843863725662231, 'accuracy': 0.543749988079071, 'recall': 0.0, 'val_loss': 0.7058087587356567, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6844 - accuracy: 0.5437 - recall: 0.0000e+00 - val_loss: 0.7058 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.5248 - recall: 0.0290Epoch: 2 Logs: {'loss': 0.6848476529121399, 'accuracy': 0.5248227119445801, 'recall': 0.028985507786273956, 'val_loss': 0.6873366236686707, 'val_accuracy': 0.47777777910232544, 'val_recall': 0.0784313753247261}\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.6848 - accuracy: 0.5248 - recall: 0.0290 - val_loss: 0.6873 - val_accuracy: 0.4778 - val_recall: 0.0784\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6802 - accuracy: 0.6383 - recall: 0.3000Epoch: 3 Logs: {'loss': 0.6801573038101196, 'accuracy': 0.6382978558540344, 'recall': 0.30000001192092896, 'val_loss': 0.6771337985992432, 'val_accuracy': 0.7222222089767456, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.6802 - accuracy: 0.6383 - recall: 0.3000 - val_loss: 0.6771 - val_accuracy: 0.7222 - val_recall: 0.5294\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6812 - accuracy: 0.7163 - recall: 0.5873Epoch: 4 Logs: {'loss': 0.6812262535095215, 'accuracy': 0.716312050819397, 'recall': 0.5873016119003296, 'val_loss': 0.6789549589157104, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.6812 - accuracy: 0.7163 - recall: 0.5873 - val_loss: 0.6790 - val_accuracy: 0.5778 - val_recall: 0.2549\n",
            "Epoch 6/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6652 - accuracy: 0.6879 - recall: 0.3016Epoch: 5 Logs: {'loss': 0.6651820540428162, 'accuracy': 0.6879432797431946, 'recall': 0.30158731341362, 'val_loss': 0.6948094964027405, 'val_accuracy': 0.4555555582046509, 'val_recall': 0.03921568766236305}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.6652 - accuracy: 0.6879 - recall: 0.3016 - val_loss: 0.6948 - val_accuracy: 0.4556 - val_recall: 0.0392\n",
            "Epoch 7/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6713 - accuracy: 0.5603 - recall: 0.0312    Epoch: 6 Logs: {'loss': 0.6713187098503113, 'accuracy': 0.5602836608886719, 'recall': 0.03125, 'val_loss': 0.7083672285079956, 'val_accuracy': 0.4333333373069763, 'val_recall': 0.0}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.6713 - accuracy: 0.5603 - recall: 0.0312 - val_loss: 0.7084 - val_accuracy: 0.4333 - val_recall: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.5390 - recall: 0.0299Epoch: 7 Logs: {'loss': 0.676393449306488, 'accuracy': 0.5390070676803589, 'recall': 0.02985074557363987, 'val_loss': 0.6825457811355591, 'val_accuracy': 0.5, 'val_recall': 0.11764705926179886}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6764 - accuracy: 0.5390 - recall: 0.0299 - val_loss: 0.6825 - val_accuracy: 0.5000 - val_recall: 0.1176\n",
            "Epoch 9/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6554 - accuracy: 0.6879 - recall: 0.2787Epoch: 8 Logs: {'loss': 0.6554298996925354, 'accuracy': 0.6879432797431946, 'recall': 0.2786885201931, 'val_loss': 0.6820990443229675, 'val_accuracy': 0.5, 'val_recall': 0.11764705926179886}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.6554 - accuracy: 0.6879 - recall: 0.2787 - val_loss: 0.6821 - val_accuracy: 0.5000 - val_recall: 0.1176\n",
            "Epoch 10/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6635 - accuracy: 0.6328 - recall: 0.1897Epoch: 9 Logs: {'loss': 0.6618565917015076, 'accuracy': 0.6453900933265686, 'recall': 0.2063492089509964, 'val_loss': 0.6825538873672485, 'val_accuracy': 0.5, 'val_recall': 0.11764705926179886}\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 0.6619 - accuracy: 0.6454 - recall: 0.2063 - val_loss: 0.6826 - val_accuracy: 0.5000 - val_recall: 0.1176\n",
            "Epoch 11/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6557 - accuracy: 0.6250 - recall: 0.2258Epoch: 10 Logs: {'loss': 0.655133068561554, 'accuracy': 0.631205677986145, 'recall': 0.2238806039094925, 'val_loss': 0.6771377325057983, 'val_accuracy': 0.5333333611488342, 'val_recall': 0.1764705926179886}\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.6551 - accuracy: 0.6312 - recall: 0.2239 - val_loss: 0.6771 - val_accuracy: 0.5333 - val_recall: 0.1765\n",
            "Epoch 12/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6522 - accuracy: 0.7021 - recall: 0.3594Epoch: 11 Logs: {'loss': 0.6521998643875122, 'accuracy': 0.7021276354789734, 'recall': 0.359375, 'val_loss': 0.6660318374633789, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6522 - accuracy: 0.7021 - recall: 0.3594 - val_loss: 0.6660 - val_accuracy: 0.5778 - val_recall: 0.2549\n",
            "Epoch 13/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6596 - accuracy: 0.6950 - recall: 0.3939Epoch: 12 Logs: {'loss': 0.6595826745033264, 'accuracy': 0.695035457611084, 'recall': 0.39393940567970276, 'val_loss': 0.6635980606079102, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6596 - accuracy: 0.6950 - recall: 0.3939 - val_loss: 0.6636 - val_accuracy: 0.5778 - val_recall: 0.2549\n",
            "Epoch 14/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.6950 - recall: 0.4444Epoch: 13 Logs: {'loss': 0.6560581922531128, 'accuracy': 0.695035457611084, 'recall': 0.4444444477558136, 'val_loss': 0.6618309617042542, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.27450981736183167}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.6561 - accuracy: 0.6950 - recall: 0.4444 - val_loss: 0.6618 - val_accuracy: 0.5889 - val_recall: 0.2745\n",
            "Epoch 15/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6460 - accuracy: 0.7376 - recall: 0.5294Epoch: 14 Logs: {'loss': 0.6459711194038391, 'accuracy': 0.73758864402771, 'recall': 0.529411792755127, 'val_loss': 0.6517824530601501, 'val_accuracy': 0.7222222089767456, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.6460 - accuracy: 0.7376 - recall: 0.5294 - val_loss: 0.6518 - val_accuracy: 0.7222 - val_recall: 0.5294\n",
            "Epoch 16/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.8085 - recall: 0.6406Epoch: 15 Logs: {'loss': 0.640575647354126, 'accuracy': 0.8085106611251831, 'recall': 0.640625, 'val_loss': 0.6503505110740662, 'val_accuracy': 0.7111111283302307, 'val_recall': 0.5098039507865906}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.6406 - accuracy: 0.8085 - recall: 0.6406 - val_loss: 0.6504 - val_accuracy: 0.7111 - val_recall: 0.5098\n",
            "Epoch 17/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.7250 - recall: 0.5135Epoch: 16 Logs: {'loss': 0.6433111429214478, 'accuracy': 0.7250000238418579, 'recall': 0.5135135054588318, 'val_loss': 0.6577984094619751, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.6433 - accuracy: 0.7250 - recall: 0.5135 - val_loss: 0.6578 - val_accuracy: 0.5778 - val_recall: 0.2549\n",
            "Epoch 18/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6271 - accuracy: 0.7305 - recall: 0.3898Epoch: 17 Logs: {'loss': 0.6271240711212158, 'accuracy': 0.7304964661598206, 'recall': 0.38983049988746643, 'val_loss': 0.6915178298950195, 'val_accuracy': 0.4888888895511627, 'val_recall': 0.09803921729326248}\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.6271 - accuracy: 0.7305 - recall: 0.3898 - val_loss: 0.6915 - val_accuracy: 0.4889 - val_recall: 0.0980\n",
            "Epoch 19/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6470 - accuracy: 0.6062 - recall: 0.2025Epoch: 18 Logs: {'loss': 0.6469550132751465, 'accuracy': 0.606249988079071, 'recall': 0.20253165066242218, 'val_loss': 0.6858590841293335, 'val_accuracy': 0.5, 'val_recall': 0.11764705926179886}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.6470 - accuracy: 0.6062 - recall: 0.2025 - val_loss: 0.6859 - val_accuracy: 0.5000 - val_recall: 0.1176\n",
            "Epoch 20/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.6525 - recall: 0.2462Epoch: 19 Logs: {'loss': 0.64333176612854, 'accuracy': 0.652482271194458, 'recall': 0.2461538463830948, 'val_loss': 0.662604033946991, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.23529411852359772}\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.6433 - accuracy: 0.6525 - recall: 0.2462 - val_loss: 0.6626 - val_accuracy: 0.5667 - val_recall: 0.2353\n",
            "Epoch 21/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.7437 - recall: 0.4412Epoch: 20 Logs: {'loss': 0.621497392654419, 'accuracy': 0.7437499761581421, 'recall': 0.44117647409439087, 'val_loss': 0.6555852890014648, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6215 - accuracy: 0.7437 - recall: 0.4412 - val_loss: 0.6556 - val_accuracy: 0.5778 - val_recall: 0.2549\n",
            "Epoch 22/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.6938 - recall: 0.3947Epoch: 21 Logs: {'loss': 0.6403055787086487, 'accuracy': 0.6937500238418579, 'recall': 0.3947368562221527, 'val_loss': 0.6505540013313293, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.27450981736183167}\n",
            "5/5 [==============================] - 1s 137ms/step - loss: 0.6403 - accuracy: 0.6938 - recall: 0.3947 - val_loss: 0.6506 - val_accuracy: 0.5889 - val_recall: 0.2745\n",
            "Epoch 23/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6264 - accuracy: 0.7109 - recall: 0.4182Epoch: 22 Logs: {'loss': 0.6215014457702637, 'accuracy': 0.7234042286872864, 'recall': 0.46875, 'val_loss': 0.6479196548461914, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.29411765933036804}\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.6215 - accuracy: 0.7234 - recall: 0.4688 - val_loss: 0.6479 - val_accuracy: 0.6000 - val_recall: 0.2941\n",
            "Epoch 24/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6441 - accuracy: 0.6797 - recall: 0.4127Epoch: 23 Logs: {'loss': 0.6425755620002747, 'accuracy': 0.695035457611084, 'recall': 0.43283581733703613, 'val_loss': 0.6347652077674866, 'val_accuracy': 0.7222222089767456, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.6426 - accuracy: 0.6950 - recall: 0.4328 - val_loss: 0.6348 - val_accuracy: 0.7222 - val_recall: 0.5294\n",
            "Epoch 25/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.7518 - recall: 0.6029Epoch: 24 Logs: {'loss': 0.6300474405288696, 'accuracy': 0.7517730593681335, 'recall': 0.6029411554336548, 'val_loss': 0.6327977776527405, 'val_accuracy': 0.7222222089767456, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6300 - accuracy: 0.7518 - recall: 0.6029 - val_loss: 0.6328 - val_accuracy: 0.7222 - val_recall: 0.5294\n",
            "Epoch 26/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6204 - accuracy: 0.7937 - recall: 0.6301Epoch: 25 Logs: {'loss': 0.6203781366348267, 'accuracy': 0.793749988079071, 'recall': 0.6301369667053223, 'val_loss': 0.6401937007904053, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.3529411852359772}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.6204 - accuracy: 0.7937 - recall: 0.6301 - val_loss: 0.6402 - val_accuracy: 0.6333 - val_recall: 0.3529\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.7660 - recall: 0.5882Epoch: 26 Logs: {'loss': 0.6168551445007324, 'accuracy': 0.7659574747085571, 'recall': 0.5882353186607361, 'val_loss': 0.635831892490387, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.6169 - accuracy: 0.7660 - recall: 0.5882 - val_loss: 0.6358 - val_accuracy: 0.6778 - val_recall: 0.4510\n",
            "Epoch 28/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6173 - accuracy: 0.7625 - recall: 0.5270Epoch: 27 Logs: {'loss': 0.6172934174537659, 'accuracy': 0.762499988079071, 'recall': 0.5270270109176636, 'val_loss': 0.6437114477157593, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.29411765933036804}\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.6173 - accuracy: 0.7625 - recall: 0.5270 - val_loss: 0.6437 - val_accuracy: 0.6000 - val_recall: 0.2941\n",
            "Epoch 29/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6103 - accuracy: 0.7589 - recall: 0.5000Epoch: 28 Logs: {'loss': 0.6102931499481201, 'accuracy': 0.758865237236023, 'recall': 0.5, 'val_loss': 0.6464919447898865, 'val_accuracy': 0.5888888835906982, 'val_recall': 0.27450981736183167}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.6103 - accuracy: 0.7589 - recall: 0.5000 - val_loss: 0.6465 - val_accuracy: 0.5889 - val_recall: 0.2745\n",
            "Epoch 30/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.7437 - recall: 0.4265Epoch: 29 Logs: {'loss': 0.6039115190505981, 'accuracy': 0.7437499761581421, 'recall': 0.4264705777168274, 'val_loss': 0.6548451781272888, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.23529411852359772}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6039 - accuracy: 0.7437 - recall: 0.4265 - val_loss: 0.6548 - val_accuracy: 0.5667 - val_recall: 0.2353\n",
            "Epoch 31/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.7447 - recall: 0.4355Epoch: 30 Logs: {'loss': 0.5985439419746399, 'accuracy': 0.7446808218955994, 'recall': 0.4354838728904724, 'val_loss': 0.6624513268470764, 'val_accuracy': 0.5555555820465088, 'val_recall': 0.21568627655506134}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5985 - accuracy: 0.7447 - recall: 0.4355 - val_loss: 0.6625 - val_accuracy: 0.5556 - val_recall: 0.2157\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6119 - accuracy: 0.7092 - recall: 0.4203Epoch: 31 Logs: {'loss': 0.611924409866333, 'accuracy': 0.7092198729515076, 'recall': 0.4202898442745209, 'val_loss': 0.6487179398536682, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 126ms/step - loss: 0.6119 - accuracy: 0.7092 - recall: 0.4203 - val_loss: 0.6487 - val_accuracy: 0.5778 - val_recall: 0.2549\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6080 - accuracy: 0.7375 - recall: 0.4143Epoch: 32 Logs: {'loss': 0.6079953908920288, 'accuracy': 0.737500011920929, 'recall': 0.41428571939468384, 'val_loss': 0.6372485756874084, 'val_accuracy': 0.6000000238418579, 'val_recall': 0.29411765933036804}\n",
            "5/5 [==============================] - 1s 136ms/step - loss: 0.6080 - accuracy: 0.7375 - recall: 0.4143 - val_loss: 0.6372 - val_accuracy: 0.6000 - val_recall: 0.2941\n",
            "Epoch 34/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.7375 - recall: 0.5256Epoch: 33 Logs: {'loss': 0.6071766018867493, 'accuracy': 0.737500011920929, 'recall': 0.5256410241127014, 'val_loss': 0.6278992295265198, 'val_accuracy': 0.644444465637207, 'val_recall': 0.3921568691730499}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.6072 - accuracy: 0.7375 - recall: 0.5256 - val_loss: 0.6279 - val_accuracy: 0.6444 - val_recall: 0.3922\n",
            "Epoch 35/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6145 - accuracy: 0.7437 - recall: 0.5000Epoch: 34 Logs: {'loss': 0.6144876480102539, 'accuracy': 0.7437499761581421, 'recall': 0.5, 'val_loss': 0.6219057440757751, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.6145 - accuracy: 0.7437 - recall: 0.5000 - val_loss: 0.6219 - val_accuracy: 0.6778 - val_recall: 0.4510\n",
            "Epoch 36/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.7518 - recall: 0.6197Epoch: 35 Logs: {'loss': 0.6098067760467529, 'accuracy': 0.7517730593681335, 'recall': 0.6197183132171631, 'val_loss': 0.6066836714744568, 'val_accuracy': 0.8111110925674438, 'val_recall': 0.686274528503418}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.6098 - accuracy: 0.7518 - recall: 0.6197 - val_loss: 0.6067 - val_accuracy: 0.8111 - val_recall: 0.6863\n",
            "Epoch 37/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.6938 - recall: 0.6835Epoch: 36 Logs: {'loss': 0.6111787557601929, 'accuracy': 0.6937500238418579, 'recall': 0.6835442781448364, 'val_loss': 0.6026001572608948, 'val_accuracy': 0.7888888716697693, 'val_recall': 0.686274528503418}\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.6112 - accuracy: 0.6938 - recall: 0.6835 - val_loss: 0.6026 - val_accuracy: 0.7889 - val_recall: 0.6863\n",
            "Epoch 38/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.7625 - recall: 0.6061Epoch: 37 Logs: {'loss': 0.6060081720352173, 'accuracy': 0.762499988079071, 'recall': 0.6060606241226196, 'val_loss': 0.6212055683135986, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.6060 - accuracy: 0.7625 - recall: 0.6061 - val_loss: 0.6212 - val_accuracy: 0.6778 - val_recall: 0.4510\n",
            "Epoch 39/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.7730 - recall: 0.5588Epoch: 38 Logs: {'loss': 0.5908740758895874, 'accuracy': 0.7730496525764465, 'recall': 0.5588235259056091, 'val_loss': 0.6466899514198303, 'val_accuracy': 0.5777778029441833, 'val_recall': 0.2549019753932953}\n",
            "5/5 [==============================] - 1s 129ms/step - loss: 0.5909 - accuracy: 0.7730 - recall: 0.5588 - val_loss: 0.6467 - val_accuracy: 0.5778 - val_recall: 0.2549\n",
            "Epoch 40/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6067 - accuracy: 0.6596 - recall: 0.3382Epoch: 39 Logs: {'loss': 0.6066994071006775, 'accuracy': 0.6595744490623474, 'recall': 0.3382352888584137, 'val_loss': 0.6550403833389282, 'val_accuracy': 0.5666666626930237, 'val_recall': 0.23529411852359772}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.6067 - accuracy: 0.6596 - recall: 0.3382 - val_loss: 0.6550 - val_accuracy: 0.5667 - val_recall: 0.2353\n",
            "Epoch 41/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.7250 - recall: 0.4605Epoch: 40 Logs: {'loss': 0.6066275835037231, 'accuracy': 0.7250000238418579, 'recall': 0.46052631735801697, 'val_loss': 0.621858537197113, 'val_accuracy': 0.6555555462837219, 'val_recall': 0.3921568691730499}\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.6066 - accuracy: 0.7250 - recall: 0.4605 - val_loss: 0.6219 - val_accuracy: 0.6556 - val_recall: 0.3922\n",
            "Epoch 42/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5922 - accuracy: 0.8014 - recall: 0.6324Epoch: 41 Logs: {'loss': 0.5922490954399109, 'accuracy': 0.8014184236526489, 'recall': 0.6323529481887817, 'val_loss': 0.6037741899490356, 'val_accuracy': 0.800000011920929, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.5922 - accuracy: 0.8014 - recall: 0.6324 - val_loss: 0.6038 - val_accuracy: 0.8000 - val_recall: 0.6667\n",
            "Epoch 43/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5977 - accuracy: 0.8000 - recall: 0.6533Epoch: 42 Logs: {'loss': 0.5977075099945068, 'accuracy': 0.800000011920929, 'recall': 0.653333306312561, 'val_loss': 0.6019251346588135, 'val_accuracy': 0.800000011920929, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 135ms/step - loss: 0.5977 - accuracy: 0.8000 - recall: 0.6533 - val_loss: 0.6019 - val_accuracy: 0.8000 - val_recall: 0.6667\n",
            "Epoch 44/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.6026 - accuracy: 0.7518 - recall: 0.6250Epoch: 43 Logs: {'loss': 0.6026273369789124, 'accuracy': 0.7517730593681335, 'recall': 0.625, 'val_loss': 0.6100146770477295, 'val_accuracy': 0.7222222089767456, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.6026 - accuracy: 0.7518 - recall: 0.6250 - val_loss: 0.6100 - val_accuracy: 0.7222 - val_recall: 0.5294\n",
            "Epoch 45/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5823 - accuracy: 0.7812 - recall: 0.6081Epoch: 44 Logs: {'loss': 0.5823113918304443, 'accuracy': 0.78125, 'recall': 0.6081081032752991, 'val_loss': 0.6130593419075012, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5823 - accuracy: 0.7812 - recall: 0.6081 - val_loss: 0.6131 - val_accuracy: 0.6778 - val_recall: 0.4510\n",
            "Epoch 46/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5915 - accuracy: 0.7518 - recall: 0.5217Epoch: 45 Logs: {'loss': 0.5914537906646729, 'accuracy': 0.7517730593681335, 'recall': 0.52173912525177, 'val_loss': 0.6267684698104858, 'val_accuracy': 0.6222222447395325, 'val_recall': 0.3333333432674408}\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5915 - accuracy: 0.7518 - recall: 0.5217 - val_loss: 0.6268 - val_accuracy: 0.6222 - val_recall: 0.3333\n",
            "Epoch 47/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.7943 - recall: 0.5484Epoch: 46 Logs: {'loss': 0.5732309222221375, 'accuracy': 0.7943262457847595, 'recall': 0.5483871102333069, 'val_loss': 0.6238529086112976, 'val_accuracy': 0.6333333253860474, 'val_recall': 0.3529411852359772}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5732 - accuracy: 0.7943 - recall: 0.5484 - val_loss: 0.6239 - val_accuracy: 0.6333 - val_recall: 0.3529\n",
            "Epoch 48/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5849 - accuracy: 0.7688 - recall: 0.5526Epoch: 47 Logs: {'loss': 0.5848644971847534, 'accuracy': 0.768750011920929, 'recall': 0.5526315569877625, 'val_loss': 0.6153621077537537, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 132ms/step - loss: 0.5849 - accuracy: 0.7688 - recall: 0.5526 - val_loss: 0.6154 - val_accuracy: 0.6778 - val_recall: 0.4314\n",
            "Epoch 49/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.8085 - recall: 0.6290Epoch: 48 Logs: {'loss': 0.5777345895767212, 'accuracy': 0.8085106611251831, 'recall': 0.6290322542190552, 'val_loss': 0.6099085807800293, 'val_accuracy': 0.6666666865348816, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5777 - accuracy: 0.8085 - recall: 0.6290 - val_loss: 0.6099 - val_accuracy: 0.6667 - val_recall: 0.4314\n",
            "Epoch 50/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5958 - accuracy: 0.7563 - recall: 0.5333Epoch: 49 Logs: {'loss': 0.5958489179611206, 'accuracy': 0.7562500238418579, 'recall': 0.5333333611488342, 'val_loss': 0.6170746684074402, 'val_accuracy': 0.644444465637207, 'val_recall': 0.37254902720451355}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5958 - accuracy: 0.7563 - recall: 0.5333 - val_loss: 0.6171 - val_accuracy: 0.6444 - val_recall: 0.3725\n",
            "Epoch 51/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.7437 - recall: 0.5309Epoch: 50 Logs: {'loss': 0.5875382423400879, 'accuracy': 0.7437499761581421, 'recall': 0.5308641791343689, 'val_loss': 0.6067373752593994, 'val_accuracy': 0.699999988079071, 'val_recall': 0.4901960790157318}\n",
            "5/5 [==============================] - 1s 139ms/step - loss: 0.5875 - accuracy: 0.7437 - recall: 0.5309 - val_loss: 0.6067 - val_accuracy: 0.7000 - val_recall: 0.4902\n",
            "Epoch 52/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5787 - accuracy: 0.7750 - recall: 0.6000Epoch: 51 Logs: {'loss': 0.578677237033844, 'accuracy': 0.7749999761581421, 'recall': 0.6000000238418579, 'val_loss': 0.6028319597244263, 'val_accuracy': 0.7222222089767456, 'val_recall': 0.529411792755127}\n",
            "5/5 [==============================] - 1s 138ms/step - loss: 0.5787 - accuracy: 0.7750 - recall: 0.6000 - val_loss: 0.6028 - val_accuracy: 0.7222 - val_recall: 0.5294\n",
            "Epoch 53/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.7812 - recall: 0.6081Epoch: 52 Logs: {'loss': 0.5835965871810913, 'accuracy': 0.78125, 'recall': 0.6081081032752991, 'val_loss': 0.6071226000785828, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.45098039507865906}\n",
            "5/5 [==============================] - 1s 134ms/step - loss: 0.5836 - accuracy: 0.7812 - recall: 0.6081 - val_loss: 0.6071 - val_accuracy: 0.6778 - val_recall: 0.4510\n",
            "Epoch 54/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.7518 - recall: 0.5507Epoch: 53 Logs: {'loss': 0.5901543498039246, 'accuracy': 0.7517730593681335, 'recall': 0.5507246255874634, 'val_loss': 0.599814772605896, 'val_accuracy': 0.7333333492279053, 'val_recall': 0.5490196347236633}\n",
            "5/5 [==============================] - 1s 140ms/step - loss: 0.5902 - accuracy: 0.7518 - recall: 0.5507 - val_loss: 0.5998 - val_accuracy: 0.7333 - val_recall: 0.5490\n",
            "Epoch 55/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.7688 - recall: 0.5811Epoch: 54 Logs: {'loss': 0.5914118885993958, 'accuracy': 0.768750011920929, 'recall': 0.5810810923576355, 'val_loss': 0.592308521270752, 'val_accuracy': 0.7888888716697693, 'val_recall': 0.6470588445663452}\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.5914 - accuracy: 0.7688 - recall: 0.5811 - val_loss: 0.5923 - val_accuracy: 0.7889 - val_recall: 0.6471\n",
            "Epoch 56/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.7872 - recall: 0.6324Epoch: 55 Logs: {'loss': 0.572496771812439, 'accuracy': 0.7872340679168701, 'recall': 0.6323529481887817, 'val_loss': 0.5882755517959595, 'val_accuracy': 0.800000011920929, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5725 - accuracy: 0.7872 - recall: 0.6324 - val_loss: 0.5883 - val_accuracy: 0.8000 - val_recall: 0.6667\n",
            "Epoch 57/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5795 - accuracy: 0.7937 - recall: 0.6579Epoch: 56 Logs: {'loss': 0.5795012712478638, 'accuracy': 0.793749988079071, 'recall': 0.6578947305679321, 'val_loss': 0.5939347147941589, 'val_accuracy': 0.7777777910232544, 'val_recall': 0.6274510025978088}\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5795 - accuracy: 0.7937 - recall: 0.6579 - val_loss: 0.5939 - val_accuracy: 0.7778 - val_recall: 0.6275\n",
            "Epoch 58/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.7943 - recall: 0.5932Epoch: 57 Logs: {'loss': 0.5774459838867188, 'accuracy': 0.7943262457847595, 'recall': 0.5932203531265259, 'val_loss': 0.6078395247459412, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5774 - accuracy: 0.7943 - recall: 0.5932 - val_loss: 0.6078 - val_accuracy: 0.6778 - val_recall: 0.4314\n",
            "Epoch 59/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5928 - accuracy: 0.7312 - recall: 0.5063Epoch: 58 Logs: {'loss': 0.5927724838256836, 'accuracy': 0.731249988079071, 'recall': 0.5063291192054749, 'val_loss': 0.6108230352401733, 'val_accuracy': 0.6666666865348816, 'val_recall': 0.4117647111415863}\n",
            "5/5 [==============================] - 1s 133ms/step - loss: 0.5928 - accuracy: 0.7312 - recall: 0.5063 - val_loss: 0.6108 - val_accuracy: 0.6667 - val_recall: 0.4118\n",
            "Epoch 60/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.7518 - recall: 0.5312Epoch: 59 Logs: {'loss': 0.5824482440948486, 'accuracy': 0.7517730593681335, 'recall': 0.53125, 'val_loss': 0.6080685257911682, 'val_accuracy': 0.6777777671813965, 'val_recall': 0.4313725531101227}\n",
            "5/5 [==============================] - 1s 127ms/step - loss: 0.5824 - accuracy: 0.7518 - recall: 0.5312 - val_loss: 0.6081 - val_accuracy: 0.6778 - val_recall: 0.4314\n",
            "Epoch 61/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.7730 - recall: 0.5652Epoch: 60 Logs: {'loss': 0.5720623731613159, 'accuracy': 0.7730496525764465, 'recall': 0.5652173757553101, 'val_loss': 0.5917009115219116, 'val_accuracy': 0.7777777910232544, 'val_recall': 0.6274510025978088}\n",
            "5/5 [==============================] - 1s 128ms/step - loss: 0.5721 - accuracy: 0.7730 - recall: 0.5652 - val_loss: 0.5917 - val_accuracy: 0.7778 - val_recall: 0.6275\n",
            "Epoch 62/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.7875 - recall: 0.6456Epoch: 61 Logs: {'loss': 0.5780452489852905, 'accuracy': 0.7875000238418579, 'recall': 0.6455696225166321, 'val_loss': 0.5822329521179199, 'val_accuracy': 0.800000011920929, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 130ms/step - loss: 0.5780 - accuracy: 0.7875 - recall: 0.6456 - val_loss: 0.5822 - val_accuracy: 0.8000 - val_recall: 0.6667\n",
            "Epoch 63/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5507 - accuracy: 0.8440 - recall: 0.7286Epoch: 62 Logs: {'loss': 0.5507309436798096, 'accuracy': 0.8439716100692749, 'recall': 0.7285714149475098, 'val_loss': 0.5777340531349182, 'val_accuracy': 0.8111110925674438, 'val_recall': 0.686274528503418}\n",
            "Val Accuracy More Than Set Thresholds {'loss': 0.5507309436798096, 'accuracy': 0.8439716100692749, 'recall': 0.7285714149475098, 'val_loss': 0.5777340531349182, 'val_accuracy': 0.8111110925674438, 'val_recall': 0.686274528503418}\n",
            "Saving Model ------------------------------\n",
            "5/5 [==============================] - 1s 239ms/step - loss: 0.5507 - accuracy: 0.8440 - recall: 0.7286 - val_loss: 0.5777 - val_accuracy: 0.8111 - val_recall: 0.6863\n",
            "Epoch 64/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.7660 - recall: 0.6716Epoch: 63 Logs: {'loss': 0.5639216899871826, 'accuracy': 0.7659574747085571, 'recall': 0.6716417670249939, 'val_loss': 0.577970027923584, 'val_accuracy': 0.800000011920929, 'val_recall': 0.6666666865348816}\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.5639 - accuracy: 0.7660 - recall: 0.6716 - val_loss: 0.5780 - val_accuracy: 0.8000 - val_recall: 0.6667\n",
            "Epoch 65/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.5789 - accuracy: 0.7518 - recall: 0.5942Epoch: 64 Logs: {'loss': 0.5788721442222595, 'accuracy': 0.7517730593681335, 'recall': 0.5942028760910034, 'val_loss': 0.5880317091941833, 'val_accuracy': 0.7777777910232544, 'val_recall': 0.6274510025978088}\n",
            "5/5 [==============================] - 1s 131ms/step - loss: 0.5789 - accuracy: 0.7518 - recall: 0.5942 - val_loss: 0.5880 - val_accuracy: 0.7778 - val_recall: 0.6275\n",
            "Epoch 66/1000\n",
            "3/5 [=================>............] - ETA: 0s - loss: 0.5869 - accuracy: 0.7396 - recall: 0.5882"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-faebbef6ace8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2,checkpoint3,checkpoint4])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations, callbacks =[checkpoint1,checkpoint2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mXceptionRecallAccuracyCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "#Whole Model Trainable\n",
        "for layer in model.layers:\n",
        "  print(layer.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b96928-71d6-4e00-9801-ea7734663923",
        "id": "Icv6s9NOcR6A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath_r = 'saug_weights_best_rmodel.hdf5'\n",
        "filepath_a = 'saug_weights_best_amodel.hdf5'\n",
        "filepath_vr = 'saug_weights_best_vrmodel.hdf5'\n",
        "filepath_va = 'saug_weights_best_vamodel.hdf5'\n",
        "\n",
        "# checkpoint = ModelCheckpoint(filepath,monitor = 'val_accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "# checkpoint1 = ModelCheckpoint(filepath_vr,monitor = 'val_recall_25',verbose = 1, save_best_only=True, mode='max')\n",
        "# checkpoint2 = ModelCheckpoint(filepath_va,monitor = 'val_accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "\n",
        "checkpoint3 = ModelCheckpoint(filepath_a,monitor = 'accuracy',verbose = 1, save_best_only=True, mode='max')\n",
        "# checkpoint4 = ModelCheckpoint(filepath_r,monitor = 'recall_25',verbose = 1, save_best_only=True, mode='max')"
      ],
      "metadata": {
        "id": "5vhIKwnlcR6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# r = tf.keras.metrics.Recall()\n"
      ],
      "metadata": {
        "id": "cX5eCy3wgs51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine Tuning\n",
        "\n",
        "optim = tf.keras.optimizers.Adam(\n",
        "    # learning_rate=0.001,\n",
        "    learning_rate=0.00001,\n",
        "    # learning_rate=0.0001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\"\n",
        ")\n",
        "\n",
        "\n",
        "# model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "# model.compile(loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall()], optimizer=optim)\n",
        "model.compile(loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall()], optimizer=optim)\n",
        "# model.compile(loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall()], optimizer=optim)\n",
        "#With 0.0001 lr.\n",
        "n_epochs = 1000\n",
        "num_train = trdf.shape[0]\n",
        "num_iterations = int(num_train/bs)\n",
        "\n",
        "history = model.fit(train_gen, epochs=n_epochs, validation_data=test_gen, steps_per_epoch=num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a81d716-3096-4fa6-f3e3-b5a0ac587158",
        "id": "Bss8nJ-QcR6A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - 2s 193ms/step - loss: 0.6886 - accuracy: 0.6313 - recall_1: 0.2188 - val_loss: 0.6901 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.6694 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7268 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6742 - accuracy: 0.6187 - recall_1: 0.0000e+00 - val_loss: 0.7304 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.6570 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7149 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - 1s 101ms/step - loss: 0.6603 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.7143 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6639 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.7191 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6673 - accuracy: 0.6103 - recall_1: 0.0000e+00 - val_loss: 0.6993 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.6581 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.6921 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6600 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.6967 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.6709 - accuracy: 0.5956 - recall_1: 0.0000e+00 - val_loss: 0.7005 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.6514 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7161 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6426 - accuracy: 0.6500 - recall_1: 0.0000e+00 - val_loss: 0.7241 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6455 - accuracy: 0.6471 - recall_1: 0.0000e+00 - val_loss: 0.7152 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6509 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.7021 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "5/5 [==============================] - 1s 100ms/step - loss: 0.6575 - accuracy: 0.6176 - recall_1: 0.0000e+00 - val_loss: 0.7002 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6378 - accuracy: 0.6562 - recall_1: 0.0000e+00 - val_loss: 0.6965 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6475 - accuracy: 0.6324 - recall_1: 0.0000e+00 - val_loss: 0.7102 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6459 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.7131 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.6506 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.6899 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.6638 - accuracy: 0.5882 - recall_1: 0.0000e+00 - val_loss: 0.6848 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.6427 - accuracy: 0.6471 - recall_1: 0.0000e+00 - val_loss: 0.7041 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.6274 - accuracy: 0.6562 - recall_1: 0.0000e+00 - val_loss: 0.7119 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.6514 - accuracy: 0.6125 - recall_1: 0.0000e+00 - val_loss: 0.6880 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6533 - accuracy: 0.5938 - recall_1: 0.0000e+00 - val_loss: 0.6794 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6397 - accuracy: 0.6250 - recall_1: 0.0000e+00 - val_loss: 0.7132 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6472 - accuracy: 0.6176 - recall_1: 0.0000e+00 - val_loss: 0.7001 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.6282 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.6882 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.6351 - accuracy: 0.6375 - recall_1: 0.0000e+00 - val_loss: 0.6833 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.6314 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.7148 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6217 - accuracy: 0.6500 - recall_1: 0.0000e+00 - val_loss: 0.6899 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.6202 - accuracy: 0.6324 - recall_1: 0.0000e+00 - val_loss: 0.6896 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.6199 - accuracy: 0.6324 - recall_1: 0.0000e+00 - val_loss: 0.6779 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.6046 - accuracy: 0.6618 - recall_1: 0.0000e+00 - val_loss: 0.7061 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6407 - accuracy: 0.6029 - recall_1: 0.0000e+00 - val_loss: 0.6830 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6321 - accuracy: 0.6471 - recall_1: 0.1273 - val_loss: 0.6708 - val_accuracy: 0.6575 - val_recall_1: 0.7059\n",
            "Epoch 36/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6282 - accuracy: 0.7941 - recall_1: 0.4902 - val_loss: 0.6900 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6135 - accuracy: 0.6544 - recall_1: 0.0208 - val_loss: 0.7326 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.6440 - accuracy: 0.6187 - recall_1: 0.0000e+00 - val_loss: 0.6847 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.6137 - accuracy: 0.6471 - recall_1: 0.0204 - val_loss: 0.6738 - val_accuracy: 0.5479 - val_recall_1: 0.0882\n",
            "Epoch 40/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.6183 - accuracy: 0.6838 - recall_1: 0.1765 - val_loss: 0.6721 - val_accuracy: 0.5753 - val_recall_1: 0.1765\n",
            "Epoch 41/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.6184 - accuracy: 0.6187 - recall_1: 0.0317 - val_loss: 0.6764 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6035 - accuracy: 0.6397 - recall_1: 0.0200 - val_loss: 0.6798 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6120 - accuracy: 0.6250 - recall_1: 0.0727 - val_loss: 0.6666 - val_accuracy: 0.6301 - val_recall_1: 0.2941\n",
            "Epoch 44/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.5965 - accuracy: 0.6618 - recall_1: 0.0213 - val_loss: 0.7046 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.6048 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.6731 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.5824 - accuracy: 0.6562 - recall_1: 0.0351 - val_loss: 0.6722 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "5/5 [==============================] - 1s 125ms/step - loss: 0.5868 - accuracy: 0.6618 - recall_1: 0.0417 - val_loss: 0.6601 - val_accuracy: 0.6301 - val_recall_1: 0.2941\n",
            "Epoch 48/1000\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 0.5895 - accuracy: 0.6765 - recall_1: 0.1731 - val_loss: 0.6606 - val_accuracy: 0.6027 - val_recall_1: 0.1765\n",
            "Epoch 49/1000\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.5840 - accuracy: 0.6471 - recall_1: 0.0769 - val_loss: 0.6560 - val_accuracy: 0.6027 - val_recall_1: 0.2059\n",
            "Epoch 50/1000\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.6094 - accuracy: 0.7132 - recall_1: 0.6545 - val_loss: 0.6513 - val_accuracy: 0.6438 - val_recall_1: 0.6765\n",
            "Epoch 51/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5855 - accuracy: 0.6912 - recall_1: 0.1522 - val_loss: 0.7469 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6533 - accuracy: 0.6397 - recall_1: 0.0000e+00 - val_loss: 0.6823 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 53/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.5970 - accuracy: 0.6562 - recall_1: 0.0179 - val_loss: 0.6704 - val_accuracy: 0.5479 - val_recall_1: 0.0882\n",
            "Epoch 54/1000\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.6026 - accuracy: 0.6250 - recall_1: 0.0164 - val_loss: 0.6677 - val_accuracy: 0.5616 - val_recall_1: 0.1176\n",
            "Epoch 55/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5788 - accuracy: 0.7647 - recall_1: 0.3111 - val_loss: 0.6569 - val_accuracy: 0.6712 - val_recall_1: 0.4412\n",
            "Epoch 56/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5716 - accuracy: 0.6750 - recall_1: 0.1034 - val_loss: 0.6690 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 57/1000\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.5799 - accuracy: 0.6471 - recall_1: 0.0400 - val_loss: 0.6617 - val_accuracy: 0.5753 - val_recall_1: 0.1176\n",
            "Epoch 58/1000\n",
            "5/5 [==============================] - 1s 101ms/step - loss: 0.5621 - accuracy: 0.7279 - recall_1: 0.2444 - val_loss: 0.6523 - val_accuracy: 0.6575 - val_recall_1: 0.3235\n",
            "Epoch 59/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5638 - accuracy: 0.6544 - recall_1: 0.1132 - val_loss: 0.6475 - val_accuracy: 0.6712 - val_recall_1: 0.3529\n",
            "Epoch 60/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5500 - accuracy: 0.7279 - recall_1: 0.2708 - val_loss: 0.6531 - val_accuracy: 0.6027 - val_recall_1: 0.1765\n",
            "Epoch 61/1000\n",
            "5/5 [==============================] - 1s 102ms/step - loss: 0.5521 - accuracy: 0.6985 - recall_1: 0.2778 - val_loss: 0.6392 - val_accuracy: 0.5890 - val_recall_1: 0.5000\n",
            "Epoch 62/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.5576 - accuracy: 0.6912 - recall_1: 0.2308 - val_loss: 0.6684 - val_accuracy: 0.5616 - val_recall_1: 0.0588\n",
            "Epoch 63/1000\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.5477 - accuracy: 0.7647 - recall_1: 0.4000 - val_loss: 0.6346 - val_accuracy: 0.6164 - val_recall_1: 0.4706\n",
            "Epoch 64/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5578 - accuracy: 0.6765 - recall_1: 0.1042 - val_loss: 0.7047 - val_accuracy: 0.5342 - val_recall_1: 0.0000e+00\n",
            "Epoch 65/1000\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.5824 - accuracy: 0.6812 - recall_1: 0.4754 - val_loss: 0.6461 - val_accuracy: 0.6438 - val_recall_1: 0.8235\n",
            "Epoch 66/1000\n",
            "5/5 [==============================] - 1s 102ms/step - loss: 0.5425 - accuracy: 0.7279 - recall_1: 0.3396 - val_loss: 0.6538 - val_accuracy: 0.6164 - val_recall_1: 0.2353\n",
            "Epoch 67/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5319 - accuracy: 0.7353 - recall_1: 0.2800 - val_loss: 0.6399 - val_accuracy: 0.6027 - val_recall_1: 0.4706\n",
            "Epoch 68/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5284 - accuracy: 0.7647 - recall_1: 0.4118 - val_loss: 0.6411 - val_accuracy: 0.6849 - val_recall_1: 0.4118\n",
            "Epoch 69/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.5399 - accuracy: 0.7250 - recall_1: 0.3276 - val_loss: 0.6366 - val_accuracy: 0.6301 - val_recall_1: 0.6765\n",
            "Epoch 70/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5309 - accuracy: 0.7500 - recall_1: 0.3922 - val_loss: 0.6286 - val_accuracy: 0.6712 - val_recall_1: 0.4118\n",
            "Epoch 71/1000\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.5434 - accuracy: 0.7875 - recall_1: 0.6825 - val_loss: 0.6374 - val_accuracy: 0.6712 - val_recall_1: 0.3529\n",
            "Epoch 72/1000\n",
            "5/5 [==============================] - 1s 105ms/step - loss: 0.5161 - accuracy: 0.7721 - recall_1: 0.3800 - val_loss: 0.6281 - val_accuracy: 0.6438 - val_recall_1: 0.6765\n",
            "Epoch 73/1000\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.5257 - accuracy: 0.7812 - recall_1: 0.6167 - val_loss: 0.6520 - val_accuracy: 0.6301 - val_recall_1: 0.2647\n",
            "Epoch 74/1000\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.5247 - accuracy: 0.7000 - recall_1: 0.2951 - val_loss: 0.6366 - val_accuracy: 0.6301 - val_recall_1: 0.8235\n",
            "Epoch 75/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.4923 - accuracy: 0.8529 - recall_1: 0.6863 - val_loss: 0.6494 - val_accuracy: 0.6712 - val_recall_1: 0.3235\n",
            "Epoch 76/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.5477 - accuracy: 0.7500 - recall_1: 0.4727 - val_loss: 0.6194 - val_accuracy: 0.6849 - val_recall_1: 0.4412\n",
            "Epoch 77/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5338 - accuracy: 0.7059 - recall_1: 0.2200 - val_loss: 0.6215 - val_accuracy: 0.6712 - val_recall_1: 0.4118\n",
            "Epoch 78/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.4973 - accuracy: 0.8235 - recall_1: 0.6667 - val_loss: 0.6295 - val_accuracy: 0.6575 - val_recall_1: 0.8235\n",
            "Epoch 79/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.4896 - accuracy: 0.7625 - recall_1: 0.4643 - val_loss: 0.6243 - val_accuracy: 0.6986 - val_recall_1: 0.4118\n",
            "Epoch 80/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.4797 - accuracy: 0.7868 - recall_1: 0.4490 - val_loss: 0.7079 - val_accuracy: 0.5068 - val_recall_1: 0.9706\n",
            "Epoch 81/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.5037 - accuracy: 0.7941 - recall_1: 0.7778 - val_loss: 0.6605 - val_accuracy: 0.6301 - val_recall_1: 0.2353\n",
            "Epoch 82/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.4994 - accuracy: 0.7437 - recall_1: 0.2727 - val_loss: 0.6213 - val_accuracy: 0.6438 - val_recall_1: 0.6765\n",
            "Epoch 83/1000\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.4724 - accuracy: 0.8162 - recall_1: 0.6327 - val_loss: 0.6174 - val_accuracy: 0.6027 - val_recall_1: 0.5000\n",
            "Epoch 84/1000\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.4954 - accuracy: 0.7563 - recall_1: 0.3934 - val_loss: 0.6198 - val_accuracy: 0.6301 - val_recall_1: 0.6765\n",
            "Epoch 85/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.4794 - accuracy: 0.8015 - recall_1: 0.4694 - val_loss: 0.6545 - val_accuracy: 0.6575 - val_recall_1: 0.3235\n",
            "Epoch 86/1000\n",
            "5/5 [==============================] - 1s 104ms/step - loss: 0.4623 - accuracy: 0.7574 - recall_1: 0.3043 - val_loss: 0.6739 - val_accuracy: 0.5342 - val_recall_1: 0.9118\n",
            "Epoch 87/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5208 - accuracy: 0.7353 - recall_1: 0.8039 - val_loss: 0.6817 - val_accuracy: 0.6027 - val_recall_1: 0.1765\n",
            "Epoch 88/1000\n",
            "5/5 [==============================] - 1s 103ms/step - loss: 0.5561 - accuracy: 0.6838 - recall_1: 0.2075 - val_loss: 0.7108 - val_accuracy: 0.5068 - val_recall_1: 0.9412\n",
            "Epoch 89/1000\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.5312 - accuracy: 0.7574 - recall_1: 0.7959 - val_loss: 0.6921 - val_accuracy: 0.5616 - val_recall_1: 0.0882\n",
            "Epoch 90/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.5185 - accuracy: 0.6838 - recall_1: 0.1569 - val_loss: 0.6332 - val_accuracy: 0.6027 - val_recall_1: 0.6471\n",
            "Epoch 91/1000\n",
            "5/5 [==============================] - 1s 107ms/step - loss: 0.6607 - accuracy: 0.5588 - recall_1: 0.7018 - val_loss: 0.6872 - val_accuracy: 0.5479 - val_recall_1: 0.9118\n",
            "Epoch 92/1000\n",
            "5/5 [==============================] - 1s 106ms/step - loss: 0.4893 - accuracy: 0.7574 - recall_1: 0.4468 - val_loss: 0.7158 - val_accuracy: 0.5068 - val_recall_1: 0.0000e+00\n",
            "Epoch 93/1000\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.5608 - accuracy: 0.6812 - recall_1: 0.0893 - val_loss: 0.6640 - val_accuracy: 0.6164 - val_recall_1: 0.5882\n",
            "Epoch 94/1000\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.5378 - accuracy: 0.7375 - recall_1: 0.5231 - val_loss: 0.7079 - val_accuracy: 0.5205 - val_recall_1: 0.8824\n",
            "Epoch 95/1000\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.5425 - accuracy: 0.7750 - recall_1: 0.7619 - val_loss: 0.6907 - val_accuracy: 0.5479 - val_recall_1: 0.8824\n",
            "Epoch 96/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5166 - accuracy: 0.7500 - recall_1: 0.5870"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-ba18ead8a7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1429\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1432\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}